{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abe8658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:15.551292Z",
     "iopub.status.busy": "2025-09-23T10:35:15.550523Z",
     "iopub.status.idle": "2025-09-23T10:35:20.078275Z",
     "shell.execute_reply": "2025-09-23T10:35:20.077334Z"
    },
    "papermill": {
     "duration": 4.541699,
     "end_time": "2025-09-23T10:35:20.080164",
     "exception": false,
     "start_time": "2025-09-23T10:35:15.538465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/bitsandbytes-20250725/bitsandbytes/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.46.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --no-deps /kaggle/input/bitsandbytes-20250725/bitsandbytes/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a881a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.098554Z",
     "iopub.status.busy": "2025-09-23T10:35:20.098291Z",
     "iopub.status.idle": "2025-09-23T10:35:20.112677Z",
     "shell.execute_reply": "2025-09-23T10:35:20.112066Z"
    },
    "papermill": {
     "duration": 0.024923,
     "end_time": "2025-09-23T10:35:20.113748",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.088825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deepseek_0_946.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deepseek_0_946.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2\"\n",
    "MODEL_TYPE = \"qwen2\"  # DeepSeek-R1 is based on Qwen2 architecture\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Increase for DeepSeek model's better long context handling\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/deepseek-r1-distill-qwen-14b-cv0.9455-fulltrain/transformers/default/1/ver_2\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 8  # Batch size 2 for RTX 5090 with 31GB VRAM\n",
    "EVAL_BATCH_SIZE = 8  # Eval can use larger batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Reduced to 32 for faster training\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.0000001\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'deepseek_r1_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"deepseek-r1-14b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"deepseek-r1-14b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 32  # LoRAのランク - reduced for memory efficiency\n",
    "LORA_ALPHA = 64  # LoRAのスケーリングパラメータ - reduced proportionally\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # DeepSeek-R1用のプロンプト - シンプルな形式\n",
    "    prompt = (\n",
    "        f\"User: [Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        \"Assistant: <think>\\n\\n</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（float16で読み込み）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # float16モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7db01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.131365Z",
     "iopub.status.busy": "2025-09-23T10:35:20.131152Z",
     "iopub.status.idle": "2025-09-23T10:35:20.142203Z",
     "shell.execute_reply": "2025-09-23T10:35:20.141521Z"
    },
    "papermill": {
     "duration": 0.021181,
     "end_time": "2025-09-23T10:35:20.143222",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.122041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_14b_0_946.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_14b_0_946.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/qwen-3/transformers/14b/1\"\n",
    "MODEL_TYPE = \"qwen2\"  # Add model type for proper handling\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Increase max length for better context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/qwen3-14b-lb0.945-fulltrain/transformers/default/1/ver_2\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 4  # Batch size 2 for RTX 5090 with 31GB VRAM\n",
    "EVAL_BATCH_SIZE = 4  # Eval can use larger batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Reduced to 32 for faster training\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.00000001\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'qwen3_14b_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"qwen3-14b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"qwen3-14b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 128  # LoRAのランク - reduced for memory efficiency\n",
    "LORA_ALPHA = 256  # LoRAのスケーリングパラメータ - reduced proportionally\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Qwen2.5-Math用の数学タスクに特化したプロンプト\n",
    "    prompt = (\n",
    "        \"<|im_start|>user\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（float16で読み込み）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # float16モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bec6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.160366Z",
     "iopub.status.busy": "2025-09-23T10:35:20.160176Z",
     "iopub.status.idle": "2025-09-23T10:35:20.171236Z",
     "shell.execute_reply": "2025-09-23T10:35:20.170673Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.021065,
     "end_time": "2025-09-23T10:35:20.172254",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.151189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_32b_0_947.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_32b_0_947.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/qwen-3/transformers/32b/1\"\n",
    "MODEL_TYPE = \"qwen2\"  # Add model type for proper handling\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 300  # Increase max length for better context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/qwen3-32b-9468/transformers/default/1/ver_2\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 16  # Batch size 2 for RTX 5090 with 31GB VRAM\n",
    "EVAL_BATCH_SIZE = 16  # Eval can use larger batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Reduced to 32 for faster training\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/best\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'qwen3_32b_0_947_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"qwen3-32b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"qwen3-32b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 16  # LoRAのランク - reduced for memory efficiency\n",
    "LORA_ALPHA = 32  # LoRAのスケーリングパラメータ - reduced proportionally\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = True  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Qwen2.5-Math用の数学タスクに特化したプロンプト\n",
    "    prompt = (\n",
    "        \"<|im_start|>user\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（4bit量子化で読み込み）\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # 4bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa6cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.190038Z",
     "iopub.status.busy": "2025-09-23T10:35:20.189856Z",
     "iopub.status.idle": "2025-09-23T10:35:20.200866Z",
     "shell.execute_reply": "2025-09-23T10:35:20.200145Z"
    },
    "papermill": {
     "duration": 0.021396,
     "end_time": "2025-09-23T10:35:20.201951",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.180555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing phi_4_0_948_fulltrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phi_4_0_948_fulltrain.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/ms-phi4/transformers/default/1/phi-4\"\n",
    "MODEL_TYPE = \"phi\"  # Phi-4 model type\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Phi-4 supports longer context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/phi-4-cv0965-fulltrain/transformers/default/1/ver_2_0965ft\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 4  # Smaller batch size for Phi-4\n",
    "EVAL_BATCH_SIZE = 4  # Eval batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Increased for effective batch size\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'phi_4_0_948_fulltrain_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"phi-4-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"phi-4-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration for Phi-4\n",
    "LORA_RANK = 64  # LoRAのランク - optimized for Phi-4\n",
    "LORA_ALPHA = 128  # LoRAのスケーリングパラメータ - 1:1 ratio with rank\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Phi-4 target modules\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率 - reduced for Phi-4\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n",
    "    prompt = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        \"<think>\\n\"\n",
    "        \"Let me analyze this mathematical misconception...\\n\"\n",
    "        \"</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（量子化なしでフルプレシジョン）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            torch_dtype=torch.float16,  # float16を使用（メモリ効率とパフォーマンスのバランス）\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # モデルは既にdevice_mapでGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "        tokenizer.pad_token_id = 100257\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5a84f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.219127Z",
     "iopub.status.busy": "2025-09-23T10:35:20.218947Z",
     "iopub.status.idle": "2025-09-23T10:35:20.229453Z",
     "shell.execute_reply": "2025-09-23T10:35:20.228838Z"
    },
    "papermill": {
     "duration": 0.020474,
     "end_time": "2025-09-23T10:35:20.230470",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.209996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing phi_4_reasoning_0_948.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phi_4_reasoning_0_948.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus\"\n",
    "MODEL_TYPE = \"phi\"  # Phi-4 model type\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Phi-4-reasoning-plus supports 32k context, but we use 1024 for efficiency\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/phi-4-reasoning-plus09476-ft/transformers/default/1/ver_2_9476ft\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 4  # Smaller batch size for Phi-4\n",
    "EVAL_BATCH_SIZE = 4  # Eval batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Increased for effective batch size\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.0000001\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'phi_4_reasoning_0_948_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"phi-4-reasoning-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"phi-4-reasoning-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration for Phi-4\n",
    "LORA_RANK = 64  # LoRAのランク - optimized for Phi-4\n",
    "LORA_ALPHA = 128  # LoRAのスケーリングパラメータ - 1:1 ratio with rank\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Phi-4 target modules\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率 - reduced for Phi-4\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n",
    "    prompt = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        \"<think>\\n\"\n",
    "        \"Let me analyze this mathematical misconception...\\n\"\n",
    "        \"</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（量子化なし）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True,  # CPUメモリ使用量を削減\n",
    "            torch_dtype=torch.float16  # FP16を使用してメモリ効率を改善\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # 8bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "        tokenizer.pad_token_id = 100349  # Phi-4-reasoning-plusのPADトークンID\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf46e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.248837Z",
     "iopub.status.busy": "2025-09-23T10:35:20.248664Z",
     "iopub.status.idle": "2025-09-23T10:35:20.267470Z",
     "shell.execute_reply": "2025-09-23T10:35:20.266899Z"
    },
    "papermill": {
     "duration": 0.030062,
     "end_time": "2025-09-23T10:35:20.268590",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.238528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing phi_new_loss_0_947.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phi_new_loss_0_947.py\n",
    "\"\"\"\n",
    "設定ファイル - Deberta モデルのトレーニングと推論用設定\n",
    "\"\"\"\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "# MODEL_NAME = \"/kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus\"\n",
    "# MODEL_TYPE = \"qwen3\"  # Add model type for proper handling\n",
    "MODEL_DIR = \"/kaggle/input/phi4-merged-masked\"\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Increase max length for better context\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES = \"0,1\"  # Use GPU 1 only\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"./\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 2  # Reduced for large model memory efficiency\n",
    "EVAL_BATCH_SIZE = 8  # Further reduced to avoid CUDA errors\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Increased for memory efficiency\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 100  # Warmup steps for learning rate scheduler\n",
    "WEIGHT_DECAY = 0.01  # Weight decay for regularization\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "USE_FP16 = True  # Use FP16 training for memory efficiency\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH ='/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "# '/home/sato/project/map/train_32835_updated.csv'\n",
    "# '/home/sato/project/map/train_final.csv'\n",
    "# '/home/sato/project/map/train_changetext.csv'\n",
    "# '/home/sato/project/map/train_ocr_corrected_openai.csv'\n",
    "# '/home/sato/project/map/train.csv'\n",
    "# '/home/sato/project/map/train_ocr_corrected_openai.csv'\n",
    "# '/home/sato/project/map/train.csv'\n",
    "# '/home/sato/project/map/train_ocr_corrected_openai_checkpoint_30400.csv'\n",
    "# '/home/sato/project/map/train.csv'\n",
    "# '/home/sato/project/map/train_ocr_corrected_openai_checkpoint_30400.csv'\n",
    "# '/home/sato/project/map/train_ocr_corrected.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/best\"\n",
    "LABEL_ENCODER_PATH = f\"/kaggle/input/question-label-mapping/label_encoder.joblib\"\n",
    "\n",
    "# Base model path for pseudo labeling (事前訓練済みモデルのパス)\n",
    "BASE_MODEL_PATH = f\"{OUTPUT_DIR}/best\"  # train.pyで作成されたモデルのパス\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"qwen3-0.6b-ocr-correct\"\n",
    "WANDB_RUN_NAME = \"qwen3-0.6b-origin-real_choice\"\n",
    "# \"qwen3-0.6b-4o-mini\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 16  # LoRAのランク\n",
    "LORA_ALPHA = 32  # LoRAのスケーリングパラメータ\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# 3段階学習用の設定\n",
    "STAGE1_EPOCHS = 3  # 第1段階のエポック数\n",
    "STAGE3_EPOCHS = 1  # 第3段階のエポック数\n",
    "STAGE3_LEARNING_RATE_RATIO = 1.0  # 第3段階の学習率比率（元の学習率に対する倍率）\n",
    "SKIP_STAGE1_TRAINING = False  # 第1段階をスキップするかどうか\n",
    "STAGE1_PRETRAINED_MODEL_PATH = \"./stage1_final\"  # 第1段階をスキップする場合の学習済み重みパス\n",
    "\"\"\"\n",
    "共通ユーティリティ関数 - 選択肢付きバージョン\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def prepare_answer_choices(train_data, mapping_file='/kaggle/input/question-label-mapping/question_answer_choice_mapping.csv'):\n",
    "    \"\"\"各問題のMC_Answer選択肢を準備（マッピングファイルを使用、小文字ラベル）\"\"\"\n",
    "    # マッピングファイルを読み込み\n",
    "    mapping_df = pd.read_csv(mapping_file)\n",
    "\n",
    "    # 各QuestionIdごとに選択肢を作成\n",
    "    choices_list = []\n",
    "\n",
    "    for question_id in train_data['QuestionId'].unique():\n",
    "        # 該当QuestionIdのマッピングを取得\n",
    "        question_mapping = mapping_df[mapping_df['QuestionId'] == question_id].copy()\n",
    "\n",
    "        if len(question_mapping) > 0:\n",
    "            # Choice（A,B,C,D）でソート\n",
    "            question_mapping = question_mapping.sort_values('Choice')\n",
    "            # 選択肢文字列を作成（小文字ラベル）\n",
    "            choice_items = []\n",
    "            choice_mapping = {}  # MC_Answer -> choice label のマッピング\n",
    "            for _, row in question_mapping.iterrows():\n",
    "                lowercase_choice = row['Choice'].lower()  # A -> a, B -> b, etc.\n",
    "                choice_items.append(f\"{lowercase_choice}. {row['MC_Answer']}\")\n",
    "                choice_mapping[row['MC_Answer']] = lowercase_choice\n",
    "            answer_choices_str = '\\n'.join(choice_items)\n",
    "        else:\n",
    "            # マッピングがない場合は従来の番号方式にフォールバック\n",
    "            question_answers = train_data[train_data['QuestionId'] == question_id]['MC_Answer'].unique()\n",
    "            choice_items = []\n",
    "            choice_mapping = {}\n",
    "            for i, ans in enumerate(question_answers):\n",
    "                lowercase_choice = chr(ord('a') + i)  # a, b, c, d, ...\n",
    "                choice_items.append(f\"{lowercase_choice}. {ans}\")\n",
    "                choice_mapping[ans] = lowercase_choice\n",
    "            answer_choices_str = '\\n'.join(choice_items)\n",
    "\n",
    "        choices_list.append({\n",
    "            'QuestionId': question_id,\n",
    "            'answer_choices_str': answer_choices_str,\n",
    "            'choice_mapping': choice_mapping  # MC_Answer -> choice label のマッピングも保存\n",
    "        })\n",
    "\n",
    "    choices = pd.DataFrame(choices_list)\n",
    "    return choices\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット（選択肢付き、回答をラベルに変換）\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # MC_Answerを選択肢ラベル（a, b, c, d）に変換\n",
    "    student_answer_label = row.get('choice_label', row['MC_Answer'])  # フォールバック\n",
    "\n",
    "    # Qwen2.5-Math用の数学タスクに特化したプロンプト（選択肢付き）\n",
    "    prompt = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        \"<think>\\n\"\n",
    "        \"Let me analyze this mathematical misconception...\\n\"\n",
    "        \"</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    print(f\"[DEBUG] *** compute_map3 function called! ***\")\n",
    "    print(f\"[DEBUG] eval_pred type: {type(eval_pred)}\")\n",
    "    print(f\"[DEBUG] eval_pred: {eval_pred}\")\n",
    "\n",
    "    try:\n",
    "        logits, labels = eval_pred\n",
    "        print(f\"[DEBUG] compute_map3 called with logits shape: {logits.shape}, labels shape: {labels.shape}\")\n",
    "\n",
    "        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "        score = 0.0\n",
    "        for i, label in enumerate(labels):\n",
    "            ranks = top3[i]\n",
    "            if ranks[0] == label:\n",
    "                score += 1.0\n",
    "            elif ranks[1] == label:\n",
    "                score += 1.0 / 2\n",
    "            elif ranks[2] == label:\n",
    "                score += 1.0 / 3\n",
    "\n",
    "        map3_score = score / len(labels)\n",
    "        result = {\"eval_map@3\": map3_score}\n",
    "        print(f\"[DEBUG] compute_map3 returning: {result}\")\n",
    "        print(f\"[DEBUG] *** compute_map3 function completed successfully! ***\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] compute_map3 failed: {e}\")\n",
    "        print(f\"[ERROR] Exception type: {type(e)}\")\n",
    "        import traceback\n",
    "        print(f\"[ERROR] Full traceback: {traceback.format_exc()}\")\n",
    "        return {\"eval_map@3\": 0.0}\n",
    "\n",
    "\n",
    "def create_submission(predictions_tuple, test_data, label_encoder, filter_true_false=True):\n",
    "    \"\"\"\n",
    "    predict_with_model の出力 (all_predictions, all_row_ids, all_probs) を使って\n",
    "    提出用の submission DataFrame を作成\n",
    "    \"\"\"\n",
    "\n",
    "    # tupleを展開\n",
    "    all_predictions, all_row_ids, all_probs = predictions_tuple\n",
    "\n",
    "    # --- QuestionIdごとの有効なラベル定義（省略せずそのまま使う） ---\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # True/False クラスを分類\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "    # QuestionIdごとにlabel_encoderのインデックスに変換\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, choices))[0]\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    # all_probs を使って確率を処理\n",
    "    for qid, correct, row_probs in zip(\n",
    "        test_data.QuestionId.tolist(),\n",
    "        test_data.is_correct.tolist(),\n",
    "        all_probs\n",
    "    ):\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # True/False制約を適用\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            else:\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row_probs[candidate_idx]\n",
    "        candidate_probs = torch.nn.functional.softmax(\n",
    "            torch.tensor(candidate_logits), dim=-1\n",
    "        ).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # label_encoderのインデックスに戻す\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    # DataFrame作成\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        \"Category:Misconception\": test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "保存された結合済みモデルとclassifierレイヤーを読み込むためのスクリプト\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from transformers import AutoModel\n",
    "import wandb\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import os, json, torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Qwen2ForSequenceClassificationWithMaskedLoss(nn.Module):\n",
    "    \"\"\"保存された結合済みモデルを読み込むためのクラス\"\"\"\n",
    "\n",
    "    def __init__(self, model_dir):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. モデル設定の読み込み\n",
    "        config_path = os.path.join(model_dir, \"model_config.pkl\")\n",
    "        with open(config_path, 'rb') as f:\n",
    "            config = pickle.load(f)\n",
    "\n",
    "        # 2. Question-Labelマッピングの読み込み\n",
    "        mapping_path = os.path.join(model_dir, \"question_label_mapping.pkl\")\n",
    "        with open(mapping_path, 'rb') as f:\n",
    "            self.question_label_map = pickle.load(f)\n",
    "\n",
    "        # 3. ベースモデル（結合済み）の読み込み\n",
    "        self.qwen = AutoModel.from_pretrained(\n",
    "            model_dir,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "        # 4. キャッシュの無効化\n",
    "        if hasattr(self.qwen.config, \"use_cache\"):\n",
    "            self.qwen.config.use_cache = False\n",
    "\n",
    "        # 5. Dropout層の初期化\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "\n",
    "        # 6. Classifierレイヤーの初期化\n",
    "        base_dtype = next(self.qwen.parameters()).dtype\n",
    "        device = next(self.qwen.parameters()).device\n",
    "        self.num_labels = config['n_classes']\n",
    "        self.classifier = nn.Linear(\n",
    "            self.qwen.config.hidden_size,\n",
    "            self.num_labels\n",
    "        ).to(dtype=base_dtype).to(device)\n",
    "\n",
    "        # 7. 保存されたclassifier重みの読み込み\n",
    "        classifier_path = os.path.join(model_dir, \"classifier_weights.pt\")\n",
    "        classifier_data = torch.load(classifier_path, map_location=device)\n",
    "        self.classifier.load_state_dict(classifier_data['classifier_state_dict'])\n",
    "\n",
    "        # 8. その他の設定\n",
    "        self.mask_value = -65000.0\n",
    "        self.config = self.qwen.config\n",
    "        self.config.num_labels = self.num_labels\n",
    "\n",
    "        print(f\"Model loaded successfully from: {model_dir}\")\n",
    "        print(f\"  - Number of classes: {self.num_labels}\")\n",
    "        print(f\"  - Question-label mapping: {len(self.question_label_map)} questions\")\n",
    "        print(f\"  - Mask value: {self.mask_value}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, question_ids=None, **kwargs):\n",
    "        \"\"\"フォワードパス（train_mask_loss.pyと同じ実装）\"\"\"\n",
    "        outputs = self.qwen(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, -1, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # QuestionIdベースのマスクを適用\n",
    "        if question_ids is not None and self.question_label_map is not None:\n",
    "            masked_logits = self.apply_question_mask(logits, question_ids)\n",
    "        else:\n",
    "            masked_logits = logits\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            if labels.dim() > 1:\n",
    "                labels = labels.view(-1)\n",
    "            if masked_logits.dim() == 3:\n",
    "                masked_logits = masked_logits.view(-1, self.num_labels)\n",
    "\n",
    "            try:\n",
    "                loss = loss_fct(masked_logits, labels)\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    loss = torch.tensor(100.0, requires_grad=True, device=masked_logits.device)\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing loss: {e}\")\n",
    "                loss = torch.tensor(100.0, requires_grad=True, device=masked_logits.device)\n",
    "\n",
    "        from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "        return SequenceClassifierOutput(loss=loss, logits=masked_logits)\n",
    "\n",
    "    def apply_question_mask(self, logits, question_ids):\n",
    "        \"\"\"QuestionIdごとに無効なラベルをマスクする\"\"\"\n",
    "        batch_size = logits.size(0)\n",
    "        device = logits.device\n",
    "\n",
    "        mask = torch.full_like(logits, self.mask_value)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            q_id = question_ids[i].item() if torch.is_tensor(question_ids[i]) else question_ids[i]\n",
    "\n",
    "            if q_id in self.question_label_map:\n",
    "                valid_labels = self.question_label_map[q_id]\n",
    "                for label_idx in valid_labels:\n",
    "                    mask[i, label_idx] = 0\n",
    "\n",
    "        masked_logits = logits + mask\n",
    "        return masked_logits\n",
    "\n",
    "\n",
    "def load_model_for_inference(model_dir):\n",
    "    \"\"\"\n",
    "    推論用にモデルを読み込む便利関数\n",
    "\n",
    "    Args:\n",
    "        model_dir: 保存されたモデルのディレクトリ\n",
    "\n",
    "    Returns:\n",
    "        model: 読み込まれたモデル\n",
    "        tokenizer: トークナイザー\n",
    "        label_encoder: ラベルエンコーダー\n",
    "    \"\"\"\n",
    "    # モデルの読み込み\n",
    "    model = Qwen2ForSequenceClassificationWithMaskedLoss(model_dir)\n",
    "    model.eval()\n",
    "\n",
    "    # トークナイザーの読み込み\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token_id = 0\n",
    "        tokenizer.pad_token = tokenizer.decode([0])\n",
    "\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    label_encoder_path = os.path.join(model_dir, \"label_encoder.joblib\")\n",
    "    label_encoder = joblib.load(label_encoder_path)\n",
    "\n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Phi-4 モデル予測スクリプト - マスク付き損失関数版で訓練されたモデル用\n",
    "テストデータに対して予測を実行し、提出ファイルを生成\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "import joblib\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# カスタムモジュールのインポート\n",
    "# from config import *\n",
    "# from utils_with_choices import prepare_answer_choices, format_input\n",
    "# from train_phi_4_best_2 import Phi4ForSequenceClassificationWithMaskedLoss\n",
    "\n",
    "# 警告を無効化\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n",
    "    prompt = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        \"<think>\\n\"\n",
    "        \"Let me analyze this mathematical misconception...\\n\"\n",
    "        \"</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def prepare_test_data(test_df):\n",
    "    \"\"\"テストデータの前処理\"\"\"\n",
    "    print(\"Preparing test data...\")\n",
    "\n",
    "    # --- QuestionId 32835のQuestionTextを更新 ---\n",
    "    print(\"Updating QuestionId 32835...\")\n",
    "    new_question_text = \"Which number is the greatest? Options: 6.0000 6.2 6.079 6.0001\"\n",
    "    mask_32835 = test_df['QuestionId'] == 32835\n",
    "    update_count = mask_32835.sum()\n",
    "\n",
    "    if update_count > 0:\n",
    "        original_text = test_df[mask_32835]['QuestionText'].iloc[0]\n",
    "        print(f\"Found {update_count} rows with QuestionId 32835\")\n",
    "        print(f\"Original: {original_text[:80]}...\")\n",
    "        print(f\"Updated to: {new_question_text}\")\n",
    "        test_df.loc[mask_32835, 'QuestionText'] = new_question_text\n",
    "    else:\n",
    "        print(\"No rows found with QuestionId 32835\")\n",
    "\n",
    "    # --- 選択肢データの準備 ---\n",
    "    print(\"Preparing answer choices for each question...\")\n",
    "    choices = prepare_answer_choices(test_df)\n",
    "    test_df = test_df.merge(choices[['QuestionId', 'answer_choices_str']], on='QuestionId', how='left')\n",
    "\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    # --- MC_Answerを選択肢ラベルに変換 ---\n",
    "    print(\"Converting MC_Answer to choice labels...\")\n",
    "    def get_choice_label(row):\n",
    "        question_id = row['QuestionId']\n",
    "        mc_answer = row['MC_Answer']\n",
    "        # 該当するchoice_mappingを取得\n",
    "        choice_mapping = choices[choices['QuestionId'] == question_id]['choice_mapping'].iloc[0]\n",
    "        return choice_mapping.get(mc_answer, mc_answer)  # マッピングがない場合は元の値\n",
    "\n",
    "    test_df['choice_label'] = test_df.apply(get_choice_label, axis=1)\n",
    "\n",
    "    # --- 入力テキストのフォーマット（is_correctは常にFalseでダミー値として設定） ---\n",
    "    print(\"Formatting input text with answer choices...\")\n",
    "    # test_df['is_correct'] = False  # テストデータには正解情報がないためダミー値\n",
    "    # test_df['text'] = test_df.apply(format_input, axis=1)\n",
    "    test_df = test_df.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test_df.is_correct = test_df.is_correct.fillna(0)\n",
    "    test_df['text'] = test_df.apply(format_input, axis=1)\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    print(\"Example test prompt:\")\n",
    "    print(test_df.text.values[0])\n",
    "\n",
    "    return test_df\n",
    "\n",
    "\n",
    "def tokenize_test_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"テストデータセットのトークナイズ（QuestionId付き）\"\"\"\n",
    "    def tokenize(batch):\n",
    "        tokenized = tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        # QuestionIdとrow_idを保持\n",
    "        tokenized['question_ids'] = batch['QuestionId']\n",
    "        tokenized['row_ids'] = batch['row_id']\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=['text', 'QuestionId', 'row_id']\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "class DataCollatorWithQuestionIdForTest:\n",
    "    \"\"\"テスト用QuestionIdを含むカスタムデータコレーター\"\"\"\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # バッチの最大長を取得\n",
    "        max_length = max(len(feature[\"input_ids\"]) for feature in features)\n",
    "\n",
    "        # パディング\n",
    "        batch = {}\n",
    "        for key in features[0].keys():\n",
    "            if key in [\"input_ids\", \"attention_mask\"]:\n",
    "                # input_idsとattention_maskをパディング\n",
    "                padded = []\n",
    "                for feature in features:\n",
    "                    # tensorをlistに変換\n",
    "                    if torch.is_tensor(feature[key]):\n",
    "                        feature_list = feature[key].tolist()\n",
    "                    else:\n",
    "                        feature_list = feature[key]\n",
    "\n",
    "                    remainder = [self.tokenizer.pad_token_id if key == \"input_ids\" else 0] * (max_length - len(feature_list))\n",
    "                    padded_feature = feature_list + remainder\n",
    "                    padded.append(padded_feature)\n",
    "                batch[key] = torch.tensor(padded, dtype=torch.long)\n",
    "            elif key == \"question_ids\":\n",
    "                # question_idsはパディング不要\n",
    "                batch[key] = torch.tensor([f[key] for f in features], dtype=torch.long)\n",
    "            elif key == \"row_ids\":\n",
    "                # row_idsはリストのまま保持\n",
    "                batch[key] = [f[key] for f in features]\n",
    "\n",
    "        return batch\n",
    "\n",
    "def predict_with_model(model, tokenizer, test_dataset, data_collator, batch_size=4, question_label_map=None):\n",
    "    \"\"\"モデルで予測を実行（マスクなし予測後に有効ラベルからTOP3選択）\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_row_ids = []\n",
    "    all_question_ids = []\n",
    "    all_probs = []   # ← 確率ベクトル保存用リストを追加\n",
    "\n",
    "    # データローダーの作成（カスタムデータコレーターを使用）\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    print(f\"Predicting on {len(test_dataset)} samples...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            question_ids = batch['question_ids']\n",
    "            row_ids = batch['row_ids']\n",
    "\n",
    "            # 予測\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            batch_predictions = []\n",
    "            for i, q_id in enumerate(question_ids):\n",
    "                q_id_value = q_id.item() if torch.is_tensor(q_id) else q_id\n",
    "                sample_probs = probs[i]\n",
    "\n",
    "                # 🔑 各サンプルの確率ベクトルを保存\n",
    "                all_probs.append(sample_probs)\n",
    "\n",
    "                if question_label_map and q_id_value in question_label_map:\n",
    "                    valid_labels = list(question_label_map[q_id_value])\n",
    "                    valid_probs = [(label, sample_probs[label]) for label in valid_labels]\n",
    "                    valid_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                    top3_labels = [label for label, _ in valid_probs[:3]]\n",
    "                    while len(top3_labels) < 3:\n",
    "                        top3_labels.append(0)\n",
    "\n",
    "                    batch_predictions.append(top3_labels)\n",
    "                else:\n",
    "                    top3_indices = np.argsort(-sample_probs)[:3]\n",
    "                    batch_predictions.append(top3_indices)\n",
    "\n",
    "            all_predictions.extend(batch_predictions)\n",
    "            all_row_ids.extend(row_ids)\n",
    "            all_question_ids.extend([q.item() if torch.is_tensor(q) else q for q in question_ids])\n",
    "\n",
    "    return all_predictions, all_row_ids, all_probs\n",
    "def main():\n",
    "    \"\"\"メイン予測関数\"\"\"\n",
    "    # GPU設定\n",
    "    if CUDA_VISIBLE_DEVICES is not None:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
    "        print(f\"Using CUDA device(s): {CUDA_VISIBLE_DEVICES}\")\n",
    "\n",
    "    # モデルの読み込み\n",
    "    model, tokenizer, le = load_model_for_inference(MODEL_DIR)\n",
    "    n_classes = len(le.classes_)\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "    # QuestionId-Labelマッピングの読み込み\n",
    "    mapping_path = f\"{MODEL_DIR}/question_label_mapping.pkl\"\n",
    "    print(f\"Loading question-label mapping from: {mapping_path}\")\n",
    "    with open(mapping_path, 'rb') as f:\n",
    "        question_label_map = pickle.load(f)\n",
    "    print(f\"Loaded mapping for {len(question_label_map)} questions\")\n",
    "\n",
    "    # --- テストデータの読み込みと前処理 ---\n",
    "    print(\"\\nLoading and preprocessing test data...\")\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "    test_df = prepare_test_data(test_df)\n",
    "\n",
    "    # データセットの作成\n",
    "    COLS = ['text', 'QuestionId', 'row_id']\n",
    "    test_ds = Dataset.from_pandas(test_df[COLS])\n",
    "\n",
    "    # --- トークナイズ ---\n",
    "    print(\"Tokenizing test dataset...\")\n",
    "    test_ds = tokenize_test_dataset(test_ds, tokenizer, MAX_LEN)\n",
    "\n",
    "    # --- データコレーターの作成 ---\n",
    "    data_collator = DataCollatorWithQuestionIdForTest(tokenizer=tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "    # --- 予測の実行 ---\n",
    "    print(\"\\nRunning predictions...\")\n",
    "    with torch.no_grad():\n",
    "        predictions = predict_with_model(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            test_ds,\n",
    "            data_collator,\n",
    "            batch_size=EVAL_BATCH_SIZE,\n",
    "            question_label_map=question_label_map\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"\\nCreating submission file...\")\n",
    "    submission = create_submission(predictions, test_df, le, filter_true_false=True)\n",
    "\n",
    "    # --- 保存 ---\n",
    "    submission.to_csv(\"./phi4_masked_submission.csv\", index=False)\n",
    "    print(\"Saved prediction file: ./phi4_masked_submission.csv\")\n",
    "    print(submission.head())\n",
    "\n",
    "    return submission\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05d7c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.286940Z",
     "iopub.status.busy": "2025-09-23T10:35:20.286757Z",
     "iopub.status.idle": "2025-09-23T10:35:20.303649Z",
     "shell.execute_reply": "2025-09-23T10:35:20.303085Z"
    },
    "papermill": {
     "duration": 0.028083,
     "end_time": "2025-09-23T10:35:20.304737",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.276654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile phi_new_loss_0_947.py\n",
    "# \"\"\"\n",
    "# 設定ファイル - Deberta モデルのトレーニングと推論用設定\n",
    "# \"\"\"\n",
    "\n",
    "# # Model configuration\n",
    "# VER = 2\n",
    "# # MODEL_NAME = \"/kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus\"\n",
    "# # MODEL_TYPE = \"qwen3\"  # Add model type for proper handling\n",
    "# MODEL_DIR = \"/kaggle/input/phi4-merged-masked\"\n",
    "# EPOCHS = 3  # Reduce epochs for initial testing\n",
    "# MAX_LEN = 250  # Increase max length for better context\n",
    "\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES = \"0,1\"  # Use GPU 1 only\n",
    "\n",
    "# # Directory settings\n",
    "# OUTPUT_DIR = f\"./\"\n",
    "\n",
    "# # Training parameters\n",
    "# TRAIN_BATCH_SIZE = 2  # Reduced for large model memory efficiency\n",
    "# EVAL_BATCH_SIZE = 8  # Further reduced to avoid CUDA errors\n",
    "# GRADIENT_ACCUMULATION_STEPS = 2  # Increased for memory efficiency\n",
    "# LEARNING_RATE = 2e-4\n",
    "# WARMUP_STEPS = 100  # Warmup steps for learning rate scheduler\n",
    "# WEIGHT_DECAY = 0.01  # Weight decay for regularization\n",
    "# LOGGING_STEPS = 50\n",
    "# SAVE_STEPS = 200\n",
    "# EVAL_STEPS = 200\n",
    "# USE_FP16 = True  # Use FP16 training for memory efficiency\n",
    "\n",
    "\n",
    "# # Data paths\n",
    "# TRAIN_DATA_PATH ='/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "# # '/home/sato/project/map/train_32835_updated.csv'\n",
    "# # '/home/sato/project/map/train_final.csv'\n",
    "# # '/home/sato/project/map/train_changetext.csv'\n",
    "# # '/home/sato/project/map/train_ocr_corrected_openai.csv'\n",
    "# # '/home/sato/project/map/train.csv'\n",
    "# # '/home/sato/project/map/train_ocr_corrected_openai.csv'\n",
    "# # '/home/sato/project/map/train.csv'\n",
    "# # '/home/sato/project/map/train_ocr_corrected_openai_checkpoint_30400.csv'\n",
    "# # '/home/sato/project/map/train.csv'\n",
    "# # '/home/sato/project/map/train_ocr_corrected_openai_checkpoint_30400.csv'\n",
    "# # '/home/sato/project/map/train_ocr_corrected.csv'\n",
    "# TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# # Model save paths\n",
    "# BEST_MODEL_PATH = f\"{OUTPUT_DIR}/best\"\n",
    "# LABEL_ENCODER_PATH = f\"/kaggle/input/question-label-mapping/label_encoder.joblib\"\n",
    "\n",
    "# # Base model path for pseudo labeling (事前訓練済みモデルのパス)\n",
    "# BASE_MODEL_PATH = f\"{OUTPUT_DIR}/best\"  # train.pyで作成されたモデルのパス\n",
    "\n",
    "# # Other settings\n",
    "# RANDOM_SEED = 42\n",
    "# VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "# # Submission settings\n",
    "# SUBMISSION_OUTPUT_PATH = 'submission.csv'\n",
    "\n",
    "# # WandB settings\n",
    "# USE_WANDB = True  # Set to False to disable WandB\n",
    "# WANDB_PROJECT = \"qwen3-0.6b-ocr-correct\"\n",
    "# WANDB_RUN_NAME = \"qwen3-0.6b-origin-real_choice\"\n",
    "# # \"qwen3-0.6b-4o-mini\"\n",
    "# WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "\n",
    "# # Early stopping settings\n",
    "# USE_EARLY_STOPPING = True\n",
    "# EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "# EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# # LoRA configuration\n",
    "# LORA_RANK = 16  # LoRAのランク\n",
    "# LORA_ALPHA = 32  # LoRAのスケーリングパラメータ\n",
    "# LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "# LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "# LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# # 3段階学習用の設定\n",
    "# STAGE1_EPOCHS = 3  # 第1段階のエポック数\n",
    "# STAGE3_EPOCHS = 1  # 第3段階のエポック数\n",
    "# STAGE3_LEARNING_RATE_RATIO = 1.0  # 第3段階の学習率比率（元の学習率に対する倍率）\n",
    "# SKIP_STAGE1_TRAINING = False  # 第1段階をスキップするかどうか\n",
    "# STAGE1_PRETRAINED_MODEL_PATH = \"./stage1_final\"  # 第1段階をスキップする場合の学習済み重みパス\n",
    "# \"\"\"\n",
    "# 共通ユーティリティ関数 - 選択肢付きバージョン\n",
    "# \"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer\n",
    "# from datasets import Dataset\n",
    "# import torch\n",
    "\n",
    "\n",
    "# def prepare_correct_answers(train_data):\n",
    "#     \"\"\"正解答案データを準備\"\"\"\n",
    "#     idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "#     correct = train_data.loc[idx].copy()\n",
    "#     correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "#     correct = correct.sort_values('c', ascending=False)\n",
    "#     correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "#     correct['is_correct'] = 1\n",
    "#     return correct\n",
    "\n",
    "\n",
    "# def prepare_answer_choices(train_data, mapping_file='/kaggle/input/question-label-mapping/question_answer_choice_mapping.csv'):\n",
    "#     \"\"\"各問題のMC_Answer選択肢を準備（マッピングファイルを使用、小文字ラベル）\"\"\"\n",
    "#     # マッピングファイルを読み込み\n",
    "#     mapping_df = pd.read_csv(mapping_file)\n",
    "\n",
    "#     # 各QuestionIdごとに選択肢を作成\n",
    "#     choices_list = []\n",
    "\n",
    "#     for question_id in train_data['QuestionId'].unique():\n",
    "#         # 該当QuestionIdのマッピングを取得\n",
    "#         question_mapping = mapping_df[mapping_df['QuestionId'] == question_id].copy()\n",
    "\n",
    "#         if len(question_mapping) > 0:\n",
    "#             # Choice（A,B,C,D）でソート\n",
    "#             question_mapping = question_mapping.sort_values('Choice')\n",
    "#             # 選択肢文字列を作成（小文字ラベル）\n",
    "#             choice_items = []\n",
    "#             choice_mapping = {}  # MC_Answer -> choice label のマッピング\n",
    "#             for _, row in question_mapping.iterrows():\n",
    "#                 lowercase_choice = row['Choice'].lower()  # A -> a, B -> b, etc.\n",
    "#                 choice_items.append(f\"{lowercase_choice}. {row['MC_Answer']}\")\n",
    "#                 choice_mapping[row['MC_Answer']] = lowercase_choice\n",
    "#             answer_choices_str = '\\n'.join(choice_items)\n",
    "#         else:\n",
    "#             # マッピングがない場合は従来の番号方式にフォールバック\n",
    "#             question_answers = train_data[train_data['QuestionId'] == question_id]['MC_Answer'].unique()\n",
    "#             choice_items = []\n",
    "#             choice_mapping = {}\n",
    "#             for i, ans in enumerate(question_answers):\n",
    "#                 lowercase_choice = chr(ord('a') + i)  # a, b, c, d, ...\n",
    "#                 choice_items.append(f\"{lowercase_choice}. {ans}\")\n",
    "#                 choice_mapping[ans] = lowercase_choice\n",
    "#             answer_choices_str = '\\n'.join(choice_items)\n",
    "\n",
    "#         choices_list.append({\n",
    "#             'QuestionId': question_id,\n",
    "#             'answer_choices_str': answer_choices_str,\n",
    "#             'choice_mapping': choice_mapping  # MC_Answer -> choice label のマッピングも保存\n",
    "#         })\n",
    "\n",
    "#     choices = pd.DataFrame(choices_list)\n",
    "#     return choices\n",
    "\n",
    "\n",
    "# def format_input(row):\n",
    "#     \"\"\"入力データをモデル用プロンプトにフォーマット（選択肢付き、回答をラベルに変換）\"\"\"\n",
    "#     if row[\"is_correct\"]:\n",
    "#         status = \"Yes\"\n",
    "#     else:\n",
    "#         status = \"No\"\n",
    "\n",
    "#     # MC_Answerを選択肢ラベル（a, b, c, d）に変換\n",
    "#     student_answer_label = row.get('choice_label', row['MC_Answer'])  # フォールバック\n",
    "\n",
    "#     # Qwen2.5-Math用の数学タスクに特化したプロンプト（選択肢付き）\n",
    "#     prompt = (\n",
    "#         \"<|user|>\\n\"\n",
    "#         f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "#         f\"Question: {row['QuestionText']}\\n\"\n",
    "#         f\"Answer: {row['MC_Answer']}\\n\"\n",
    "#         f\"Correct?: {status}\\n\"\n",
    "#         f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "#         \"<|end|>\\n\"\n",
    "#         \"<|assistant|>\\n\"\n",
    "#         \"<think>\\n\"\n",
    "#         \"Let me analyze this mathematical misconception...\\n\"\n",
    "#         \"</think>\\n\\n\"\n",
    "#     )\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "# def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "#     \"\"\"データセットをトークナイズ\"\"\"\n",
    "#     def tokenize(batch):\n",
    "#         # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "#         return tokenizer(\n",
    "#             batch['text'],\n",
    "#             padding=False,  # パディングはDataCollatorに任せる\n",
    "#             truncation=True,\n",
    "#             max_length=max_len,\n",
    "#             return_tensors=None  # map時は'None'を使用\n",
    "#         )\n",
    "\n",
    "#     dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "#     # columnsの設定時にlabelを保持\n",
    "#     columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "#     dataset.set_format(type='torch', columns=columns)\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# def compute_map3(eval_pred):\n",
    "#     \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "#     print(f\"[DEBUG] *** compute_map3 function called! ***\")\n",
    "#     print(f\"[DEBUG] eval_pred type: {type(eval_pred)}\")\n",
    "#     print(f\"[DEBUG] eval_pred: {eval_pred}\")\n",
    "\n",
    "#     try:\n",
    "#         logits, labels = eval_pred\n",
    "#         print(f\"[DEBUG] compute_map3 called with logits shape: {logits.shape}, labels shape: {labels.shape}\")\n",
    "\n",
    "#         probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "#         top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "#         score = 0.0\n",
    "#         for i, label in enumerate(labels):\n",
    "#             ranks = top3[i]\n",
    "#             if ranks[0] == label:\n",
    "#                 score += 1.0\n",
    "#             elif ranks[1] == label:\n",
    "#                 score += 1.0 / 2\n",
    "#             elif ranks[2] == label:\n",
    "#                 score += 1.0 / 3\n",
    "\n",
    "#         map3_score = score / len(labels)\n",
    "#         result = {\"eval_map@3\": map3_score}\n",
    "#         print(f\"[DEBUG] compute_map3 returning: {result}\")\n",
    "#         print(f\"[DEBUG] *** compute_map3 function completed successfully! ***\")\n",
    "#         return result\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] compute_map3 failed: {e}\")\n",
    "#         print(f\"[ERROR] Exception type: {type(e)}\")\n",
    "#         import traceback\n",
    "#         print(f\"[ERROR] Full traceback: {traceback.format_exc()}\")\n",
    "#         return {\"eval_map@3\": 0.0}\n",
    "\n",
    "\n",
    "# def create_submission(predictions, test_data, label_encoder):\n",
    "#     \"\"\"予測結果から提出用ファイルを作成\"\"\"\n",
    "#     probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "#     top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "#     flat = top3.flatten()\n",
    "#     decoded = label_encoder.inverse_transform(flat)\n",
    "#     top3_labels = decoded.reshape(top3.shape)\n",
    "#     pred_strings = [\" \".join(r) for r in top3_labels]\n",
    "\n",
    "#     submission = pd.DataFrame({\n",
    "#         'row_id': test_data.row_id.values,\n",
    "#         'Category:Misconception': pred_strings\n",
    "#     })\n",
    "#     return submission\n",
    "\n",
    "# \"\"\"\n",
    "# 保存された結合済みモデルとclassifierレイヤーを読み込むためのスクリプト\n",
    "# \"\"\"\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import os\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import joblib\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import (\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     Trainer,\n",
    "#     AutoConfig\n",
    "# )\n",
    "# from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "# from datasets import Dataset\n",
    "# import joblib\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "# from transformers import AutoModel\n",
    "# import wandb\n",
    "# from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "# import pickle\n",
    "# from collections import defaultdict\n",
    "# import os, json, torch\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# class Qwen2ForSequenceClassificationWithMaskedLoss(nn.Module):\n",
    "#     \"\"\"保存された結合済みモデルを読み込むためのクラス\"\"\"\n",
    "\n",
    "#     def __init__(self, model_dir):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # 1. モデル設定の読み込み\n",
    "#         config_path = os.path.join(model_dir, \"model_config.pkl\")\n",
    "#         with open(config_path, 'rb') as f:\n",
    "#             config = pickle.load(f)\n",
    "\n",
    "#         # 2. Question-Labelマッピングの読み込み\n",
    "#         mapping_path = os.path.join(model_dir, \"question_label_mapping.pkl\")\n",
    "#         with open(mapping_path, 'rb') as f:\n",
    "#             self.question_label_map = pickle.load(f)\n",
    "\n",
    "#         # 3. ベースモデル（結合済み）の読み込み\n",
    "#         self.qwen = AutoModel.from_pretrained(\n",
    "#             model_dir,\n",
    "#             trust_remote_code=True,\n",
    "#             torch_dtype=torch.float16,\n",
    "#             device_map=\"auto\",\n",
    "#             low_cpu_mem_usage=True\n",
    "#         )\n",
    "\n",
    "#         # 4. キャッシュの無効化\n",
    "#         if hasattr(self.qwen.config, \"use_cache\"):\n",
    "#             self.qwen.config.use_cache = False\n",
    "\n",
    "#         # 5. Dropout層の初期化\n",
    "#         self.dropout = nn.Dropout(config['dropout'])\n",
    "\n",
    "#         # 6. Classifierレイヤーの初期化\n",
    "#         base_dtype = next(self.qwen.parameters()).dtype\n",
    "#         device = next(self.qwen.parameters()).device\n",
    "#         self.num_labels = config['n_classes']\n",
    "#         self.classifier = nn.Linear(\n",
    "#             self.qwen.config.hidden_size,\n",
    "#             self.num_labels\n",
    "#         ).to(dtype=base_dtype).to(device)\n",
    "\n",
    "#         # 7. 保存されたclassifier重みの読み込み\n",
    "#         classifier_path = os.path.join(model_dir, \"classifier_weights.pt\")\n",
    "#         classifier_data = torch.load(classifier_path, map_location=device)\n",
    "#         self.classifier.load_state_dict(classifier_data['classifier_state_dict'])\n",
    "\n",
    "#         # 8. その他の設定\n",
    "#         self.mask_value = -65000.0\n",
    "#         self.config = self.qwen.config\n",
    "#         self.config.num_labels = self.num_labels\n",
    "\n",
    "#         print(f\"Model loaded successfully from: {model_dir}\")\n",
    "#         print(f\"  - Number of classes: {self.num_labels}\")\n",
    "#         print(f\"  - Question-label mapping: {len(self.question_label_map)} questions\")\n",
    "#         print(f\"  - Mask value: {self.mask_value}\")\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask=None, labels=None, question_ids=None, **kwargs):\n",
    "#         \"\"\"フォワードパス（train_mask_loss.pyと同じ実装）\"\"\"\n",
    "#         outputs = self.qwen(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.last_hidden_state[:, -1, :]\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "\n",
    "#         # QuestionIdベースのマスクを適用\n",
    "#         if question_ids is not None and self.question_label_map is not None:\n",
    "#             masked_logits = self.apply_question_mask(logits, question_ids)\n",
    "#         else:\n",
    "#             masked_logits = logits\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             if labels.dim() > 1:\n",
    "#                 labels = labels.view(-1)\n",
    "#             if masked_logits.dim() == 3:\n",
    "#                 masked_logits = masked_logits.view(-1, self.num_labels)\n",
    "\n",
    "#             try:\n",
    "#                 loss = loss_fct(masked_logits, labels)\n",
    "#                 if torch.isnan(loss) or torch.isinf(loss):\n",
    "#                     loss = torch.tensor(100.0, requires_grad=True, device=masked_logits.device)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error computing loss: {e}\")\n",
    "#                 loss = torch.tensor(100.0, requires_grad=True, device=masked_logits.device)\n",
    "\n",
    "#         from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "#         return SequenceClassifierOutput(loss=loss, logits=masked_logits)\n",
    "\n",
    "#     def apply_question_mask(self, logits, question_ids):\n",
    "#         \"\"\"QuestionIdごとに無効なラベルをマスクする\"\"\"\n",
    "#         batch_size = logits.size(0)\n",
    "#         device = logits.device\n",
    "\n",
    "#         mask = torch.full_like(logits, self.mask_value)\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "#             q_id = question_ids[i].item() if torch.is_tensor(question_ids[i]) else question_ids[i]\n",
    "\n",
    "#             if q_id in self.question_label_map:\n",
    "#                 valid_labels = self.question_label_map[q_id]\n",
    "#                 for label_idx in valid_labels:\n",
    "#                     mask[i, label_idx] = 0\n",
    "\n",
    "#         masked_logits = logits + mask\n",
    "#         return masked_logits\n",
    "\n",
    "\n",
    "# def load_model_for_inference(model_dir):\n",
    "#     \"\"\"\n",
    "#     推論用にモデルを読み込む便利関数\n",
    "\n",
    "#     Args:\n",
    "#         model_dir: 保存されたモデルのディレクトリ\n",
    "\n",
    "#     Returns:\n",
    "#         model: 読み込まれたモデル\n",
    "#         tokenizer: トークナイザー\n",
    "#         label_encoder: ラベルエンコーダー\n",
    "#     \"\"\"\n",
    "#     # モデルの読み込み\n",
    "#     model = Qwen2ForSequenceClassificationWithMaskedLoss(model_dir)\n",
    "#     model.eval()\n",
    "\n",
    "#     # トークナイザーの読み込み\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token_id = 0\n",
    "#         tokenizer.pad_token = tokenizer.decode([0])\n",
    "\n",
    "#     # ラベルエンコーダーの読み込み\n",
    "#     label_encoder_path = os.path.join(model_dir, \"label_encoder.joblib\")\n",
    "#     label_encoder = joblib.load(label_encoder_path)\n",
    "\n",
    "#     return model, tokenizer, label_encoder\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Phi-4 モデル予測スクリプト - マスク付き損失関数版で訓練されたモデル用\n",
    "# テストデータに対して予測を実行し、提出ファイルを生成\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from peft import PeftModel\n",
    "# import joblib\n",
    "# from datasets import Dataset\n",
    "# from tqdm import tqdm\n",
    "# import pickle\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # カスタムモジュールのインポート\n",
    "# # from config import *\n",
    "# # from utils_with_choices import prepare_answer_choices, format_input\n",
    "# # from train_phi_4_best_2 import Phi4ForSequenceClassificationWithMaskedLoss\n",
    "\n",
    "# # 警告を無効化\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def prepare_correct_answers(train_data):\n",
    "#     \"\"\"正解答案データを準備\"\"\"\n",
    "#     idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "#     correct = train_data.loc[idx].copy()\n",
    "#     correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "#     correct = correct.sort_values('c', ascending=False)\n",
    "#     correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "#     correct['is_correct'] = 1\n",
    "#     return correct\n",
    "\n",
    "\n",
    "# def format_input(row):\n",
    "#     \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "#     if row[\"is_correct\"]:\n",
    "#         status = \"Yes\"\n",
    "#     else:\n",
    "#         status = \"No\"\n",
    "\n",
    "#     # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n",
    "#     prompt = (\n",
    "#         \"<|user|>\\n\"\n",
    "#         f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "#         f\"Question: {row['QuestionText']}\\n\"\n",
    "#         f\"Answer: {row['MC_Answer']}\\n\"\n",
    "#         f\"Correct?: {status}\\n\"\n",
    "#         f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "#         \"<|end|>\\n\"\n",
    "#         \"<|assistant|>\\n\"\n",
    "#         \"<think>\\n\"\n",
    "#         \"Let me analyze this mathematical misconception...\\n\"\n",
    "#         \"</think>\\n\\n\"\n",
    "#     )\n",
    "#     return prompt\n",
    "\n",
    "# def prepare_test_data(test_df):\n",
    "#     \"\"\"テストデータの前処理\"\"\"\n",
    "#     print(\"Preparing test data...\")\n",
    "\n",
    "#     # --- QuestionId 32835のQuestionTextを更新 ---\n",
    "#     print(\"Updating QuestionId 32835...\")\n",
    "#     new_question_text = \"Which number is the greatest? Options: 6.0000 6.2 6.079 6.0001\"\n",
    "#     mask_32835 = test_df['QuestionId'] == 32835\n",
    "#     update_count = mask_32835.sum()\n",
    "\n",
    "#     if update_count > 0:\n",
    "#         original_text = test_df[mask_32835]['QuestionText'].iloc[0]\n",
    "#         print(f\"Found {update_count} rows with QuestionId 32835\")\n",
    "#         print(f\"Original: {original_text[:80]}...\")\n",
    "#         print(f\"Updated to: {new_question_text}\")\n",
    "#         test_df.loc[mask_32835, 'QuestionText'] = new_question_text\n",
    "#     else:\n",
    "#         print(\"No rows found with QuestionId 32835\")\n",
    "\n",
    "#     # --- 選択肢データの準備 ---\n",
    "#     print(\"Preparing answer choices for each question...\")\n",
    "#     choices = prepare_answer_choices(test_df)\n",
    "#     test_df = test_df.merge(choices[['QuestionId', 'answer_choices_str']], on='QuestionId', how='left')\n",
    "\n",
    "#     train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "#     train.Misconception = train.Misconception.fillna('NA')\n",
    "#     correct = prepare_correct_answers(train)\n",
    "\n",
    "#     # --- MC_Answerを選択肢ラベルに変換 ---\n",
    "#     print(\"Converting MC_Answer to choice labels...\")\n",
    "#     def get_choice_label(row):\n",
    "#         question_id = row['QuestionId']\n",
    "#         mc_answer = row['MC_Answer']\n",
    "#         # 該当するchoice_mappingを取得\n",
    "#         choice_mapping = choices[choices['QuestionId'] == question_id]['choice_mapping'].iloc[0]\n",
    "#         return choice_mapping.get(mc_answer, mc_answer)  # マッピングがない場合は元の値\n",
    "\n",
    "#     test_df['choice_label'] = test_df.apply(get_choice_label, axis=1)\n",
    "\n",
    "#     # --- 入力テキストのフォーマット（is_correctは常にFalseでダミー値として設定） ---\n",
    "#     print(\"Formatting input text with answer choices...\")\n",
    "#     # test_df['is_correct'] = False  # テストデータには正解情報がないためダミー値\n",
    "#     # test_df['text'] = test_df.apply(format_input, axis=1)\n",
    "#     test_df = test_df.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "#     test_df.is_correct = test_df.is_correct.fillna(0)\n",
    "#     test_df['text'] = test_df.apply(format_input, axis=1)\n",
    "#     print(f\"Test data shape: {test_df.shape}\")\n",
    "#     print(\"Example test prompt:\")\n",
    "#     print(test_df.text.values[0])\n",
    "\n",
    "#     return test_df\n",
    "\n",
    "\n",
    "# def tokenize_test_dataset(dataset, tokenizer, max_len):\n",
    "#     \"\"\"テストデータセットのトークナイズ（QuestionId付き）\"\"\"\n",
    "#     def tokenize(batch):\n",
    "#         tokenized = tokenizer(\n",
    "#             batch['text'],\n",
    "#             padding=False,  # パディングはDataCollatorに任せる\n",
    "#             truncation=True,\n",
    "#             max_length=max_len,\n",
    "#             return_tensors=None\n",
    "#         )\n",
    "#         # QuestionIdとrow_idを保持\n",
    "#         tokenized['question_ids'] = batch['QuestionId']\n",
    "#         tokenized['row_ids'] = batch['row_id']\n",
    "#         return tokenized\n",
    "\n",
    "#     tokenized_dataset = dataset.map(\n",
    "#         tokenize,\n",
    "#         batched=True,\n",
    "#         remove_columns=['text', 'QuestionId', 'row_id']\n",
    "#     )\n",
    "\n",
    "#     return tokenized_dataset\n",
    "\n",
    "\n",
    "# class DataCollatorWithQuestionIdForTest:\n",
    "#     \"\"\"テスト用QuestionIdを含むカスタムデータコレーター\"\"\"\n",
    "#     def __init__(self, tokenizer, max_length):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __call__(self, features):\n",
    "#         # バッチの最大長を取得\n",
    "#         max_length = max(len(feature[\"input_ids\"]) for feature in features)\n",
    "\n",
    "#         # パディング\n",
    "#         batch = {}\n",
    "#         for key in features[0].keys():\n",
    "#             if key in [\"input_ids\", \"attention_mask\"]:\n",
    "#                 # input_idsとattention_maskをパディング\n",
    "#                 padded = []\n",
    "#                 for feature in features:\n",
    "#                     # tensorをlistに変換\n",
    "#                     if torch.is_tensor(feature[key]):\n",
    "#                         feature_list = feature[key].tolist()\n",
    "#                     else:\n",
    "#                         feature_list = feature[key]\n",
    "\n",
    "#                     remainder = [self.tokenizer.pad_token_id if key == \"input_ids\" else 0] * (max_length - len(feature_list))\n",
    "#                     padded_feature = feature_list + remainder\n",
    "#                     padded.append(padded_feature)\n",
    "#                 batch[key] = torch.tensor(padded, dtype=torch.long)\n",
    "#             elif key == \"question_ids\":\n",
    "#                 # question_idsはパディング不要\n",
    "#                 batch[key] = torch.tensor([f[key] for f in features], dtype=torch.long)\n",
    "#             elif key == \"row_ids\":\n",
    "#                 # row_idsはリストのまま保持\n",
    "#                 batch[key] = [f[key] for f in features]\n",
    "\n",
    "#         return batch\n",
    "\n",
    "# def predict_with_model(model, tokenizer, test_dataset, data_collator, batch_size=4, question_label_map=None):\n",
    "#     \"\"\"モデルで予測を実行（マスクなし予測後に有効ラベルからTOP3選択）\"\"\"\n",
    "#     model.eval()\n",
    "\n",
    "#     all_predictions = []\n",
    "#     all_row_ids = []\n",
    "#     all_question_ids = []\n",
    "#     all_probs = []   # ← 確率ベクトル保存用リストを追加\n",
    "\n",
    "#     # データローダーの作成（カスタムデータコレーターを使用）\n",
    "#     dataloader = torch.utils.data.DataLoader(\n",
    "#         test_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=data_collator\n",
    "#     )\n",
    "\n",
    "#     print(f\"Predicting on {len(test_dataset)} samples...\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "#             input_ids = batch['input_ids'].cuda()\n",
    "#             attention_mask = batch['attention_mask'].cuda()\n",
    "#             question_ids = batch['question_ids']\n",
    "#             row_ids = batch['row_ids']\n",
    "\n",
    "#             # 予測\n",
    "#             outputs = model(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_mask=attention_mask\n",
    "#             )\n",
    "#             probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "#             batch_predictions = []\n",
    "#             for i, q_id in enumerate(question_ids):\n",
    "#                 q_id_value = q_id.item() if torch.is_tensor(q_id) else q_id\n",
    "#                 sample_probs = probs[i]\n",
    "\n",
    "#                 # 🔑 各サンプルの確率ベクトルを保存\n",
    "#                 all_probs.append(sample_probs)\n",
    "\n",
    "#                 if question_label_map and q_id_value in question_label_map:\n",
    "#                     valid_labels = list(question_label_map[q_id_value])\n",
    "#                     valid_probs = [(label, sample_probs[label]) for label in valid_labels]\n",
    "#                     valid_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#                     top3_labels = [label for label, _ in valid_probs[:3]]\n",
    "#                     while len(top3_labels) < 3:\n",
    "#                         top3_labels.append(0)\n",
    "\n",
    "#                     batch_predictions.append(top3_labels)\n",
    "#                 else:\n",
    "#                     top3_indices = np.argsort(-sample_probs)[:3]\n",
    "#                     batch_predictions.append(top3_indices)\n",
    "\n",
    "#             all_predictions.extend(batch_predictions)\n",
    "#             all_row_ids.extend(row_ids)\n",
    "#             all_question_ids.extend([q.item() if torch.is_tensor(q) else q for q in question_ids])\n",
    "\n",
    "#     return all_predictions, all_row_ids, all_probs\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"メイン予測関数\"\"\"\n",
    "\n",
    "#     # GPU設定\n",
    "#     if CUDA_VISIBLE_DEVICES is not None:\n",
    "#         os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
    "#         print(f\"Using CUDA device(s): {CUDA_VISIBLE_DEVICES}\")\n",
    "\n",
    "\n",
    "\n",
    "#     # モデルの読み込み\n",
    "#     model, tokenizer, le = load_model_for_inference(MODEL_DIR)\n",
    "\n",
    "#     n_classes = len(le.classes_)\n",
    "#     print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "#     # QuestionId-Labelマッピングの読み込み\n",
    "#     mapping_path = f\"{MODEL_DIR}/question_label_mapping.pkl\"\n",
    "#     print(f\"Loading question-label mapping from: {mapping_path}\")\n",
    "#     with open(mapping_path, 'rb') as f:\n",
    "#         question_label_map = pickle.load(f)\n",
    "#     print(f\"Loaded mapping for {len(question_label_map)} questions\")\n",
    "\n",
    "#     # --- テストデータの読み込みと前処理 ---\n",
    "#     print(\"\\nLoading and preprocessing test data...\")\n",
    "#     test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "#     print(f\"Original test data shape: {test_df.shape}\")\n",
    "\n",
    "#     # 前処理の実行\n",
    "#     test_df = prepare_test_data(test_df)\n",
    "\n",
    "#     # データセットの作成\n",
    "#     COLS = ['text', 'QuestionId', 'row_id']\n",
    "#     test_ds = Dataset.from_pandas(test_df[COLS])\n",
    "\n",
    "#     # # --- トークナイザーの初期化 ---\n",
    "#     # print(\"\\nInitializing tokenizer...\")\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "#     # # パディングトークンの設定\n",
    "#     # if tokenizer.pad_token is None:\n",
    "#     #     tokenizer.pad_token_id = 0\n",
    "#     #     tokenizer.pad_token = tokenizer.decode([0])\n",
    "\n",
    "#     # --- テストデータのトークナイズ ---\n",
    "#     print(\"Tokenizing test dataset...\")\n",
    "#     test_ds = tokenize_test_dataset(test_ds, tokenizer, MAX_LEN)\n",
    "\n",
    "#     # --- モデルの読み込み ---\n",
    "#     print(\"\\nLoading model...\")\n",
    "\n",
    "#     # --- データコレーターの作成 ---\n",
    "#     data_collator = DataCollatorWithQuestionIdForTest(tokenizer=tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "#     # --- 予測の実行 ---\n",
    "#     print(\"\\nRunning predictions...\")\n",
    "#     # predictions, row_ids = predict_with_model(\n",
    "#     #     model,\n",
    "#     #     tokenizer,\n",
    "#     #     test_ds,\n",
    "#     #     data_collator,\n",
    "#     #     batch_size=EVAL_BATCH_SIZE,\n",
    "#     #     question_label_map=question_label_map\n",
    "#     # )\n",
    "#     all_predictions, all_row_ids, probs_all = predict_with_model(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     test_ds,\n",
    "#     data_collator,\n",
    "#     batch_size=EVAL_BATCH_SIZE,\n",
    "#     question_label_map=question_label_map\n",
    "#     )\n",
    "\n",
    "#     # --- 予測結果の処理 ---\n",
    "#     print(\"\\nProcessing predictions...\")\n",
    "\n",
    "\n",
    "\n",
    "#     # --- 提出ファイルの作成 ---\n",
    "#     submission_data = []\n",
    "#     for row_id, pred_indices, prob_vector in zip(all_row_ids, all_predictions, probs_all):\n",
    "#         # ラベルに変換\n",
    "#         pred_labels = le.inverse_transform(range(len(prob_vector)))\n",
    "#         prob_list = prob_vector.tolist()\n",
    "\n",
    "#         row = {\n",
    "#             \"row_id\": row_id,\n",
    "#             \"preds\": str(list(pred_labels)),  # 全クラスのラベル\n",
    "#             \"probs\": str(list(prob_list))     # 各ラベルの確率\n",
    "#         }\n",
    "#         submission_data.append(row)\n",
    "\n",
    "#     submission_df = pd.DataFrame(submission_data)\n",
    "#     submission_df = submission_df.sort_values(\"row_id\")\n",
    "\n",
    "#     submission_path = \"./phi4_masked_submission.csv\"\n",
    "#     submission_df.to_csv(submission_path, index=False)\n",
    "#     print(f\"Saved prediction file for ensemble: {submission_path}\")\n",
    "\n",
    "\n",
    "#     # 統計情報の表示\n",
    "#     print(f\"\\nSubmission statistics:\")\n",
    "#     print(f\"Total predictions: {len(submission_df)}\")\n",
    "#     print(f\"First 5 predictions:\")\n",
    "#     print(submission_df.head())\n",
    "\n",
    "#     # ユニークな誤概念の数を計算\n",
    "#     all_misconceptions = []\n",
    "#     for misconceptions in submission_df['Category:Misconception'].values:\n",
    "#         all_misconceptions.extend(misconceptions.split())\n",
    "#     unique_misconceptions = len(set(all_misconceptions))\n",
    "#     print(f\"\\nUnique misconceptions in predictions: {unique_misconceptions}\")\n",
    "\n",
    "#     print(\"\\nPrediction completed successfully!\")\n",
    "#     return submission_df\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     submission = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373fd3f",
   "metadata": {
    "papermill": {
     "duration": 0.007969,
     "end_time": "2025-09-23T10:35:20.320648",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.312679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69641740",
   "metadata": {
    "papermill": {
     "duration": 0.007883,
     "end_time": "2025-09-23T10:35:20.336597",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.328714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2edb25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:35:20.353407Z",
     "iopub.status.busy": "2025-09-23T10:35:20.353211Z",
     "iopub.status.idle": "2025-09-23T10:41:51.548153Z",
     "shell.execute_reply": "2025-09-23T10:41:51.547191Z"
    },
    "papermill": {
     "duration": 391.204917,
     "end_time": "2025-09-23T10:41:51.549669",
     "exception": false,
     "start_time": "2025-09-23T10:35:20.344752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 10:35:38.704035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758623738.888515      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758623738.944436      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Found 2 GPUs\r\n",
      "Loading label encoder...\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\r\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\r\n",
      "  warnings.warn(\r\n",
      "Loading trained model and tokenizer...\r\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/qwen3-32b-9468/transformers/default/1/ver_2/best\r\n",
      "Loading base model from: /kaggle/input/qwen-3/transformers/32b/1\r\n",
      "Loading checkpoint shards: 100%|████████████████| 17/17 [05:41<00:00, 20.07s/it]\r\n",
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/qwen-3/transformers/32b/1 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "Successfully loaded LoRA fine-tuned model\r\n",
      "Loading test data...\r\n",
      "Loading training data for correct answers...\r\n",
      "Preprocessing test data...\r\n",
      "Tokenizing test data...\r\n",
      "Map: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 95.76 examples/s]\r\n",
      "Running inference...\r\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1497.43it/s]\r\n",
      "Creating submission file...\r\n",
      "Submission file saved to: qwen3_32b_0_947_submission.csv\r\n",
      "\r\n",
      "Submission preview:\r\n",
      "   row_id  ...                             Category:Misconception\r\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\r\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\r\n",
      "2   36698  ...  True_Correct:NA True_Neither:NA True_Misconcep...\r\n",
      "\r\n",
      "[3 rows x 6 columns]\r\n",
      "\r\n",
      "Submission shape: (3, 6)\r\n"
     ]
    }
   ],
   "source": [
    "!python qwen3_32b_0_947.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8f7e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:41:51.572018Z",
     "iopub.status.busy": "2025-09-23T10:41:51.571781Z",
     "iopub.status.idle": "2025-09-23T10:44:51.027098Z",
     "shell.execute_reply": "2025-09-23T10:44:51.026339Z"
    },
    "papermill": {
     "duration": 179.468547,
     "end_time": "2025-09-23T10:44:51.028561",
     "exception": false,
     "start_time": "2025-09-23T10:41:51.560014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 10:41:59.580180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758624119.606795      81 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758624119.614434      81 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Found 2 GPUs\r\n",
      "Loading label encoder...\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\r\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\r\n",
      "  warnings.warn(\r\n",
      "Loading trained model and tokenizer...\r\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/phi-4-reasoning-plus09476-ft/transformers/default/1/ver_2_9476ft/checkpoint-1722\r\n",
      "Loading base model from: /kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [02:31<00:00, 25.28s/it]\r\n",
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "Successfully loaded LoRA fine-tuned model\r\n",
      "Loading test data...\r\n",
      "Loading training data for correct answers...\r\n",
      "Preprocessing test data...\r\n",
      "Tokenizing test data...\r\n",
      "Map: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 97.33 examples/s]\r\n",
      "Running inference...\r\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1483.66it/s]\r\n",
      "Creating submission file...\r\n",
      "Submission file saved to: phi_4_reasoning_0_948_submission.csv\r\n",
      "\r\n",
      "Submission preview:\r\n",
      "   row_id  ...                             Category:Misconception\r\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\r\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\r\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\r\n",
      "\r\n",
      "[3 rows x 6 columns]\r\n",
      "\r\n",
      "Submission shape: (3, 6)\r\n"
     ]
    }
   ],
   "source": [
    "!python phi_4_reasoning_0_948.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671e09ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:44:51.051628Z",
     "iopub.status.busy": "2025-09-23T10:44:51.051414Z",
     "iopub.status.idle": "2025-09-23T10:47:42.512614Z",
     "shell.execute_reply": "2025-09-23T10:47:42.511880Z"
    },
    "papermill": {
     "duration": 171.47371,
     "end_time": "2025-09-23T10:47:42.514051",
     "exception": false,
     "start_time": "2025-09-23T10:44:51.040341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 10:44:58.042286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758624298.064934     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758624298.072009     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Found 2 GPUs\r\n",
      "Loading label encoder...\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\r\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\r\n",
      "  warnings.warn(\r\n",
      "Loading trained model and tokenizer...\r\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/phi-4-cv0965-fulltrain/transformers/default/1/ver_2_0965ft/checkpoint-1722\r\n",
      "Loading base model from: /kaggle/input/ms-phi4/transformers/default/1/phi-4\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [02:29<00:00, 25.00s/it]\r\n",
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/ms-phi4/transformers/default/1/phi-4 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "Successfully loaded LoRA fine-tuned model\r\n",
      "Loading test data...\r\n",
      "Loading training data for correct answers...\r\n",
      "Preprocessing test data...\r\n",
      "Tokenizing test data...\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 451.21 examples/s]\r\n",
      "Running inference...\r\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1431.50it/s]\r\n",
      "Creating submission file...\r\n",
      "Submission file saved to: phi_4_0_948_fulltrain_submission.csv\r\n",
      "\r\n",
      "Submission preview:\r\n",
      "   row_id  ...                             Category:Misconception\r\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\r\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\r\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\r\n",
      "\r\n",
      "[3 rows x 6 columns]\r\n",
      "\r\n",
      "Submission shape: (3, 6)\r\n"
     ]
    }
   ],
   "source": [
    "!python phi_4_0_948_fulltrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b41128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:47:42.537899Z",
     "iopub.status.busy": "2025-09-23T10:47:42.537616Z",
     "iopub.status.idle": "2025-09-23T10:50:34.277226Z",
     "shell.execute_reply": "2025-09-23T10:50:34.276440Z"
    },
    "papermill": {
     "duration": 171.753013,
     "end_time": "2025-09-23T10:50:34.278939",
     "exception": false,
     "start_time": "2025-09-23T10:47:42.525926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 10:47:48.935993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758624468.958211     143 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758624468.965103     143 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Found 2 GPUs\r\n",
      "Loading label encoder...\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\r\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\r\n",
      "  warnings.warn(\r\n",
      "Loading trained model and tokenizer...\r\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/deepseek-r1-distill-qwen-14b-cv0.9455-fulltrain/transformers/default/1/ver_2/checkpoint-1722\r\n",
      "Loading base model from: /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:30<00:00, 37.62s/it]\r\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "Successfully loaded LoRA fine-tuned model\r\n",
      "Loading test data...\r\n",
      "Loading training data for correct answers...\r\n",
      "Preprocessing test data...\r\n",
      "Tokenizing test data...\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 451.97 examples/s]\r\n",
      "Running inference...\r\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1311.54it/s]\r\n",
      "Creating submission file...\r\n",
      "Submission file saved to: deepseek_r1_submission.csv\r\n",
      "\r\n",
      "Submission preview:\r\n",
      "   row_id  ...                             Category:Misconception\r\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\r\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\r\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\r\n",
      "\r\n",
      "[3 rows x 6 columns]\r\n",
      "\r\n",
      "Submission shape: (3, 6)\r\n"
     ]
    }
   ],
   "source": [
    "!python deepseek_0_946.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658063d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:50:34.362366Z",
     "iopub.status.busy": "2025-09-23T10:50:34.362081Z",
     "iopub.status.idle": "2025-09-23T10:53:27.645978Z",
     "shell.execute_reply": "2025-09-23T10:53:27.645219Z"
    },
    "papermill": {
     "duration": 173.298409,
     "end_time": "2025-09-23T10:53:27.647443",
     "exception": false,
     "start_time": "2025-09-23T10:50:34.349034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 10:50:39.946718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758624639.969482     174 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758624639.976231     174 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Found 2 GPUs\r\n",
      "Loading label encoder...\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\r\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\r\n",
      "  warnings.warn(\r\n",
      "Loading trained model and tokenizer...\r\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/qwen3-14b-lb0.945-fulltrain/transformers/default/1/ver_2/checkpoint-1722\r\n",
      "Loading base model from: /kaggle/input/qwen-3/transformers/14b/1\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:33<00:00, 19.22s/it]\r\n",
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/qwen-3/transformers/14b/1 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "Successfully loaded LoRA fine-tuned model\r\n",
      "Loading test data...\r\n",
      "Loading training data for correct answers...\r\n",
      "Preprocessing test data...\r\n",
      "Tokenizing test data...\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 466.07 examples/s]\r\n",
      "Running inference...\r\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1476.87it/s]\r\n",
      "Creating submission file...\r\n",
      "Submission file saved to: qwen3_14b_submission.csv\r\n",
      "\r\n",
      "Submission preview:\r\n",
      "   row_id  ...                             Category:Misconception\r\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\r\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\r\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\r\n",
      "\r\n",
      "[3 rows x 6 columns]\r\n",
      "\r\n",
      "Submission shape: (3, 6)\r\n"
     ]
    }
   ],
   "source": [
    "!python qwen3_14b_0_946.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff82cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:53:27.674912Z",
     "iopub.status.busy": "2025-09-23T10:53:27.674674Z",
     "iopub.status.idle": "2025-09-23T10:58:07.047378Z",
     "shell.execute_reply": "2025-09-23T10:58:07.046407Z"
    },
    "papermill": {
     "duration": 279.387829,
     "end_time": "2025-09-23T10:58:07.048935",
     "exception": false,
     "start_time": "2025-09-23T10:53:27.661106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 10:53:33.906704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758624813.929240     205 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758624813.936130     205 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using CUDA device(s): 0,1\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [04:20<00:00, 43.44s/it]\r\n",
      "Model loaded successfully from: /kaggle/input/phi4-merged-masked\r\n",
      "  - Number of classes: 65\r\n",
      "  - Question-label mapping: 15 questions\r\n",
      "  - Mask value: -65000.0\r\n",
      "Number of classes: 65\r\n",
      "Loading question-label mapping from: /kaggle/input/phi4-merged-masked/question_label_mapping.pkl\r\n",
      "Loaded mapping for 15 questions\r\n",
      "\r\n",
      "Loading and preprocessing test data...\r\n",
      "Preparing test data...\r\n",
      "Updating QuestionId 32835...\r\n",
      "Found 1 rows with QuestionId 32835\r\n",
      "Original: Which number is the greatest?...\r\n",
      "Updated to: Which number is the greatest? Options: 6.0000 6.2 6.079 6.0001\r\n",
      "Preparing answer choices for each question...\r\n",
      "Converting MC_Answer to choice labels...\r\n",
      "Formatting input text with answer choices...\r\n",
      "Test data shape: (3, 9)\r\n",
      "Example test prompt:\r\n",
      "<|user|>\r\n",
      "[Mathematical Misconception Analysis Task]\r\n",
      "\r\n",
      "Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.]\r\n",
      "Answer: \\( \\frac{1}{3} \\)\r\n",
      "Correct?: Yes\r\n",
      "Explanation: I think that 1/3 is the answer, as it's the simplest form of 3/9.\r\n",
      "<|end|>\r\n",
      "<|assistant|>\r\n",
      "<think>\r\n",
      "Let me analyze this mathematical misconception...\r\n",
      "</think>\r\n",
      "\r\n",
      "\r\n",
      "Tokenizing test dataset...\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 295.44 examples/s]\r\n",
      "\r\n",
      "Running predictions...\r\n",
      "Predicting on 3 samples...\r\n",
      "Predicting: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.10s/it]\r\n",
      "\r\n",
      "Creating submission file...\r\n",
      "Saved prediction file: ./phi4_masked_submission.csv\r\n",
      "   row_id  ...                             Category:Misconception\r\n",
      "0   36696  ...  True_Correct:NA True_Misconception:Incomplete ...\r\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\r\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\r\n",
      "\r\n",
      "[3 rows x 6 columns]\r\n"
     ]
    }
   ],
   "source": [
    "!python  phi_new_loss_0_947.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8660ab3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.077599Z",
     "iopub.status.busy": "2025-09-23T10:58:07.077357Z",
     "iopub.status.idle": "2025-09-23T10:58:07.080781Z",
     "shell.execute_reply": "2025-09-23T10:58:07.080206Z"
    },
    "papermill": {
     "duration": 0.018646,
     "end_time": "2025-09-23T10:58:07.081864",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.063218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# a = pd.read_csv('/kaggle/working/phi4_masked_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe48c095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.109859Z",
     "iopub.status.busy": "2025-09-23T10:58:07.109613Z",
     "iopub.status.idle": "2025-09-23T10:58:07.112460Z",
     "shell.execute_reply": "2025-09-23T10:58:07.111986Z"
    },
    "papermill": {
     "duration": 0.018359,
     "end_time": "2025-09-23T10:58:07.113521",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.095162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f3a9aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.140918Z",
     "iopub.status.busy": "2025-09-23T10:58:07.140716Z",
     "iopub.status.idle": "2025-09-23T10:58:07.146818Z",
     "shell.execute_reply": "2025-09-23T10:58:07.146061Z"
    },
    "papermill": {
     "duration": 0.02106,
     "end_time": "2025-09-23T10:58:07.147952",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.126892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c b a\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_k_ensemble(ll, k=3, weights=None):\n",
    "\n",
    "    lists = [l.split(' ') for l in ll]\n",
    "    # If no weights provided, use equal weighting\n",
    "    if weights is None:\n",
    "        weights = [1.0 for _ in lists]\n",
    "    score = defaultdict(int)\n",
    "\n",
    "    for i, lst in enumerate(lists):\n",
    "        weight = weights[i]\n",
    "        for rank, item in enumerate(lst):\n",
    "            score[item] += (len(lst) - rank) * weight\n",
    "\n",
    "    sorted_items = sorted(score.items(), key=lambda x: -x[1])\n",
    "    return ' '.join([item for item, _ in sorted_items[:k]])\n",
    "\n",
    "list1 = 'a b d f'\n",
    "list2 = 'b c a e'\n",
    "list3 = 'c e b'\n",
    "list4 = 'c e d'\n",
    "\n",
    "print(get_top_k_ensemble([list1, list2, list3, list4], k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99209f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.175509Z",
     "iopub.status.busy": "2025-09-23T10:58:07.174904Z",
     "iopub.status.idle": "2025-09-23T10:58:07.457009Z",
     "shell.execute_reply": "2025-09-23T10:58:07.456162Z"
    },
    "papermill": {
     "duration": 0.297231,
     "end_time": "2025-09-23T10:58:07.458388",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.161157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv('/kaggle/working/deepseek_r1_submission.csv')\n",
    "df2 = pd.read_csv('/kaggle/working/qwen3_14b_submission.csv')\n",
    "df3 = pd.read_csv('/kaggle/working/phi_4_reasoning_0_948_submission.csv')\n",
    "df4 = pd.read_csv('/kaggle/working/phi_4_0_948_fulltrain_submission.csv')\n",
    "df5 = pd.read_csv('/kaggle/working/qwen3_32b_0_947_submission.csv')\n",
    "df6 = pd.read_csv('/kaggle/working/phi4_masked_submission.csv')\n",
    "\n",
    "df1 = df1.sort_values('row_id').reset_index(drop=True)\n",
    "df2 = df2.sort_values('row_id').reset_index(drop=True)\n",
    "df3 = df3.sort_values('row_id').reset_index(drop=True)\n",
    "df4 = df4.sort_values('row_id').reset_index(drop=True)\n",
    "df5 = df5.sort_values('row_id').reset_index(drop=True)\n",
    "df6 = df6.sort_values('row_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bdd5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.487023Z",
     "iopub.status.busy": "2025-09-23T10:58:07.486786Z",
     "iopub.status.idle": "2025-09-23T10:58:07.498583Z",
     "shell.execute_reply": "2025-09-23T10:58:07.497935Z"
    },
    "papermill": {
     "duration": 0.027005,
     "end_time": "2025-09-23T10:58:07.499820",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.472815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "for r1, r2, r3, r4, r5, r6 in zip(\n",
    "    df1.itertuples(), df2.itertuples(), df3.itertuples(),\n",
    "    df4.itertuples(), df5.itertuples(), df6.itertuples()\n",
    "):\n",
    "    prob_preds_1 = sorted([(pb, pr) for pb, pr in zip(eval(r1.probs), eval(r1.preds))], key=lambda x: x[1])\n",
    "    prob_preds_2 = sorted([(pb, pr) for pb, pr in zip(eval(r2.probs), eval(r2.preds))], key=lambda x: x[1])\n",
    "    prob_preds_3 = sorted([(pb, pr) for pb, pr in zip(eval(r3.probs), eval(r3.preds))], key=lambda x: x[1])\n",
    "    prob_preds_4 = sorted([(pb, pr) for pb, pr in zip(eval(r4.probs), eval(r4.preds))], key=lambda x: x[1])\n",
    "    prob_preds_5 = sorted([(pb, pr) for pb, pr in zip(eval(r5.probs), eval(r5.preds))], key=lambda x: x[1])\n",
    "    prob_preds_6 = sorted([(pb, pr) for pb, pr in zip(eval(r6.probs), eval(r6.preds))], key=lambda x: x[1])\n",
    "\n",
    "    # Should be same for all row_ids\n",
    "    choices = [x[1] for x in prob_preds_1]\n",
    "\n",
    "    # Set your model weights here (must match number of models).\n",
    "    # Default: equal weights (edit as needed).\n",
    "    weights = np.array([0.1, 0.1, 0.2, 0.2, 0.15, 0.25], dtype=float)\n",
    "\n",
    "    weighted_probs = np.average([\n",
    "        [x[0] for x in prob_preds_1],\n",
    "        [x[0] for x in prob_preds_2],\n",
    "        [x[0] for x in prob_preds_3],\n",
    "        [x[0] for x in prob_preds_4],\n",
    "        [x[0] for x in prob_preds_5],\n",
    "        [x[0] for x in prob_preds_6],\n",
    "    ], axis=0, weights=weights)\n",
    "\n",
    "    final_prob_preds = sorted([(l, p) for l, p in zip(choices, weighted_probs)], key=lambda x: -x[1])\n",
    "\n",
    "    row = {\n",
    "        \"row_id\": r1.row_id,\n",
    "        \"Category:Misconception\": \" \".join([x[0] for x in final_prob_preds[:3]])\n",
    "    }\n",
    "\n",
    "    ensemble_predictions.append(row)\n",
    "\n",
    "ensemble_predictions = pd.DataFrame(ensemble_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6b6526b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.527252Z",
     "iopub.status.busy": "2025-09-23T10:58:07.527052Z",
     "iopub.status.idle": "2025-09-23T10:58:07.530626Z",
     "shell.execute_reply": "2025-09-23T10:58:07.529966Z"
    },
    "papermill": {
     "duration": 0.018519,
     "end_time": "2025-09-23T10:58:07.531682",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.513163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensemble_predictions = []\n",
    "# for r1, r2, r3, r4, r5 in zip(df1.itertuples(), df2.itertuples(), df3.itertuples(), df4.itertuples(), df5.itertuples()):\n",
    "\n",
    "#     prob_preds_1 = sorted([(pb, pr) for pb, pr in zip(eval(r1.probs), eval(r1.preds))], key=lambda x: x[1])\n",
    "#     prob_preds_2 = sorted([(pb, pr) for pb, pr in zip(eval(r2.probs), eval(r2.preds))], key=lambda x: x[1])\n",
    "#     prob_preds_3 = sorted([(pb, pr) for pb, pr in zip(eval(r3.probs), eval(r3.preds))], key=lambda x: x[1])\n",
    "#     prob_preds_4 = sorted([(pb, pr) for pb, pr in zip(eval(r4.probs), eval(r4.preds))], key=lambda x: x[1])\n",
    "#     prob_preds_5 = sorted([(pb, pr) for pb, pr in zip(eval(r5.probs), eval(r5.preds))], key=lambda x: x[1])\n",
    "\n",
    "#     # Should be same for all row_ids\n",
    "#     choices = [x[1] for x in prob_preds_1]\n",
    "\n",
    "#     mean_probs = np.mean([\n",
    "#         [x[0] for x in prob_preds_1],\n",
    "#         [x[0] for x in prob_preds_2],\n",
    "#         [x[0] for x in prob_preds_3],\n",
    "#         [x[0] for x in prob_preds_4],\n",
    "#         [x[0] for x in prob_preds_5],\n",
    "#     ],\n",
    "#     axis=0)\n",
    "\n",
    "#     final_prob_preds = sorted([(l, p) for l, p in zip(choices, mean_probs)], key=lambda x: -x[1])\n",
    "\n",
    "#     row = {\n",
    "#         \"row_id\": r1.row_id,\n",
    "#         \"Category:Misconception\": \" \".join([x[0] for x in final_prob_preds[:3]])\n",
    "#     }\n",
    "\n",
    "#     ensemble_predictions.append(row)\n",
    "\n",
    "# ensemble_predictions = pd.DataFrame(ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5167079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.559293Z",
     "iopub.status.busy": "2025-09-23T10:58:07.559104Z",
     "iopub.status.idle": "2025-09-23T10:58:07.563570Z",
     "shell.execute_reply": "2025-09-23T10:58:07.562904Z"
    },
    "papermill": {
     "duration": 0.019238,
     "end_time": "2025-09-23T10:58:07.564600",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.545362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_predictions.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4477c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:58:07.592471Z",
     "iopub.status.busy": "2025-09-23T10:58:07.591888Z",
     "iopub.status.idle": "2025-09-23T10:58:07.606320Z",
     "shell.execute_reply": "2025-09-23T10:58:07.605817Z"
    },
    "papermill": {
     "duration": 0.029165,
     "end_time": "2025-09-23T10:58:07.607318",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.578153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Category:Misconception</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36696</td>\n",
       "      <td>True_Correct:NA True_Neither:NA True_Misconcep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36697</td>\n",
       "      <td>False_Misconception:WNB False_Neither:NA False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36698</td>\n",
       "      <td>True_Neither:NA True_Correct:NA True_Misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                             Category:Misconception\n",
       "0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n",
       "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
       "2   36698  True_Neither:NA True_Correct:NA True_Misconcep..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a434e",
   "metadata": {
    "papermill": {
     "duration": 0.013221,
     "end_time": "2025-09-23T10:58:07.634300",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.621079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acfc26",
   "metadata": {
    "papermill": {
     "duration": 0.013187,
     "end_time": "2025-09-23T10:58:07.660918",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.647731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edc746",
   "metadata": {
    "papermill": {
     "duration": 0.013201,
     "end_time": "2025-09-23T10:58:07.687468",
     "exception": false,
     "start_time": "2025-09-23T10:58:07.674267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 8203568,
     "sourceId": 12962237,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210603,
     "sourceId": 12972436,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 252296453,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 225262,
     "modelInstanceId": 204048,
     "sourceId": 256580,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301527,
     "sourceId": 363149,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301540,
     "sourceId": 363168,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 410342,
     "modelInstanceId": 391645,
     "sourceId": 492735,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411100,
     "modelInstanceId": 392435,
     "sourceId": 493918,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411495,
     "modelInstanceId": 392864,
     "sourceId": 494692,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411517,
     "modelInstanceId": 392888,
     "sourceId": 494731,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411554,
     "modelInstanceId": 392927,
     "sourceId": 494800,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 414003,
     "modelInstanceId": 395433,
     "sourceId": 497710,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 414910,
     "modelInstanceId": 396413,
     "sourceId": 498993,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1376.677973,
   "end_time": "2025-09-23T10:58:08.018995",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-23T10:35:11.341022",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
