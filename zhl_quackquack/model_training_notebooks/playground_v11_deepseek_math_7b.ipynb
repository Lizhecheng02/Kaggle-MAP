{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0c1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Auto-reload imported modules ---\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil, warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    LogitsProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training, TaskType\n",
    "import joblib\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "VER = 50\n",
    "DIR = f\"ver_{VER}\"; os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"./models/Qwen2.5-14B-Instruct\"   # or \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-7B-Instruct\"\n",
    "MODEL_NAME = \"./models/Qwen2.5-Math-7B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# Qwen/Qwen2.5-Math-1.5B-Instruct\n",
    "\n",
    "MAX_LEN = 256\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "CV_FOLD = 5\n",
    "CV_SEED = 42\n",
    "USE_SINGLE_FOLD = False\n",
    "EVAL_MODE = \"vote@3\"  # \"vote\" or \"vote@3\" (use this one)\n",
    "\n",
    "TRAIN_CSV = \"./raw_data/train.csv\"\n",
    "TEST_CSV  = \"./raw_data/test.csv\"\n",
    "CLEAN_MISLABEL = \"ignore\"   # ignore | fix | remove\n",
    "\n",
    "N_CLASSES = 65\n",
    "# TODO: Add flash-attention-2\n",
    "# TODO: Train Val Split, some low counts < 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecb5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"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\")\n",
    "\n",
    "# # Choose your model Qwen/Qwen2.5-0.5B-Instruct\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-Math-7B-Instruct\"\n",
    "\n",
    "# # Download and save model + tokenizer locally\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7b2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mislabel_entries(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"Using {CLEAN_MISLABEL} for data cleaning Strat\")\n",
    "    qid = 31778\n",
    "    correct_answer = r\"\\( 6 \\)\"\n",
    "    rows_to_fix = []\n",
    "    for idx, row in train[train['QuestionId'] == qid].iterrows():\n",
    "        is_correct_answer = row['MC_Answer'] == correct_answer\n",
    "        is_true = str(row['Category']).startswith(\"True\")\n",
    "        if is_correct_answer and not is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "        elif not is_correct_answer and is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "    assert len(rows_to_fix) == 18, \"Expected 18 mislabeled entries to fix, found a different number.\"\n",
    "\n",
    "    if CLEAN_MISLABEL == \"ignore\":\n",
    "        return train\n",
    "    elif CLEAN_MISLABEL == \"remove\":\n",
    "        return train.drop(index=rows_to_fix).reset_index(drop=True)\n",
    "    elif CLEAN_MISLABEL == \"fix\":\n",
    "        for idx in rows_to_fix:\n",
    "            row = train.loc[idx]\n",
    "            cat = str(row['Category']).split(\"_\", 1)[-1]\n",
    "            prefix = \"True\" if row['MC_Answer'] == correct_answer else \"False\"\n",
    "            train.at[idx, 'Category'] = f\"{prefix}_{cat}\"\n",
    "        return train\n",
    "    else:\n",
    "        raise ValueError(\"CLEAN_MISLABEL must be 'ignore', 'remove', or 'fix'\")\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    train = pd.read_csv(TRAIN_CSV)\n",
    "    train = clean_mislabel_entries(train)\n",
    "    train['Misconception'] = train['Misconception'].fillna('NA')\n",
    "    train['target'] = train['Category'] + \":\" + train['Misconception']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train['label'] = le.fit_transform(train['target'])\n",
    "\n",
    "    idx = train['Category'].str.startswith(\"True\")\n",
    "    correct = (\n",
    "        train[idx].groupby(['QuestionId','MC_Answer']).size()\n",
    "        .reset_index(name='c').sort_values('c', ascending=False)\n",
    "        .drop_duplicates(['QuestionId']).assign(is_correct=1)[['QuestionId','MC_Answer','is_correct']]\n",
    "    )\n",
    "    train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    train['is_correct'] = train['is_correct'].fillna(0)\n",
    "\n",
    "    # suppose you also have a QuestionId -> CorrectAnswerText mapping\n",
    "    answers = train.loc[train[\"is_correct\"] == 1, [\"QuestionId\", \"MC_Answer\"]].rename(\n",
    "        columns={\"MC_Answer\": \"TrueAnswer\"}\n",
    "    ).drop_duplicates(['QuestionId'], keep=\"first\")\n",
    "    \n",
    "    train = train.merge(answers, on=\"QuestionId\", how=\"left\")\n",
    "\n",
    "    train[\"split_key\"] = (train['QuestionId'].astype(str) + \"_\" + train['label'].astype(str)).astype('category').cat.codes\n",
    "    return train, le\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"Yes\" if row['is_correct'] else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Student Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\\n\"\n",
    "    )\n",
    "\n",
    "def format_input_v2(row):\n",
    "    x = \"Yes\" if row['is_correct'] else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"True Answer: {row['TrueAnswer']}\\n\"\n",
    "        f\"Student Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\\n\"\n",
    "    )\n",
    "\n",
    "def prepare_dataset(df, tokenizer, cols=['text', 'label']):\n",
    "    df = df[cols].copy().reset_index(drop=True)\n",
    "    df['label'] = df['label'].astype(np.int64)\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "    ds = ds.map(lambda batch: tokenizer(batch['text'], truncation=True, max_length=MAX_LEN), batched=True, remove_columns=['text'])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e25cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tokenizer.pad_token\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Missing tokenizer.pad_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def load_base_model_bf16():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=N_CLASSES,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # model.config.use_cache = False  # better for training/checkpointing\n",
    "    return model\n",
    "\n",
    "def load_base_model_nf4():\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=N_CLASSES,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfacdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute MAP@3 ---\n",
    "def compute_map3(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    match = (top3 == labels[:, None])\n",
    "    map3 = np.mean([1 if m[0] else 0.5 if m[1] else 1/3 if m[2] else 0 for m in match])\n",
    "    return {\"map@3\": map3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3839a52-88d9-4c82-b568-e78d7dd6a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, weight=self.alpha, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)   # = softmax prob of the true class\n",
    "        focal = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal.sum()\n",
    "        return focal\n",
    "\n",
    "# 1) Cost Sensitive Loss v1\n",
    "class CostSensitiveTrainer(Trainer):\n",
    "    def __init__(self, alpha=None, gamma=2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if alpha is not None:\n",
    "            self.alpha = alpha.to(self.model.device)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: dict[str, torch.Tensor],\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch=None,\n",
    "    ):\n",
    "        # extract labels\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # compute cross-entropy\n",
    "        loss_fn = FocalLoss(alpha=self.alpha, gamma=self.gamma)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08002e88-7d9b-46d5-8561-3c80c8eacc99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# v21 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=3,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\", \"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=3,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v22 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=3e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=3.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v23 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=3,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v27 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=3e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict( # Good\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=3.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict( \n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict( # Best\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v28 save for 7b \n",
    "param_sets = [\n",
    "    dict( # maybe optimal for this target_modules\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=4e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v29 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.75e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.25e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v30 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.25e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5e77ef-284b-4061-b0c3-4299f72478b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# v31\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 1,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=32, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e05260e-6eec-4ca7-bfc9-f52c397bf69d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# V33\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.1e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.3e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V34\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.8e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.9e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V35\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.8e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V37\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=3e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.6e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V39\n",
    "param_sets = [\n",
    "    dict( # best\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.4e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.6e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V39\n",
    "param_sets = [\n",
    "    # dict( \n",
    "    #     name=\"qv_r16_alpha32_e2\",\n",
    "    #     target_modules=[\"q_proj\", \"v_proj\",\"o_proj\"],\n",
    "    #     r=16, lora_alpha=32,   # α/r = 4\n",
    "    #     lr=2.5e-4,\n",
    "    #     dropout=0.05,\n",
    "    #     epochs=2,\n",
    "    #     lr_scheduler = \"linear\",\n",
    "    #     lr_scheduler_kwargs = {},\n",
    "    #     use_class_weights = False,\n",
    "    #     gamma = 0,\n",
    "    # ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=3e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7203c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V39\n",
    "param_sets = [\n",
    "    dict( # best\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.6e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V40\n",
    "param_sets = [\n",
    "    dict( # best\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=64, lora_alpha=64,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=32, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=32, lora_alpha=64,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V33\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r64_alpha64_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=64, lora_alpha=64,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=32, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.6e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "# [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "\n",
    "\n",
    "train_df, le = load_and_preprocess_data()\n",
    "n_classes = train_df['label'].nunique()\n",
    "print(f\"Total of {n_classes} classes.\")\n",
    "train_df['text'] = train_df.apply(format_input, axis=1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CV_FOLD, shuffle=True, random_state=CV_SEED)\n",
    "fold_indices = list(skf.split(train_df, train_df['split_key']))\n",
    "if USE_SINGLE_FOLD:\n",
    "    fold_indices = [fold_indices[0]]\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "for fold in range(len(fold_indices)):\n",
    "    tr_idx, va_idx = fold_indices[fold]\n",
    "    tr, va = train_df.iloc[tr_idx].copy(), train_df.iloc[va_idx].copy()\n",
    "\n",
    "    # Get Class Weights\n",
    "    class_counts = np.bincount(tr['label'])\n",
    "    safe_counts = np.where(class_counts > 0, class_counts, 1)\n",
    "    weights = class_counts.max() / safe_counts\n",
    "    weights = np.sqrt(weights)\n",
    "    weights = weights / weights.sum() * len(class_counts)  # normalize around #classes\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    ds_tr = prepare_dataset(tr, tokenizer)\n",
    "    ds_va = prepare_dataset(va, tokenizer)\n",
    "    \n",
    "    best_map = -1.0\n",
    "    for repeat_idx in range(len(param_sets)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{CV_FOLD} REPEAT {repeat_idx} ===\")\n",
    "\n",
    "        cfg = param_sets[repeat_idx]\n",
    "        print(f\"Trying {cfg['name']}\")\n",
    "\n",
    "        # LoRA config for this repeat\n",
    "        lora_config = LoraConfig(\n",
    "            r=cfg[\"r\"],\n",
    "            lora_alpha=cfg[\"lora_alpha\"],\n",
    "            target_modules=cfg[\"target_modules\"],\n",
    "            lora_dropout=cfg[\"dropout\"],\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,  # Or CAUSAL_LM, etc. depending on model\n",
    "            modules_to_save=[\"classifier\", \"score\"],\n",
    "            inference_mode=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            del model\n",
    "            del trainer\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # model = load_base_model_nf4()\n",
    "        model = load_base_model_bf16()\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        if model.config.pad_token_id is None:\n",
    "            print(f\"Missig model.config.pad_token_id\")\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # model = PeftModel.from_pretrained(model, \"./ver_10/fold_0/checkpoint-917/\")\n",
    "\n",
    "        LR_RATE = cfg[\"lr\"]\n",
    "        EPOCHS = cfg[\"epochs\"]\n",
    "\n",
    "        LR_SCHEDULER = cfg[\"lr_scheduler\"]\n",
    "        LR_KWARGS = cfg[\"lr_scheduler_kwargs\"]\n",
    "\n",
    "        BATCH_SIZE = 16\n",
    "        SINGLE_BATCH_SIZE = 4\n",
    "\n",
    "        alpha = None\n",
    "        if cfg[\"use_class_weights\"]:\n",
    "            alpha = class_weights\n",
    "\n",
    "        gamma = cfg[\"gamma\"]\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{DIR}/fold_{fold}\",\n",
    "            num_train_epochs=EPOCHS,\n",
    "            per_device_train_batch_size=SINGLE_BATCH_SIZE, # use 2 for 14b\n",
    "            per_device_eval_batch_size=16,\n",
    "            eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "            save_strategy=\"steps\", # Save model every 'save_steps'\n",
    "            eval_steps=1/(10*EPOCHS),\n",
    "            save_steps=1/(10*EPOCHS),\n",
    "            save_total_limit=1,\n",
    "            learning_rate=LR_RATE,\n",
    "            metric_for_best_model=\"map@3\",\n",
    "            greater_is_better=True,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f\"{DIR}/logs_fold_{fold}/repeat_{repeat_idx}\",\n",
    "            logging_steps=1/(10*EPOCHS),\n",
    "            report_to=\"tensorboard\",\n",
    "            bf16=True, # TRAIN WITH BF16 IF LOCAL GPU IS NEWER GPU          \n",
    "            fp16=False, # INFER WITH FP16 BECAUSE KAGGLE IS T4 GPU\n",
    "            eval_accumulation_steps=1,\n",
    "            gradient_accumulation_steps=BATCH_SIZE//SINGLE_BATCH_SIZE,\n",
    "            lr_scheduler_type=LR_SCHEDULER,\n",
    "            lr_scheduler_kwargs=LR_KWARGS,\n",
    "            # weight_decay=0.01,\n",
    "        )\n",
    "        trainer = CostSensitiveTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_va,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_map3,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        if TRAIN_MODEL:\n",
    "            # trainer.evaluate()\n",
    "\n",
    "            trainer.train()\n",
    "\n",
    "            final_map = trainer.evaluate()[\"eval_map@3\"]\n",
    "            print(f\"Repeat {repeat_idx} eval/map@3 = {final_map:.6f}\")\n",
    "\n",
    "            if final_map > best_map:\n",
    "                best_map = final_map\n",
    "                save_dir = f\"{DIR}/fold_{fold}/best\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                # Save LoRA adapters\n",
    "                trainer.save_model(save_dir)\n",
    "                # Save label encoder once per fold\n",
    "                joblib.dump(le, f\"{DIR}/fold_{fold}/label_encoder.joblib\")\n",
    "\n",
    "        \n",
    "        # cleanup HF checkpoints if any\n",
    "        \n",
    "        for ckpt in sorted(Path(f\"{DIR}/fold_{fold}\").glob(\"checkpoint-*\")):\n",
    "            shutil.rmtree(ckpt, ignore_errors=True)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8311c-6ca9-42b8-8c8a-5c42ceb4337f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424117d0-3ec8-4213-9fde-a4335ec971f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74886533-9ec8-4fe6-b556-819e9300ae7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7255319-edf3-4438-993a-03aba2fa42d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
