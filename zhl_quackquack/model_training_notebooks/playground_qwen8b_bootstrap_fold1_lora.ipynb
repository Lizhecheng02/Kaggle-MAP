{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Auto-reload imported modules ---\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil, warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    LogitsProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training, TaskType\n",
    "import joblib\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "VER = 54\n",
    "DIR = f\"ver_{VER}\"; os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "# MODEL_NAME = \"./models/Qwen2.5-14B-Instruct\"   # or \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-7B-Instruct\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-Math-7B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# Qwen/Qwen2.5-Math-1.5B-Instruct\n",
    "\n",
    "# LOAD_FROM = \"./ver_38\"\n",
    "LOAD_FROM = None\n",
    "\n",
    "MAX_LEN = 256\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "CV_FOLD = 5\n",
    "CV_SEED = 42\n",
    "USE_SINGLE_FOLD = False\n",
    "EVAL_MODE = \"vote@3\"  # \"vote\" or \"vote@3\" (use this one)\n",
    "\n",
    "TRAIN_CSV = \"./raw_data/train_with_clusters_manual_checked.csv\"\n",
    "TEST_CSV  = \"./raw_data/test.csv\"\n",
    "CLEAN_MISLABEL = \"ignore\"   # ignore | fix | remove\n",
    "\n",
    "N_CLASSES = 65\n",
    "# TODO: Add flash-attention-2\n",
    "# TODO: Train Val Split, some low counts < 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb5fa2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"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\")\n",
    "\n",
    "# # Choose your model Qwen/Qwen2.5-0.5B-Instruct\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-Math-7B-Instruct\"\n",
    "\n",
    "# # Download and save model + tokenizer locally\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mislabel_entries(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"Using {CLEAN_MISLABEL} for data cleaning Strat\")\n",
    "    qid = 31778\n",
    "    correct_answer = r\"\\( 6 \\)\"\n",
    "    rows_to_fix = []\n",
    "    for idx, row in train[train['QuestionId'] == qid].iterrows():\n",
    "        is_correct_answer = row['MC_Answer'] == correct_answer\n",
    "        is_true = str(row['Category']).startswith(\"True\")\n",
    "        if is_correct_answer and not is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "        elif not is_correct_answer and is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "    assert len(rows_to_fix) == 18, \"Expected 18 mislabeled entries to fix, found a different number.\"\n",
    "\n",
    "    if CLEAN_MISLABEL == \"ignore\":\n",
    "        return train\n",
    "    elif CLEAN_MISLABEL == \"remove\":\n",
    "        return train.drop(index=rows_to_fix).reset_index(drop=True)\n",
    "    elif CLEAN_MISLABEL == \"fix\":\n",
    "        for idx in rows_to_fix:\n",
    "            row = train.loc[idx]\n",
    "            cat = str(row['Category']).split(\"_\", 1)[-1]\n",
    "            prefix = \"True\" if row['MC_Answer'] == correct_answer else \"False\"\n",
    "            train.at[idx, 'Category'] = f\"{prefix}_{cat}\"\n",
    "        return train\n",
    "    else:\n",
    "        raise ValueError(\"CLEAN_MISLABEL must be 'ignore', 'remove', or 'fix'\")\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    train = pd.read_csv(TRAIN_CSV)\n",
    "    train = clean_mislabel_entries(train)\n",
    "    train['Misconception'] = train['Misconception'].fillna('NA')\n",
    "    train['target'] = train['Category'] + \":\" + train['Misconception']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train['label'] = le.fit_transform(train['target'])\n",
    "\n",
    "    if \"is_correct\" in train.columns:\n",
    "        train = train.drop(columns = \"is_correct\")\n",
    "    \n",
    "    idx = train['Category'].str.startswith(\"True\")\n",
    "    correct = (\n",
    "        train[idx].groupby(['QuestionId','MC_Answer']).size()\n",
    "        .reset_index(name='c').sort_values('c', ascending=False)\n",
    "        .drop_duplicates(['QuestionId']).assign(is_correct=1)[['QuestionId','MC_Answer','is_correct']]\n",
    "    )\n",
    "\n",
    "    train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    train['is_correct'] = train['is_correct'].fillna(0)\n",
    "\n",
    "    # suppose you also have a QuestionId -> CorrectAnswerText mapping\n",
    "    answers = train.loc[train[\"is_correct\"] == 1, [\"QuestionId\", \"MC_Answer\"]].rename(\n",
    "        columns={\"MC_Answer\": \"TrueAnswer\"}\n",
    "    ).drop_duplicates(['QuestionId'], keep=\"first\")\n",
    "    \n",
    "    train = train.merge(answers, on=\"QuestionId\", how=\"left\")\n",
    "\n",
    "    train[\"split_key\"] = (train['QuestionId'].astype(str) + \"_\" + train['label'].astype(str)).astype('category').cat.codes\n",
    "    return train, le\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"Yes\" if row['is_correct'] else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Student Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\\n\"\n",
    "    )\n",
    "\n",
    "def format_input_v2(row):\n",
    "    x = \"Yes\" if row['is_correct'] else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"True Answer: {row['TrueAnswer']}\\n\"\n",
    "        f\"Student Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\\n\"\n",
    "    )\n",
    "\n",
    "def prepare_dataset(df, tokenizer, cols=['text', 'label']):\n",
    "    df = df[cols].copy().reset_index(drop=True)\n",
    "    df['label'] = df['label'].astype(np.int64)\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "    ds = ds.map(lambda batch: tokenizer(batch['text'], truncation=True, max_length=MAX_LEN), batched=True, remove_columns=['text'])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e25cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Missing tokenizer.pad_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def load_base_model_bf16():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=N_CLASSES,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # model.config.use_cache = False  # better for training/checkpointing\n",
    "    return model\n",
    "\n",
    "def load_base_model_nf4():\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=N_CLASSES,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute MAP@3 ---\n",
    "def compute_map3(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    match = (top3 == labels[:, None])\n",
    "    map3 = np.mean([1 if m[0] else 0.5 if m[1] else 1/3 if m[2] else 0 for m in match])\n",
    "    return {\"map@3\": map3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d1c85-8e0b-4ece-8085-9e1f8fba6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# --- Beta Schedules ---\n",
    "def linear_beta(step, total_steps, start=1.0, end=0.6):\n",
    "    frac = step / max(1, total_steps)\n",
    "    return start + (end - start) * frac\n",
    "\n",
    "def cosine_beta(step, total_steps, start=1.0, end=0.6):\n",
    "    cos = (1 + math.cos(math.pi * step / total_steps)) / 2\n",
    "    return end + (start - end) * cos\n",
    "\n",
    "def step_beta(step, total_steps, warmup_frac=0.1, start=1.0, mid=0.7, end=0.3):\n",
    "    warmup_steps = int(total_steps * warmup_frac)\n",
    "    if step < warmup_steps:\n",
    "        return start  # CE warmup\n",
    "    else:\n",
    "        frac = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
    "        return mid + (end - mid) * frac\n",
    "\n",
    "# --- Bootstrap Trainer ---\n",
    "class BootstrapTrainer(Trainer):\n",
    "    def __init__(self, *args, beta_schedule=\"linear\", start=1.0, mid=0.7, end=0.3, warmup_frac=0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beta_schedule = beta_schedule\n",
    "        self.start = start\n",
    "        self.mid = mid\n",
    "        self.end = end\n",
    "        self.warmup_frac = warmup_frac\n",
    "\n",
    "    def compute_loss(self,\n",
    "        model: nn.Module,\n",
    "        inputs: dict[str, torch.Tensor],\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch=None,\n",
    "    ):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # soft_labels = inputs.pop(\"soft_labels\")  # distribution (for training)\n",
    "        # row_ids = inputs.pop(\"row_id\")\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # --- Step + Schedule ---\n",
    "        step = self.state.global_step\n",
    "        total_steps = max(1, self.state.max_steps)\n",
    "\n",
    "        if self.beta_schedule == \"linear\":\n",
    "            beta = linear_beta(step, total_steps, start=self.start, end=self.end)\n",
    "        elif self.beta_schedule == \"cosine\":\n",
    "            beta = cosine_beta(step, total_steps, start=self.start, end=self.end)\n",
    "        else:  # step decay\n",
    "            beta = step_beta(step, total_steps, warmup_frac=self.warmup_frac,\n",
    "                             start=self.start, mid=self.mid, end=self.end)\n",
    "\n",
    "        # --- Bootstrapped Loss ---\n",
    "        one_hot = torch.zeros_like(probs).scatter_(1, labels.unsqueeze(1), 1)\n",
    "        boot_targets = beta * one_hot + (1 - beta) * probs.detach()\n",
    "        loss = -(boot_targets * torch.log(probs)).sum(dim=1).mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b891f-89c0-4e4b-becb-600e713f7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# V50\n",
    "param_sets = [\n",
    "    dict( # best 1,  LOAD_FROM = None\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.7,\n",
    "        end=0.3,\n",
    "        warmup_frac=0.5,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"linear\",\n",
    "        start=1.0,\n",
    "        mid=None,\n",
    "        end=0.5,\n",
    "        warmup_frac=None,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"cosine\",\n",
    "        start=1.0,\n",
    "        mid=None,\n",
    "        end=0.5,\n",
    "        warmup_frac=None,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v51\n",
    "param_sets = [\n",
    "    dict( # best 2, LOAD_FROM = None\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.5,\n",
    "        end=0.5,\n",
    "        warmup_frac=0.5,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=3,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.7,\n",
    "        end=0.3,\n",
    "        warmup_frac=0.66,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=4,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.7,\n",
    "        end=0.3,\n",
    "        warmup_frac=0.5,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v52\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1e-5,\n",
    "        dropout=0.05,\n",
    "        epochs=1,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"linear\",\n",
    "        start=0.5,\n",
    "        mid=0.5,\n",
    "        end=0.5,\n",
    "        warmup_frac=0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-5,\n",
    "        dropout=0.05,\n",
    "        epochs=1,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"linear\",\n",
    "        start=0.5,\n",
    "        mid=0.5,\n",
    "        end=0.5,\n",
    "        warmup_frac=0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=5e-5,\n",
    "        dropout=0.05,\n",
    "        epochs=1,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"linear\",\n",
    "        start=0.5,\n",
    "        mid=0.5,\n",
    "        end=0.5,\n",
    "        warmup_frac=0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v53 LOAD_FROM = None\n",
    "param_sets = [\n",
    "    # dict( # best 1,  LOAD_FROM = None\n",
    "    #     name=\"qv_r16_alpha32_e2\",\n",
    "    #     target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    #     r=16, lora_alpha=32,   # α/r = 4\n",
    "    #     lr=2.5e-4,\n",
    "    #     dropout=0.05,\n",
    "    #     epochs=2,\n",
    "    #     lr_scheduler = \"linear\",\n",
    "    #     lr_scheduler_kwargs = {},\n",
    "    #     beta_schedule = \"step\",\n",
    "    #     start=1.0,\n",
    "    #     mid=0.7,\n",
    "    #     end=0.3,\n",
    "    #     warmup_frac=0.5,\n",
    "    # ),\n",
    "    # dict( # best 2, LOAD_FROM = None\n",
    "    #     name=\"qv_r16_alpha32_e2\",\n",
    "    #     target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    #     r=16, lora_alpha=32,   # α/r = 4\n",
    "    #     lr=2.5e-4,\n",
    "    #     dropout=0.05,\n",
    "    #     epochs=2,\n",
    "    #     lr_scheduler = \"linear\",\n",
    "    #     lr_scheduler_kwargs = {},\n",
    "    #     beta_schedule = \"step\",\n",
    "    #     start=1.0,\n",
    "    #     mid=0.5,\n",
    "    #     end=0.5,\n",
    "    #     warmup_frac=0.5,\n",
    "    # ),\n",
    "    dict( \n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.3,\n",
    "        end=0.3,\n",
    "        warmup_frac=0.5,\n",
    "    ),\n",
    "    dict( \n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.5,\n",
    "        end=0.1,\n",
    "        warmup_frac=0.5,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7203c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v54 LOAD_FROM = None\n",
    "param_sets = [\n",
    "    dict( # best 1,  LOAD_FROM = None\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.7,\n",
    "        end=0.3,\n",
    "        warmup_frac=0.5,\n",
    "    ),\n",
    "    dict( # best 2, LOAD_FROM = None\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        beta_schedule = \"step\",\n",
    "        start=1.0,\n",
    "        mid=0.5,\n",
    "        end=0.5,\n",
    "        warmup_frac=0.5,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "\n",
    "\n",
    "train_df, le = load_and_preprocess_data()\n",
    "n_classes = train_df['label'].nunique()\n",
    "print(f\"Total of {n_classes} classes.\")\n",
    "train_df['text'] = train_df.apply(format_input, axis=1)\n",
    "\n",
    "# get soft label distribution\n",
    "# dists = build_label_distributions(train_df)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CV_FOLD, shuffle=True, random_state=CV_SEED)\n",
    "fold_indices = list(skf.split(train_df, train_df['split_key']))\n",
    "if USE_SINGLE_FOLD:\n",
    "    fold_indices = [fold_indices[0]]\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# data_collator = DataCollatorWithSoftLabels(tokenizer=tokenizer)\n",
    "\n",
    "for fold in range(len(fold_indices)):\n",
    "    tr_idx, va_idx = fold_indices[fold]\n",
    "    tr, va = train_df.iloc[tr_idx].copy(), train_df.iloc[va_idx].copy()\n",
    "\n",
    "    # Get Class Weights\n",
    "    class_counts = np.bincount(tr['label'])\n",
    "    safe_counts = np.where(class_counts > 0, class_counts, 1)\n",
    "    weights = class_counts.max() / safe_counts\n",
    "    weights = np.sqrt(weights)\n",
    "    weights = weights / weights.sum() * len(class_counts)  # normalize around #classes\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    # get soft label distribution\n",
    "    dists = None\n",
    "\n",
    "    ds_tr = prepare_dataset(tr, tokenizer)\n",
    "    ds_va = prepare_dataset(va, tokenizer)\n",
    "    \n",
    "    best_map = -1.0\n",
    "    for repeat_idx in range(len(param_sets)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{CV_FOLD} REPEAT {repeat_idx} ===\")\n",
    "\n",
    "        cfg = param_sets[repeat_idx]\n",
    "        print(f\"Trying {cfg['name']}\")\n",
    "\n",
    "        # LoRA config for this repeat\n",
    "        lora_config = LoraConfig(\n",
    "            r=cfg[\"r\"],\n",
    "            lora_alpha=cfg[\"lora_alpha\"],\n",
    "            target_modules=cfg[\"target_modules\"],\n",
    "            lora_dropout=cfg[\"dropout\"],\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,  # Or CAUSAL_LM, etc. depending on model\n",
    "            modules_to_save=[\"classifier\", \"score\"],\n",
    "            inference_mode=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            del model\n",
    "            del trainer\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # model = load_base_model_nf4()\n",
    "        model = load_base_model_bf16()\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "        # if LOAD_FROM is None:\n",
    "        #     model = get_peft_model(model, lora_config)\n",
    "        # else:\n",
    "        #     lora_pretrained_path = os.path.join(LOAD_FROM, f\"fold_{fold}\", \"best\")\n",
    "        #     print(f\"Loading Pretrained Lora Weights from {lora_pretrained_path}\")\n",
    "        #     model = PeftModel.from_pretrained(model, lora_pretrained_path)\n",
    "\n",
    "        if model.config.pad_token_id is None:\n",
    "            print(f\"Missig model.config.pad_token_id\")\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # model = PeftModel.from_pretrained(model, \"./ver_10/fold_0/checkpoint-917/\")\n",
    "\n",
    "        LR_RATE = cfg[\"lr\"]\n",
    "        EPOCHS = cfg[\"epochs\"]\n",
    "\n",
    "        LR_SCHEDULER = cfg[\"lr_scheduler\"]\n",
    "        LR_KWARGS = cfg[\"lr_scheduler_kwargs\"]\n",
    "\n",
    "        BATCH_SIZE = 16\n",
    "        SINGLE_BATCH_SIZE = 4\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{DIR}/fold_{fold}\",\n",
    "            num_train_epochs=EPOCHS,\n",
    "            per_device_train_batch_size=SINGLE_BATCH_SIZE, # use 2 for 14b\n",
    "            per_device_eval_batch_size=16,\n",
    "            eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "            save_strategy=\"steps\", # Save model every 'save_steps'\n",
    "            eval_steps=1/(5*EPOCHS),\n",
    "            save_steps=1/(5*EPOCHS),\n",
    "            save_total_limit=1,\n",
    "            learning_rate=LR_RATE,\n",
    "            metric_for_best_model=\"map@3\",\n",
    "            greater_is_better=True,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f\"{DIR}/logs_fold_{fold}/repeat_{repeat_idx}\",\n",
    "            logging_steps=1/(5*EPOCHS),\n",
    "            report_to=\"tensorboard\",\n",
    "            bf16=True, # TRAIN WITH BF16 IF LOCAL GPU IS NEWER GPU          \n",
    "            fp16=False, # INFER WITH FP16 BECAUSE KAGGLE IS T4 GPU\n",
    "            eval_accumulation_steps=1,\n",
    "            gradient_accumulation_steps=BATCH_SIZE//SINGLE_BATCH_SIZE,\n",
    "            lr_scheduler_type=LR_SCHEDULER,\n",
    "            lr_scheduler_kwargs=LR_KWARGS,\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "        trainer = BootstrapTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_va,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_map3,\n",
    "            beta_schedule=cfg[\"beta_schedule\"],\n",
    "            start=cfg[\"start\"],\n",
    "            mid=cfg[\"mid\"],\n",
    "            end=cfg[\"end\"],\n",
    "            warmup_frac=cfg[\"warmup_frac\"],\n",
    "        )\n",
    "\n",
    "        if TRAIN_MODEL:\n",
    "\n",
    "            trainer.train()\n",
    "\n",
    "            final_map = trainer.evaluate()[\"eval_map@3\"]\n",
    "            print(f\"Repeat {repeat_idx} eval/map@3 = {final_map:.6f}\")\n",
    "\n",
    "            if final_map > best_map:\n",
    "                best_map = final_map\n",
    "                save_dir = f\"{DIR}/fold_{fold}/best\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                # Save LoRA adapters\n",
    "                trainer.save_model(save_dir)\n",
    "                # Save label encoder once per fold\n",
    "                joblib.dump(le, f\"{DIR}/fold_{fold}/label_encoder.joblib\")\n",
    "\n",
    "        \n",
    "        # cleanup HF checkpoints if any\n",
    "        for ckpt in sorted(Path(f\"{DIR}/fold_{fold}\").glob(\"checkpoint-*\")):\n",
    "            shutil.rmtree(ckpt, ignore_errors=True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424117d0-3ec8-4213-9fde-a4335ec971f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7255319-edf3-4438-993a-03aba2fa42d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a95d7f-1b12-4e24-9e58-8e601986b113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
