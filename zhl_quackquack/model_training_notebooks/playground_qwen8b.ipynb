{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0c1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Auto-reload imported modules ---\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil, warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    LogitsProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training, TaskType\n",
    "import joblib\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "VER = 38\n",
    "DIR = f\"ver_{VER}\"; os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"./models/Qwen2.5-14B-Instruct\"   # or \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-7B-Instruct\"\n",
    "MODEL_NAME = \"./models/Qwen2.5-Math-7B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# Qwen/Qwen2.5-Math-1.5B-Instruct\n",
    "\n",
    "MAX_LEN = 256\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "CV_FOLD = 5\n",
    "CV_SEED = 42\n",
    "USE_SINGLE_FOLD = False\n",
    "EVAL_MODE = \"vote@3\"  # \"vote\" or \"vote@3\" (use this one)\n",
    "\n",
    "TRAIN_CSV = \"./raw_data/train.csv\"\n",
    "TEST_CSV  = \"./raw_data/test.csv\"\n",
    "CLEAN_MISLABEL = \"ignore\"   # ignore | fix | remove\n",
    "\n",
    "N_CLASSES = 65\n",
    "# TODO: Add flash-attention-2\n",
    "# TODO: Train Val Split, some low counts < 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecb5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"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\")\n",
    "\n",
    "# # Choose your model Qwen/Qwen2.5-0.5B-Instruct\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-Math-7B-Instruct\"\n",
    "\n",
    "# # Download and save model + tokenizer locally\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7b2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mislabel_entries(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"Using {CLEAN_MISLABEL} for data cleaning Strat\")\n",
    "    qid = 31778\n",
    "    correct_answer = r\"\\( 6 \\)\"\n",
    "    rows_to_fix = []\n",
    "    for idx, row in train[train['QuestionId'] == qid].iterrows():\n",
    "        is_correct_answer = row['MC_Answer'] == correct_answer\n",
    "        is_true = str(row['Category']).startswith(\"True\")\n",
    "        if is_correct_answer and not is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "        elif not is_correct_answer and is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "    assert len(rows_to_fix) == 18, \"Expected 18 mislabeled entries to fix, found a different number.\"\n",
    "\n",
    "    if CLEAN_MISLABEL == \"ignore\":\n",
    "        return train\n",
    "    elif CLEAN_MISLABEL == \"remove\":\n",
    "        return train.drop(index=rows_to_fix).reset_index(drop=True)\n",
    "    elif CLEAN_MISLABEL == \"fix\":\n",
    "        for idx in rows_to_fix:\n",
    "            row = train.loc[idx]\n",
    "            cat = str(row['Category']).split(\"_\", 1)[-1]\n",
    "            prefix = \"True\" if row['MC_Answer'] == correct_answer else \"False\"\n",
    "            train.at[idx, 'Category'] = f\"{prefix}_{cat}\"\n",
    "        return train\n",
    "    else:\n",
    "        raise ValueError(\"CLEAN_MISLABEL must be 'ignore', 'remove', or 'fix'\")\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    train = pd.read_csv(TRAIN_CSV)\n",
    "    train = clean_mislabel_entries(train)\n",
    "    train['Misconception'] = train['Misconception'].fillna('NA')\n",
    "    train['target'] = train['Category'] + \":\" + train['Misconception']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train['label'] = le.fit_transform(train['target'])\n",
    "\n",
    "    idx = train['Category'].str.startswith(\"True\")\n",
    "    correct = (\n",
    "        train[idx].groupby(['QuestionId','MC_Answer']).size()\n",
    "        .reset_index(name='c').sort_values('c', ascending=False)\n",
    "        .drop_duplicates(['QuestionId']).assign(is_correct=1)[['QuestionId','MC_Answer','is_correct']]\n",
    "    )\n",
    "    train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    train['is_correct'] = train['is_correct'].fillna(0)\n",
    "\n",
    "    # suppose you also have a QuestionId -> CorrectAnswerText mapping\n",
    "    answers = train.loc[train[\"is_correct\"] == 1, [\"QuestionId\", \"MC_Answer\"]].rename(\n",
    "        columns={\"MC_Answer\": \"TrueAnswer\"}\n",
    "    ).drop_duplicates(['QuestionId'], keep=\"first\")\n",
    "    \n",
    "    train = train.merge(answers, on=\"QuestionId\", how=\"left\")\n",
    "\n",
    "    train[\"split_key\"] = (train['QuestionId'].astype(str) + \"_\" + train['label'].astype(str)).astype('category').cat.codes\n",
    "    return train, le\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"Yes\" if row['is_correct'] else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Student Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\\n\"\n",
    "    )\n",
    "\n",
    "def format_input_v2(row):\n",
    "    x = \"Yes\" if row['is_correct'] else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"True Answer: {row['TrueAnswer']}\\n\"\n",
    "        f\"Student Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\\n\"\n",
    "    )\n",
    "\n",
    "def prepare_dataset(df, tokenizer, cols=['text', 'label']):\n",
    "    df = df[cols].copy().reset_index(drop=True)\n",
    "    df['label'] = df['label'].astype(np.int64)\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "    ds = ds.map(lambda batch: tokenizer(batch['text'], truncation=True, max_length=MAX_LEN), batched=True, remove_columns=['text'])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e25cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Missing tokenizer.pad_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def load_base_model_bf16():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=N_CLASSES,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # model.config.use_cache = False  # better for training/checkpointing\n",
    "    return model\n",
    "\n",
    "def load_base_model_nf4():\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=N_CLASSES,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfacdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute MAP@3 ---\n",
    "def compute_map3(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    match = (top3 == labels[:, None])\n",
    "    map3 = np.mean([1 if m[0] else 0.5 if m[1] else 1/3 if m[2] else 0 for m in match])\n",
    "    return {\"map@3\": map3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3839a52-88d9-4c82-b568-e78d7dd6a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, weight=self.alpha, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)   # = softmax prob of the true class\n",
    "        focal = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal.sum()\n",
    "        return focal\n",
    "\n",
    "# 1) Cost Sensitive Loss v1\n",
    "class CostSensitiveTrainer(Trainer):\n",
    "    def __init__(self, alpha=None, gamma=2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if alpha is not None:\n",
    "            self.alpha = alpha.to(self.model.device)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: dict[str, torch.Tensor],\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch=None,\n",
    "    ):\n",
    "        # extract labels\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # compute cross-entropy\n",
    "        loss_fn = FocalLoss(alpha=self.alpha, gamma=self.gamma)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08002e88-7d9b-46d5-8561-3c80c8eacc99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# v21 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=3,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\", \"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=3,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v22 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=3e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=3.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v23 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=3,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v27 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=3e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict( # Good\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=3.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict( \n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict( # Best\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v28 save for 7b \n",
    "param_sets = [\n",
    "    dict( # maybe optimal for this target_modules\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=4e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.5e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v29 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.75e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.25e-4,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# v30 save for 7b \n",
    "param_sets = [\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.25e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5e77ef-284b-4061-b0c3-4299f72478b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# v31\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 1,\n",
    "    ),\n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=32, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7203c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ignore for data cleaning Strat\n",
      "Total of 65 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9959b41f429148209625183b9ee98f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69809bb46cb4a10a07c65902dc7d85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 REPEAT 0 ===\n",
      "Trying qv_r16_alpha32_e2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876b9f4fbafe438f9d87a8bc7f4e04db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missig model.config.pad_token_id\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3670' max='3670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3670/3670 1:29:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Map@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>0.913100</td>\n",
       "      <td>0.649315</td>\n",
       "      <td>0.874046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.492166</td>\n",
       "      <td>0.908311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.400437</td>\n",
       "      <td>0.924364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1468</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.358815</td>\n",
       "      <td>0.936035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.346315</td>\n",
       "      <td>0.935490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2202</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>0.345041</td>\n",
       "      <td>0.941326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2569</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.322636</td>\n",
       "      <td>0.942371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2936</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.333762</td>\n",
       "      <td>0.944755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3303</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.300311</td>\n",
       "      <td>0.946730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.301552</td>\n",
       "      <td>0.947389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 01:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 eval/map@3 = 0.947389\n",
      "\n",
      "=== Fold 1/5 REPEAT 1 ===\n",
      "Trying qv_r16_alpha32_e2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2bba1ddfe941999623217a13e57740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missig model.config.pad_token_id\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3670' max='3670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3670/3670 1:28:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Map@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.513984</td>\n",
       "      <td>0.905154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>0.482507</td>\n",
       "      <td>0.912511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.427048</td>\n",
       "      <td>0.917961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1468</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>0.391397</td>\n",
       "      <td>0.929178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.355431</td>\n",
       "      <td>0.935559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2202</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.340854</td>\n",
       "      <td>0.942620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2569</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.336676</td>\n",
       "      <td>0.941076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2936</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.336400</td>\n",
       "      <td>0.943279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3303</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.308756</td>\n",
       "      <td>0.943710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.306025</td>\n",
       "      <td>0.946163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 01:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1 eval/map@3 = 0.946163\n",
      "\n",
      "=== Fold 1/5 REPEAT 2 ===\n",
      "Trying qv_r16_alpha32_e2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e3d778a1144c28ad3993aa1340ce52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missig model.config.pad_token_id\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1270' max='3670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1270/3670 29:40 < 56:10, 0.71 it/s, Epoch 0.69/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Map@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>0.908600</td>\n",
       "      <td>0.543272</td>\n",
       "      <td>0.903633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.529510</td>\n",
       "      <td>0.907811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.439321</td>\n",
       "      <td>0.917620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# V33\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.1e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.3e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V34\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.8e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.9e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V35\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.8e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.2e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V37\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=3e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=1.6e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# V38\n",
    "param_sets = [\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.4e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=16, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.6e-4,\n",
    "        dropout=0.05,\n",
    "        epochs=2,\n",
    "        lr_scheduler = \"linear\",\n",
    "        lr_scheduler_kwargs = {},\n",
    "        use_class_weights = False,\n",
    "        gamma = 0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "\n",
    "\n",
    "train_df, le = load_and_preprocess_data()\n",
    "n_classes = train_df['label'].nunique()\n",
    "print(f\"Total of {n_classes} classes.\")\n",
    "train_df['text'] = train_df.apply(format_input, axis=1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CV_FOLD, shuffle=True, random_state=CV_SEED)\n",
    "fold_indices = list(skf.split(train_df, train_df['split_key']))\n",
    "if USE_SINGLE_FOLD:\n",
    "    fold_indices = [fold_indices[0]]\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "for fold in range(len(fold_indices)):\n",
    "    tr_idx, va_idx = fold_indices[fold]\n",
    "    tr, va = train_df.iloc[tr_idx].copy(), train_df.iloc[va_idx].copy()\n",
    "\n",
    "    # Get Class Weights\n",
    "    class_counts = np.bincount(tr['label'])\n",
    "    safe_counts = np.where(class_counts > 0, class_counts, 1)\n",
    "    weights = class_counts.max() / safe_counts\n",
    "    weights = np.sqrt(weights)\n",
    "    weights = weights / weights.sum() * len(class_counts)  # normalize around #classes\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    ds_tr = prepare_dataset(tr, tokenizer)\n",
    "    ds_va = prepare_dataset(va, tokenizer)\n",
    "    \n",
    "    best_map = -1.0\n",
    "    for repeat_idx in range(len(param_sets)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{CV_FOLD} REPEAT {repeat_idx} ===\")\n",
    "\n",
    "        cfg = param_sets[repeat_idx]\n",
    "        print(f\"Trying {cfg['name']}\")\n",
    "\n",
    "        # LoRA config for this repeat\n",
    "        lora_config = LoraConfig(\n",
    "            r=cfg[\"r\"],\n",
    "            lora_alpha=cfg[\"lora_alpha\"],\n",
    "            target_modules=cfg[\"target_modules\"],\n",
    "            lora_dropout=cfg[\"dropout\"],\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,  # Or CAUSAL_LM, etc. depending on model\n",
    "            modules_to_save=[\"classifier\", \"score\"],\n",
    "            inference_mode=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            del model\n",
    "            del trainer\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # model = load_base_model_nf4()\n",
    "        model = load_base_model_bf16()\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        if model.config.pad_token_id is None:\n",
    "            print(f\"Missig model.config.pad_token_id\")\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # model = PeftModel.from_pretrained(model, \"./ver_10/fold_0/checkpoint-917/\")\n",
    "\n",
    "        LR_RATE = cfg[\"lr\"]\n",
    "        EPOCHS = cfg[\"epochs\"]\n",
    "\n",
    "        LR_SCHEDULER = cfg[\"lr_scheduler\"]\n",
    "        LR_KWARGS = cfg[\"lr_scheduler_kwargs\"]\n",
    "\n",
    "        BATCH_SIZE = 16\n",
    "        SINGLE_BATCH_SIZE = 4\n",
    "\n",
    "        alpha = None\n",
    "        if cfg[\"use_class_weights\"]:\n",
    "            alpha = class_weights\n",
    "\n",
    "        gamma = cfg[\"gamma\"]\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{DIR}/fold_{fold}\",\n",
    "            num_train_epochs=EPOCHS,\n",
    "            per_device_train_batch_size=SINGLE_BATCH_SIZE, # use 2 for 14b\n",
    "            per_device_eval_batch_size=16,\n",
    "            eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "            save_strategy=\"steps\", # Save model every 'save_steps'\n",
    "            eval_steps=1/(5*EPOCHS),\n",
    "            save_steps=1/(5*EPOCHS),\n",
    "            save_total_limit=1,\n",
    "            learning_rate=LR_RATE,\n",
    "            metric_for_best_model=\"map@3\",\n",
    "            greater_is_better=True,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f\"{DIR}/logs_fold_{fold}/repeat_{repeat_idx}\",\n",
    "            logging_steps=1/(5*EPOCHS),\n",
    "            report_to=\"tensorboard\",\n",
    "            bf16=True, # TRAIN WITH BF16 IF LOCAL GPU IS NEWER GPU          \n",
    "            fp16=False, # INFER WITH FP16 BECAUSE KAGGLE IS T4 GPU\n",
    "            eval_accumulation_steps=1,\n",
    "            gradient_accumulation_steps=BATCH_SIZE//SINGLE_BATCH_SIZE,\n",
    "            lr_scheduler_type=LR_SCHEDULER,\n",
    "            lr_scheduler_kwargs=LR_KWARGS,\n",
    "        )\n",
    "        trainer = CostSensitiveTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_va,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_map3,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        if TRAIN_MODEL:\n",
    "            # trainer.evaluate()\n",
    "\n",
    "            trainer.train()\n",
    "\n",
    "            final_map = trainer.evaluate()[\"eval_map@3\"]\n",
    "            print(f\"Repeat {repeat_idx} eval/map@3 = {final_map:.6f}\")\n",
    "\n",
    "            if final_map > best_map:\n",
    "                best_map = final_map\n",
    "                save_dir = f\"{DIR}/fold_{fold}/best\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                # Save LoRA adapters\n",
    "                trainer.save_model(save_dir)\n",
    "                # Save label encoder once per fold\n",
    "                joblib.dump(le, f\"{DIR}/fold_{fold}/label_encoder.joblib\")\n",
    "\n",
    "        \n",
    "        # cleanup HF checkpoints if any\n",
    "        for ckpt in sorted(Path(f\"{DIR}/fold_{fold}\").glob(\"checkpoint-*\")):\n",
    "            shutil.rmtree(ckpt, ignore_errors=True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8311c-6ca9-42b8-8c8a-5c42ceb4337f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424117d0-3ec8-4213-9fde-a4335ec971f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74886533-9ec8-4fe6-b556-819e9300ae7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7255319-edf3-4438-993a-03aba2fa42d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
