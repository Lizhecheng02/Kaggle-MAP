{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0c1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Auto-reload imported modules ---\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil, warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    LogitsProcessor,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "VER = 10\n",
    "DIR = f\"ver_{VER}\"; os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"./models/Qwen2.5-14B-Instruct\"   # or \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# MODEL_NAME = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "MAX_LEN = 1610\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "CV_FOLD = 5\n",
    "CV_SEED = 42\n",
    "USE_SINGLE_FOLD = True\n",
    "EVAL_MODE = \"vote@3\"  # \"vote\" or \"vote@3\" (use this one)\n",
    "\n",
    "TRAIN_CSV = \"./raw_data/train.csv\"\n",
    "TEST_CSV  = \"./raw_data/test.csv\"\n",
    "CLEAN_MISLABEL = \"ignore\"   # ignore | fix | remove\n",
    "\n",
    "# TODO: Add flash-attention-2\n",
    "# TODO: Train Val Split, some low counts < 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecb5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"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\")\n",
    "\n",
    "# # Choose your model Qwen/Qwen2.5-0.5B-Instruct\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# save_path = \"./models/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# # Download and save model + tokenizer locally\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aec3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_list = [\n",
    "    '■','□','▲','△','▼','▽','◆','◇','○','●','★','☆','♦','♥','♠','♣',\n",
    "    '§','†','‡','※','∞','±','≠','≈','√','∑','∏','∆','Ω','μ','∂',\n",
    "    '→','←','↑','↓','↔','↕','〈','〉','『','』','│','─','┌','┐','└','┘','┼','█','▓','▒',\n",
    "    '£','¥','€','₩','©','®','™','♪','♫','☀','☁','☂','☃','☎'\n",
    "]\n",
    "\n",
    "target_values = ['False_Correct:NA',\n",
    " 'False_Misconception:Adding_across',\n",
    " 'False_Misconception:Adding_terms',\n",
    " 'False_Misconception:Additive',\n",
    " 'False_Misconception:Base_rate',\n",
    " 'False_Misconception:Certainty',\n",
    " 'False_Misconception:Definition',\n",
    " 'False_Misconception:Denominator-only_change',\n",
    " 'False_Misconception:Division',\n",
    " 'False_Misconception:Duplication',\n",
    " 'False_Misconception:Firstterm',\n",
    " 'False_Misconception:FlipChange',\n",
    " 'False_Misconception:Ignores_zeroes',\n",
    " 'False_Misconception:Incomplete',\n",
    " 'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    " 'False_Misconception:Interior',\n",
    " 'False_Misconception:Inverse_operation',\n",
    " 'False_Misconception:Inversion',\n",
    " 'False_Misconception:Irrelevant',\n",
    " 'False_Misconception:Longer_is_bigger',\n",
    " 'False_Misconception:Mult',\n",
    " 'False_Misconception:Multiplying_by_4',\n",
    " 'False_Misconception:Not_variable',\n",
    " 'False_Misconception:Positive',\n",
    " 'False_Misconception:Scale',\n",
    " 'False_Misconception:Shorter_is_bigger',\n",
    " 'False_Misconception:Subtraction',\n",
    " 'False_Misconception:SwapDividend',\n",
    " 'False_Misconception:Tacking',\n",
    " 'False_Misconception:Unknowable',\n",
    " 'False_Misconception:WNB',\n",
    " 'False_Misconception:Whole_numbers_larger',\n",
    " 'False_Misconception:Wrong_Fraction',\n",
    " 'False_Misconception:Wrong_Operation',\n",
    " 'False_Misconception:Wrong_fraction',\n",
    " 'False_Misconception:Wrong_term',\n",
    " 'False_Neither:NA',\n",
    " 'True_Correct:NA',\n",
    " 'True_Misconception:Adding_across',\n",
    " 'True_Misconception:Additive',\n",
    " 'True_Misconception:Base_rate',\n",
    " 'True_Misconception:Definition',\n",
    " 'True_Misconception:Denominator-only_change',\n",
    " 'True_Misconception:Division',\n",
    " 'True_Misconception:Duplication',\n",
    " 'True_Misconception:Firstterm',\n",
    " 'True_Misconception:FlipChange',\n",
    " 'True_Misconception:Incomplete',\n",
    " 'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    " 'True_Misconception:Inversion',\n",
    " 'True_Misconception:Irrelevant',\n",
    " 'True_Misconception:Longer_is_bigger',\n",
    " 'True_Misconception:Mult',\n",
    " 'True_Misconception:Multiplying_by_4',\n",
    " 'True_Misconception:Not_variable',\n",
    " 'True_Misconception:Positive',\n",
    " 'True_Misconception:Shorter_is_bigger',\n",
    " 'True_Misconception:Subtraction',\n",
    " 'True_Misconception:SwapDividend',\n",
    " 'True_Misconception:Tacking',\n",
    " 'True_Misconception:WNB',\n",
    " 'True_Misconception:Whole_numbers_larger',\n",
    " 'True_Misconception:Wrong_fraction',\n",
    " 'True_Misconception:Wrong_term',\n",
    " 'True_Neither:NA']\n",
    "\n",
    "target_token_map = {label: char for label, char in zip(target_values, special_character_list)}\n",
    "token_target_map = {v: k for k, v in target_token_map.items()}\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are now tasked with analyzing math problems and classifying student responses. Given a math problem, the student's chosen answer, whether it's correct, and the student's explanation, you need to determine the appropriate Category and Misconception classification.\n",
    "Below are the available Category:Misconception classifications you can choose from.\n",
    "\n",
    "Your job is to output exactly ONE classification token from the allowed set below.\n",
    "\n",
    "OUTPUT RULES (READ CAREFULLY):\n",
    "1) Your entire reply must be exactly one character: a single token from the allowed set.\n",
    "2) Do NOT output any words, labels, punctuation, quotes, spaces, or newlines.\n",
    "3) Do NOT explain your reasoning or restate the problem.\n",
    "4) Choose the token that best matches the Category:Misconception for the given input.\n",
    "\n",
    "ALLOWED OUTPUT TOKENS (token : meaning):\n",
    "\"\"\" + '\\n'.join([f\"{k} : {v}\" for k, v in token_target_map.items() ]) +\"\"\"\n",
    "\n",
    "Please analyze the given input and provide your classification.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7b2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mislabel_entries(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"Using {CLEAN_MISLABEL} for data cleaning Strat\")\n",
    "    qid = 31778\n",
    "    correct_answer = r\"\\( 6 \\)\"\n",
    "    rows_to_fix = []\n",
    "    for idx, row in train[train['QuestionId'] == qid].iterrows():\n",
    "        is_correct_answer = row['MC_Answer'] == correct_answer\n",
    "        is_true = str(row['Category']).startswith(\"True\")\n",
    "        if is_correct_answer and not is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "        elif not is_correct_answer and is_true:\n",
    "            rows_to_fix.append(idx)\n",
    "    assert len(rows_to_fix) == 18, \"Expected 18 mislabeled entries to fix, found a different number.\"\n",
    "\n",
    "    if CLEAN_MISLABEL == \"ignore\":\n",
    "        return train\n",
    "    elif CLEAN_MISLABEL == \"remove\":\n",
    "        return train.drop(index=rows_to_fix).reset_index(drop=True)\n",
    "    elif CLEAN_MISLABEL == \"fix\":\n",
    "        for idx in rows_to_fix:\n",
    "            row = train.loc[idx]\n",
    "            cat = str(row['Category']).split(\"_\", 1)[-1]\n",
    "            prefix = \"True\" if row['MC_Answer'] == correct_answer else \"False\"\n",
    "            train.at[idx, 'Category'] = f\"{prefix}_{cat}\"\n",
    "        return train\n",
    "    else:\n",
    "        raise ValueError(\"CLEAN_MISLABEL must be 'ignore', 'remove', or 'fix'\")\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    train = pd.read_csv(TRAIN_CSV)\n",
    "    train = clean_mislabel_entries(train)\n",
    "    train['Misconception'] = train['Misconception'].fillna('NA')\n",
    "    train['target'] = train['Category'] + \":\" + train['Misconception']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train['label'] = le.fit_transform(train['target'])\n",
    "\n",
    "    idx = train['Category'].str.startswith(\"True\")\n",
    "    correct = (\n",
    "        train[idx].groupby(['QuestionId','MC_Answer']).size()\n",
    "        .reset_index(name='c').sort_values('c', ascending=False)\n",
    "        .drop_duplicates(['QuestionId']).assign(is_correct=1)[['QuestionId','MC_Answer','is_correct']]\n",
    "    )\n",
    "    train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    train['is_correct'] = train['is_correct'].fillna(0)\n",
    "\n",
    "    train[\"split_key\"] = (train['QuestionId'].astype(str) + \"_\" + train['label'].astype(str)).astype('category').cat.codes\n",
    "    return train, le\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"Yes\" if row['is_correct'] else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Student Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "def preprocess_function_conversational_prompt_completion(row):\n",
    "    assiant_token = target_token_map[ row['target'] ]\n",
    "    return {\n",
    "        \"prompt\": [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "                   {\"role\": \"user\", \"content\": format_input(row)}],\n",
    "        \"completion\": [\n",
    "            {\"role\": \"assistant\", \"content\": assiant_token}\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e25cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # important for causal LM\n",
    "\n",
    "allowed_token_ids = [tokenizer.encode(str(i), add_special_tokens=False)[0] for i in special_character_list]\n",
    "\n",
    "# Make sure each special symbol is a single token for this tokenizer\n",
    "special_token_ids = []\n",
    "for ch in special_character_list:\n",
    "    ids = tokenizer.encode(ch, add_special_tokens=False)\n",
    "    assert len(ids) == 1, f\"Symbol {ch} is not a single token with this tokenizer.\"\n",
    "    special_token_ids.append(ids[0])\n",
    "\n",
    "def load_base_model_bf16():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    # model.config.use_cache = False  # better for training/checkpointing\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute MAP@3 ---\n",
    "def compute_map3(eval_pred):\n",
    "    logits_ex, labels_ex = eval_pred\n",
    "\n",
    "    start_token = 77091\n",
    "    offset = 2\n",
    "\n",
    "    # --- Find all (b, t) positions of the start token ---\n",
    "    b_idx, t_idx = np.where(labels_ex == start_token)\n",
    "    target_positions = t_idx + offset\n",
    "\n",
    "    labels_ex = labels_ex[b_idx, target_positions]\n",
    "    logits_ex = logits_ex[b_idx, target_positions-1, :]\n",
    "\n",
    "    # --- Mask logits outside allowed_token_ids ---\n",
    "    mask = np.full(logits_ex.shape, -np.inf, dtype=np.float32)\n",
    "    mask[:, allowed_token_ids] = logits_ex[:, allowed_token_ids]\n",
    "    logits_ex = mask\n",
    "\n",
    "    # --- Top 3 predictions ---\n",
    "    top3_ids = np.argsort(-logits_ex, axis=1)[:, :3]  # (N, 3)\n",
    "\n",
    "    match = (top3_ids == labels_ex[:, None])\n",
    "    map3 = np.mean([1 if m[0] else 0.5 if m[1] else 1/3 if m[2] else 0 for m in match])\n",
    "    return {\"map@3\": map3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54945520-5e19-444f-a73c-20c236563fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_token_ids_tensor = torch.tensor(allowed_token_ids, dtype=torch.long)\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    # logits: [B, T, V]  (for causal LM heads)\n",
    "    # labels: [B, T]\n",
    "    start_token_id = 77091\n",
    "    offset = 2\n",
    "    \n",
    "    if logits.ndim == 3:\n",
    "        # Find positions where label at t == start_token\n",
    "        # Note: doing this per-batch, so labels is smaller\n",
    "        b_idx, t_idx = torch.where(labels == start_token_id)\n",
    "        target_positions = t_idx + offset\n",
    "\n",
    "        # Slice logits to these positions\n",
    "        logits = logits[b_idx, target_positions-1, :]\n",
    "\n",
    "    # Mask: keep only allowed token ids\n",
    "    logits = logits.index_select(dim=-1, index=allowed_token_ids_tensor.to(logits.device))\n",
    "\n",
    "    # Optionally: top-3 on GPU to shrink size even more\n",
    "    top3_vals, top3_ids = torch.topk(logits, k=3, dim=-1)\n",
    "    return top3_ids  # Pass only top3 IDs to compute_metrics\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    start_token_id = 77091\n",
    "    offset = 2\n",
    "    \n",
    "    top3_ids, labels_ex = eval_pred  # top3_ids already on CPU\n",
    "    top3_ids = top3_ids.cpu().numpy() if torch.is_tensor(top3_ids) else top3_ids\n",
    "    labels_ex = labels_ex.cpu().numpy() if torch.is_tensor(labels_ex) else labels_ex\n",
    "\n",
    "    # For labels_ex, we still need to slice to the target positions\n",
    "    # Same logic as in preprocess: find start token positions\n",
    "    b_idx, t_idx = np.where(labels_ex == start_token_id)\n",
    "    target_positions = t_idx + offset\n",
    "    labels_ex = labels_ex[b_idx, target_positions]\n",
    "\n",
    "    match = (top3_ids == labels_ex[:, None])\n",
    "    map3 = np.mean([1 if m[0] else 0.5 if m[1] else 1/3 if m[2] else 0 for m in match])\n",
    "    return {\"map@3\": map3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7203c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ignore for data cleaning Strat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2065283254648ce8b1a89cf056d89eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ffd392dba24fd09064df03d06af548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 REPEAT 0 ===\n",
      "Trying attn_mlp_r8_alpha32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc847a13033462da416b55903d65c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23445b1e0fe46a69d3b135d57e69773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/29356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81ac9f072ab4a5a9c6d35463052c78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/29356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3682ef67b89e41ec94564901cfe3fd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/7340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48566d2630ac4eb0a1d249c09f731222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/7340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='612' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 2:32:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial eval/map@3 = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1126' max='3670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1126/3670 2:35:51 < 5:52:46, 0.12 it/s, Epoch 0.61/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Map@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.084372</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_sets = [\n",
    "    # 4) Attention + MLP (heavier); more epochs, lower LR\n",
    "    dict(\n",
    "        name=\"attn_mlp_r8_alpha32\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=8, lora_alpha=32,\n",
    "        lr=8e-5,\n",
    "        epochs=4\n",
    "    ),\n",
    "    \n",
    "    # 0) Minimal, fast, strong baseline for your setup\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_fast\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,           # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=3\n",
    "    ),\n",
    "\n",
    "    # 1) Same topology, higher capacity (r), keep scaling constant\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha64_cap\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=16, lora_alpha=64,          # α/r = 4\n",
    "        lr=1.8e-4,                    # little lower LR for more params\n",
    "        epochs=3\n",
    "    ),\n",
    "\n",
    "    # 2) Add o_proj to mix heads; keep rank moderate\n",
    "    dict(\n",
    "        name=\"qvo_r8_alpha32_mix\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=8, lora_alpha=32,\n",
    "        lr=2.0e-4,\n",
    "        epochs=3\n",
    "    ),\n",
    "\n",
    "    # 3) Full attention stack; step down LR\n",
    "    dict(\n",
    "        name=\"attn_full_r8_alpha32\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=8, lora_alpha=32,\n",
    "        lr=1.2e-4,\n",
    "        epochs=3\n",
    "    ),\n",
    "\n",
    "    # 5) Heavier rank on attention-only (stress test capacity); reduce LR\n",
    "    dict(\n",
    "        name=\"attn_full_r16_alpha64\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=16, lora_alpha=64,\n",
    "        lr=9e-5,\n",
    "        epochs=3\n",
    "    ),\n",
    "\n",
    "    # 6) Low-rank but bigger α/r (stronger injected update); watch stability\n",
    "    dict(\n",
    "        name=\"qv_r4_alpha32_boost\",   # α/r = 8 (intentional)\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=4, lora_alpha=32,\n",
    "        lr=2.0e-4,\n",
    "        epochs=3\n",
    "    ),\n",
    "\n",
    "    # 7) Medium: q+v plus MLP, but small r to keep params in check\n",
    "    dict(\n",
    "        name=\"qv_mlp_r4_alpha16\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=4, lora_alpha=16,           # α/r = 4\n",
    "        lr=1.2e-4,\n",
    "        epochs=4\n",
    "    ),\n",
    "]\n",
    "\n",
    "param_sets = [\n",
    "    # 4) Attention + MLP (heavier); more epochs, lower LR\n",
    "    dict(\n",
    "        name=\"attn_mlp_r8_alpha32\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        r=8, lora_alpha=32,\n",
    "        lr=8e-5,\n",
    "        epochs=2,\n",
    "    ),\n",
    "    \n",
    "    # 0) Minimal, strong baseline\n",
    "    dict(\n",
    "        name=\"qv_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=8, lora_alpha=32,   # α/r = 4\n",
    "        lr=2.5e-4,\n",
    "        epochs=2\n",
    "    ),\n",
    "    # 1) More capacity, reduce LR\n",
    "    dict(\n",
    "        name=\"qv_r16_alpha64_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\"],\n",
    "        r=16, lora_alpha=64,  # α/r = 4\n",
    "        lr=1.6e-4,\n",
    "        epochs=2\n",
    "    ),\n",
    "    # 2) Add o_proj head-mixing, moderate LR\n",
    "    dict(\n",
    "        name=\"qvo_r8_alpha32_e2\",\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=8, lora_alpha=32,\n",
    "        lr=2.0e-4,\n",
    "        epochs=2\n",
    "    ),\n",
    "    # 3) Full attention, step down LR\n",
    "    dict(\n",
    "        name=\"attn_full_r8_alpha32_e1\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        r=8, lora_alpha=32,\n",
    "        lr=1.2e-4,\n",
    "        epochs=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "train_df, le = load_and_preprocess_data()\n",
    "n_classes = train_df['label'].nunique()\n",
    "train_df['text'] = train_df.apply(format_input, axis=1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CV_FOLD, shuffle=True, random_state=CV_SEED)\n",
    "fold_indices = list(skf.split(train_df, train_df['split_key']))\n",
    "if USE_SINGLE_FOLD:\n",
    "    fold_indices = [fold_indices[0]]\n",
    "    \n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(fold_indices):\n",
    "    tr, va = train_df.iloc[tr_idx].copy(), train_df.iloc[va_idx].copy()\n",
    "\n",
    "    ds_tr = Dataset.from_pandas(tr, preserve_index=False)\n",
    "    ds_tr = ds_tr.map(preprocess_function_conversational_prompt_completion, num_proc=1, remove_columns=ds_tr.column_names)\n",
    "    \n",
    "    ds_va = Dataset.from_pandas(va, preserve_index=False)\n",
    "    ds_va = ds_va.map(preprocess_function_conversational_prompt_completion, num_proc=1, remove_columns=ds_va.column_names)\n",
    "    \n",
    "    best_map = -1.0\n",
    "    for repeat_idx in range(len(param_sets)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{CV_FOLD} REPEAT {repeat_idx} ===\")\n",
    "\n",
    "        cfg = param_sets[repeat_idx]\n",
    "        print(f\"Trying {cfg['name']}\")\n",
    "\n",
    "        # LoRA config for this repeat\n",
    "        lora_config = LoraConfig(\n",
    "            r=cfg[\"r\"],\n",
    "            lora_alpha=cfg[\"lora_alpha\"],\n",
    "            target_modules=cfg[\"target_modules\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        \n",
    "        model = load_base_model_bf16()\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        # model = PeftModel.from_pretrained(model, \"./ver_10/fold_0/checkpoint-200/\")\n",
    "\n",
    "        LR_RATE = cfg[\"lr\"]\n",
    "        EPOCHS = cfg[\"epochs\"]\n",
    "\n",
    "        BATCH_SIZE = 16\n",
    "\n",
    "        training_args = SFTConfig(\n",
    "            output_dir=f\"{DIR}/fold_{fold}\",\n",
    "            num_train_epochs=EPOCHS,\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=24,\n",
    "            eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "            save_strategy=\"steps\", # Save model every 'save_steps'\n",
    "            eval_steps=len(tr)/BATCH_SIZE//2,\n",
    "            save_steps=len(tr)/BATCH_SIZE//2,\n",
    "            save_total_limit=1,\n",
    "            learning_rate=LR_RATE,\n",
    "            metric_for_best_model=\"map@3\",\n",
    "            greater_is_better=True,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f\"{DIR}/logs_fold_{fold}/repeat_{repeat_idx}\",\n",
    "            logging_steps=len(tr)/BATCH_SIZE//2,\n",
    "            report_to=\"tensorboard\",\n",
    "            bf16=True, # TRAIN WITH BF16 IF LOCAL GPU IS NEWER GPU          \n",
    "            fp16=False, # INFER WITH FP16 BECAUSE KAGGLE IS T4 GPU\n",
    "            eval_accumulation_steps=1,\n",
    "            gradient_accumulation_steps=BATCH_SIZE//2,\n",
    "            completion_only_loss=True,\n",
    "            max_length = MAX_LEN,\n",
    "            packing = True,\n",
    "        )\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_va,\n",
    "            # peft_config=lora_config,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "            compute_metrics=compute_map3,\n",
    "        )\n",
    "\n",
    "        if TRAIN_MODEL:\n",
    "            initial_map = trainer.evaluate()[\"eval_map@3\"]  # Initial evaluation before training\n",
    "            print(f\"Initial eval/map@3 = {initial_map:.6f}\")\n",
    "            if initial_map > best_map:\n",
    "                best_map = initial_map\n",
    "                save_dir = f\"{DIR}/fold_{fold}/best\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                # Save LoRA adapters\n",
    "                trainer.save_model(save_dir)\n",
    "                # Save label encoder once per fold\n",
    "                joblib.dump(le, f\"{DIR}/fold_{fold}/label_encoder.joblib\")\n",
    "            \n",
    "            trainer.train()\n",
    "\n",
    "            final_map = eval_result[\"eval_map@3\"]\n",
    "            print(f\"Repeat {repeat_idx} eval/map@3 = {final_map:.6f}\")\n",
    "\n",
    "            if final_map > best_map:\n",
    "                best_map = final_map\n",
    "                save_dir = f\"{DIR}/fold_{fold}/best\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                # Save LoRA adapters\n",
    "                trainer.save_model(save_dir)\n",
    "                # Save label encoder once per fold\n",
    "                joblib.dump(le, f\"{DIR}/fold_{fold}/label_encoder.joblib\")\n",
    "\n",
    "        # cleanup HF checkpoints if any\n",
    "        for ckpt in sorted(Path(f\"{DIR}/fold_{fold}\").glob(\"checkpoint-*\")):\n",
    "            shutil.rmtree(ckpt, ignore_errors=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd5693-2469-46be-942e-9c30753427a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
