{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7FdTSU6AwDa"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "設定ファイル - Phi-4 モデルのトレーニングと推論用設定\n",
        "\"\"\"\n",
        "\n",
        "# Model configuration\n",
        "VER = 2\n",
        "MODEL_NAME = \"microsoft/Phi-4\"\n",
        "MODEL_TYPE = \"phi\"  # Phi-4 model type\n",
        "EPOCHS = 2  # Reduce epochs for initial testing\n",
        "MAX_LEN = 250  # Phi-4 supports longer context\n",
        "\n",
        "# Directory settings\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/map/phi-4-2epoch\"\n",
        "\n",
        "# Training parameters\n",
        "TRAIN_BATCH_SIZE =4  # Smaller batch size for Phi-4\n",
        "EVAL_BATCH_SIZE = 8  # Eval batch size\n",
        "GRADIENT_ACCUMULATION_STEPS = 16  # Increased for effective batch size\n",
        "LEARNING_RATE = 2e-4\n",
        "LOGGING_STEPS = 50\n",
        "SAVE_STEPS = 229\n",
        "\n",
        "# 1772\n",
        "EVAL_STEPS = 229\n",
        "# 1772\n",
        "\n",
        "\n",
        "# Data paths\n",
        "TRAIN_DATA_PATH = '/content/drive/MyDrive/map/train.csv'\n",
        "TEST_DATA_PATH = '/content/drive/MyDrive/map/test.csv'\n",
        "\n",
        "# Model save paths\n",
        "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/best\"\n",
        "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
        "\n",
        "# Other settings\n",
        "RANDOM_SEED = 42\n",
        "VALIDATION_SPLIT = 0.2\n",
        "# 0.0000001\n",
        "\n",
        "# GPU settings\n",
        "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n",
        "\n",
        "# Submission settings\n",
        "SUBMISSION_OUTPUT_PATH = 'submission.csv'\n",
        "\n",
        "# WandB settings\n",
        "USE_WANDB = True  # Set to False to disable WandB\n",
        "WANDB_PROJECT = \"phi-4-math-misconceptions\"\n",
        "WANDB_RUN_NAME = f\"phi-4-ver{VER}\"\n",
        "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
        "\n",
        "# Early stopping settings\n",
        "USE_EARLY_STOPPING = True\n",
        "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
        "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
        "\n",
        "# LoRA configuration for Phi-4\n",
        "LORA_RANK = 128  # LoRAのランク - optimized for Phi-4\n",
        "LORA_ALPHA = 256  # LoRAのスケーリングパラメータ - 1:1 ratio with rank\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Phi-4 target modules\n",
        "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率 - reduced for Phi-4\n",
        "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
        "USE_DORA = False  # DoRA (Weight-Decomposed Low-Rank Adaptation) を使用する場合はTrue\n",
        "\n",
        "# Memory optimization settings\n",
        "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
        "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
        "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
        "\n",
        "ATTENTION_IMPLEMENTATION = \"eager\"  # Options: \"eager\", \"flash_attention_2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAPPING_PATH = '/content/drive/MyDrive/map/question_answer_choice_mapping.csv'"
      ],
      "metadata": {
        "id": "tcHCutw33oT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "共通ユーティリティ関数 - 選択肢付きバージョン\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "\n",
        "def prepare_correct_answers(train_data):\n",
        "    \"\"\"正解答案データを準備\"\"\"\n",
        "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
        "    correct = train_data.loc[idx].copy()\n",
        "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
        "    correct = correct.sort_values('c', ascending=False)\n",
        "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
        "    correct['is_correct'] = 1\n",
        "    return correct\n",
        "\n",
        "\n",
        "def prepare_answer_choices(train_data, mapping_file=MAPPING_PATH):\n",
        "    \"\"\"各問題のMC_Answer選択肢を準備（マッピングファイルを使用、小文字ラベル）\"\"\"\n",
        "    # マッピングファイルを読み込み\n",
        "    mapping_df = pd.read_csv(mapping_file)\n",
        "\n",
        "    # 各QuestionIdごとに選択肢を作成\n",
        "    choices_list = []\n",
        "\n",
        "    for question_id in train_data['QuestionId'].unique():\n",
        "        # 該当QuestionIdのマッピングを取得\n",
        "        question_mapping = mapping_df[mapping_df['QuestionId'] == question_id].copy()\n",
        "\n",
        "        if len(question_mapping) > 0:\n",
        "            # Choice（A,B,C,D）でソート\n",
        "            question_mapping = question_mapping.sort_values('Choice')\n",
        "            # 選択肢文字列を作成（小文字ラベル）\n",
        "            choice_items = []\n",
        "            choice_mapping = {}  # MC_Answer -> choice label のマッピング\n",
        "            for _, row in question_mapping.iterrows():\n",
        "                lowercase_choice = row['Choice'].lower()  # A -> a, B -> b, etc.\n",
        "                choice_items.append(f\"{lowercase_choice}. {row['MC_Answer']}\")\n",
        "                choice_mapping[row['MC_Answer']] = lowercase_choice\n",
        "            answer_choices_str = '\\n'.join(choice_items)\n",
        "        else:\n",
        "            # マッピングがない場合は従来の番号方式にフォールバック\n",
        "            question_answers = train_data[train_data['QuestionId'] == question_id]['MC_Answer'].unique()\n",
        "            choice_items = []\n",
        "            choice_mapping = {}\n",
        "            for i, ans in enumerate(question_answers):\n",
        "                lowercase_choice = chr(ord('a') + i)  # a, b, c, d, ...\n",
        "                choice_items.append(f\"{lowercase_choice}. {ans}\")\n",
        "                choice_mapping[ans] = lowercase_choice\n",
        "            answer_choices_str = '\\n'.join(choice_items)\n",
        "\n",
        "        choices_list.append({\n",
        "            'QuestionId': question_id,\n",
        "            'answer_choices_str': answer_choices_str,\n",
        "            'choice_mapping': choice_mapping  # MC_Answer -> choice label のマッピングも保存\n",
        "        })\n",
        "\n",
        "    choices = pd.DataFrame(choices_list)\n",
        "    return choices\n",
        "\n",
        "\n",
        "def format_input(row):\n",
        "    \"\"\"入力データをモデル用プロンプトにフォーマット（選択肢付き、回答をラベルに変換）\"\"\"\n",
        "    if row[\"is_correct\"]:\n",
        "        status = \"Yes\"\n",
        "    else:\n",
        "        status = \"No\"\n",
        "\n",
        "    # MC_Answerを選択肢ラベル（a, b, c, d）に変換\n",
        "    student_answer_label = row.get('choice_label', row['MC_Answer'])  # フォールバック\n",
        "\n",
        "    # Qwen2.5-Math用の数学タスクに特化したプロンプト（選択肢付き）\n",
        "    prompt = (\n",
        "        \"<|user|>\\n\"\n",
        "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
        "        f\"Question: {row['QuestionText']}\\n\"\n",
        "        f\"Answer: {row['MC_Answer']}\\n\"\n",
        "        f\"Correct?: {status}\\n\"\n",
        "        f\"Explanation: {row['StudentExplanation']}\\n\"\n",
        "        \"<|end|>\\n\"\n",
        "        \"<|assistant|>\\n\"\n",
        "        \"<think>\\n\"\n",
        "        \"Let me analyze this mathematical misconception...\\n\"\n",
        "        \"</think>\\n\\n\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def tokenize_dataset(dataset, tokenizer, max_len):\n",
        "    \"\"\"データセットをトークナイズ\"\"\"\n",
        "    def tokenize(batch):\n",
        "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
        "        return tokenizer(\n",
        "            batch['text'],\n",
        "            padding=False,  # パディングはDataCollatorに任せる\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            return_tensors=None  # map時は'None'を使用\n",
        "        )\n",
        "\n",
        "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
        "    # columnsの設定時にlabelを保持\n",
        "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
        "    dataset.set_format(type='torch', columns=columns)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def compute_map3(eval_pred):\n",
        "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
        "    print(f\"[DEBUG] *** compute_map3 function called! ***\")\n",
        "    print(f\"[DEBUG] eval_pred type: {type(eval_pred)}\")\n",
        "    print(f\"[DEBUG] eval_pred: {eval_pred}\")\n",
        "\n",
        "    try:\n",
        "        logits, labels = eval_pred\n",
        "        print(f\"[DEBUG] compute_map3 called with logits shape: {logits.shape}, labels shape: {labels.shape}\")\n",
        "\n",
        "        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
        "        top3 = np.argsort(-probs, axis=1)[:, :3]\n",
        "        score = 0.0\n",
        "        for i, label in enumerate(labels):\n",
        "            ranks = top3[i]\n",
        "            if ranks[0] == label:\n",
        "                score += 1.0\n",
        "            elif ranks[1] == label:\n",
        "                score += 1.0 / 2\n",
        "            elif ranks[2] == label:\n",
        "                score += 1.0 / 3\n",
        "\n",
        "        map3_score = score / len(labels)\n",
        "        result = {\"eval_map@3\": map3_score}\n",
        "        print(f\"[DEBUG] compute_map3 returning: {result}\")\n",
        "        print(f\"[DEBUG] *** compute_map3 function completed successfully! ***\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] compute_map3 failed: {e}\")\n",
        "        print(f\"[ERROR] Exception type: {type(e)}\")\n",
        "        import traceback\n",
        "        print(f\"[ERROR] Full traceback: {traceback.format_exc()}\")\n",
        "        return {\"eval_map@3\": 0.0}\n",
        "\n",
        "\n",
        "def create_submission(predictions, test_data, label_encoder):\n",
        "    \"\"\"予測結果から提出用ファイルを作成\"\"\"\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
        "    flat = top3.flatten()\n",
        "    decoded = label_encoder.inverse_transform(flat)\n",
        "    top3_labels = decoded.reshape(top3.shape)\n",
        "    pred_strings = [\" \".join(r) for r in top3_labels]\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'row_id': test_data.row_id.values,\n",
        "        'Category:Misconception': pred_strings\n",
        "    })\n",
        "    return submission"
      ],
      "metadata": {
        "id": "eEkw5trRA3ly",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f1bcca45-a0ab-455c-a4e1-83e1d63f53df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MAPPING_PATH' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-98344014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_answer_choices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAPPING_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;34m\"\"\"各問題のMC_Answer選択肢を準備（マッピングファイルを使用、小文字ラベル）\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# マッピングファイルを読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MAPPING_PATH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "カスタムデータコレーター for Qwen3モデル\n",
        "\"\"\"\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Union\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "from transformers.file_utils import PaddingStrategy\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
        "    \"\"\"\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: int = None\n",
        "    pad_to_multiple_of: int = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # バッチの最大長を取得\n",
        "        max_length = max(len(feature[\"input_ids\"]) for feature in features)\n",
        "\n",
        "        # パディング\n",
        "        batch = {}\n",
        "        for key in features[0].keys():\n",
        "            if key == \"label\":\n",
        "                # ラベルはパディング不要\n",
        "                batch[key] = torch.tensor([f[key] for f in features], dtype=torch.long)\n",
        "            elif key in [\"input_ids\", \"attention_mask\"]:\n",
        "                # input_idsとattention_maskをパディング\n",
        "                padded = []\n",
        "                for feature in features:\n",
        "                    # tensorをlistに変換\n",
        "                    if torch.is_tensor(feature[key]):\n",
        "                        feature_list = feature[key].tolist()\n",
        "                    else:\n",
        "                        feature_list = feature[key]\n",
        "\n",
        "                    remainder = [self.tokenizer.pad_token_id if key == \"input_ids\" else 0] * (max_length - len(feature_list))\n",
        "                    padded_feature = feature_list + remainder\n",
        "                    padded.append(padded_feature)\n",
        "                batch[key] = torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "        # labelsフィールドを追加（Trainerが期待するため）\n",
        "        if \"label\" in batch:\n",
        "            batch[\"labels\"] = batch.pop(\"label\")  # labelを削除してlabelsに変更\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "T6jatFDyBEI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Qwen-3-0.6B モデルトレーニングスクリプト - QuestionIDベースのマスク付き損失関数バージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoConfig\n",
        ")\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from datasets import Dataset\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from transformers import AutoModel\n",
        "import wandb\n",
        "from transformers import EarlyStoppingCallback, TrainerCallback\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "# # カスタムモジュールのインポート（選択肢付きバージョン）\n",
        "# from config import *\n",
        "# from utils_with_choices import prepare_correct_answers, prepare_answer_choices, format_input, compute_map3\n",
        "# from data_collator import DataCollatorWithPadding\n",
        "\n",
        "\n",
        "class SaveBestMap3Callback(TrainerCallback):\n",
        "    \"\"\"eval_map@3が最高値を更新した際にモデルを保存するコールバック\"\"\"\n",
        "    def __init__(self, save_dir, tokenizer):\n",
        "        self.save_dir = save_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.best_map3 = 0.0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics, model=None, **kwargs):\n",
        "        current_map3 = metrics.get('eval_map@3', 0.0)\n",
        "\n",
        "        if current_map3 > self.best_map3:\n",
        "            self.best_map3 = current_map3\n",
        "\n",
        "            # 専用ディレクトリに保存\n",
        "            best_map3_path = os.path.join(self.save_dir, 'best_map3')\n",
        "            os.makedirs(best_map3_path, exist_ok=True)\n",
        "\n",
        "            # LoRAアダプターのみを保存\n",
        "            model.save_pretrained(best_map3_path)\n",
        "            self.tokenizer.save_pretrained(best_map3_path)\n",
        "\n",
        "            print(f\"\\n新しいベストMAP@3スコア: {current_map3:.4f} - モデルを {best_map3_path} に保存しました\")\n",
        "\n",
        "        return control\n",
        "\n",
        "\n",
        "class Qwen2ForSequenceClassificationWithMaskedLoss(nn.Module):\n",
        "    \"\"\"Qwen2モデルを分類タスク用にカスタマイズ - マスク付き損失関数版\"\"\"\n",
        "    def __init__(self, model_name, num_labels, question_label_map=None,attn_implementation=\"eager\"):\n",
        "        super().__init__()\n",
        "        from transformers import AutoModel\n",
        "        # self.qwen = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "        # self.phi = AutoModel.from_pretrained(\n",
        "        #     model_name,\n",
        "        #     trust_remote_code=True,\n",
        "        #     attn_implementation=attn_implementation\n",
        "        # )\n",
        "        self.qwen  = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            attn_implementation=attn_implementation,\n",
        "            device_map=\"auto\",\n",
        "            # load_in_8bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "\n",
        "        )\n",
        "        if hasattr(self.qwen.config, \"use_cache\"):\n",
        "          self.qwen.config.use_cache = False\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.qwen.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # PEFTライブラリとの互換性のためconfigを追加\n",
        "        self.config = self.qwen.config\n",
        "        self.config.num_labels = num_labels\n",
        "\n",
        "        # QuestionIdごとの有効ラベルマップ\n",
        "        self.question_label_map = question_label_map\n",
        "\n",
        "        # マスク値（無効なラベルに適用する大きな負の値）\n",
        "        self.mask_value = -65000\n",
        "        # -1e10\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, question_ids=None, **kwargs):\n",
        "        # Transformersが渡す追加の引数（inputs_embeds等）をkwargsで受け取る\n",
        "        # 基本のqwenモデルに必要な引数のみを渡す\n",
        "        outputs = self.qwen(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # 最後のトークンの隠れ状態を使用\n",
        "        pooled_output = outputs.last_hidden_state[:, -1, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # QuestionIdベースのマスクを適用\n",
        "        if question_ids is not None and self.question_label_map is not None:\n",
        "            masked_logits = self.apply_question_mask(logits, question_ids)\n",
        "            # デバッグ: 最初のバッチで確認\n",
        "            if torch.rand(1).item() < 0.001:  # 0.1%の確率でデバッグ出力\n",
        "                print(f\"[MASK DEBUG] Original logits range: [{logits.min():.3f}, {logits.max():.3f}]\")\n",
        "                print(f\"[MASK DEBUG] Masked logits range: [{masked_logits.min():.3f}, {masked_logits.max():.3f}]\")\n",
        "                print(f\"[MASK DEBUG] Question IDs: {question_ids[:3].tolist()}\")\n",
        "        else:\n",
        "            masked_logits = logits\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            # マスクされたlogitsで損失を計算\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # ラベルの形状をチェック\n",
        "            if labels.dim() > 1:\n",
        "                labels = labels.view(-1)\n",
        "            if masked_logits.dim() == 3:\n",
        "                masked_logits = masked_logits.view(-1, self.num_labels)\n",
        "\n",
        "            # 損失を計算\n",
        "            try:\n",
        "                loss = loss_fct(masked_logits, labels)\n",
        "\n",
        "                # 最初の数回でデバッグ情報を表示\n",
        "                if torch.rand(1).item() < 0.001:  # 0.1%の確率でデバッグ出力\n",
        "                    print(f\"[LOSS DEBUG] Computed loss: {loss.item():.6f}\")\n",
        "                    print(f\"[LOSS DEBUG] Labels: {labels[:5].tolist()}\")\n",
        "                    print(f\"[LOSS DEBUG] Masked logits shape: {masked_logits.shape}\")\n",
        "\n",
        "                # 損失がNaNまたは無限大でないことを確認\n",
        "                if torch.isnan(loss) or torch.isinf(loss):\n",
        "                    print(f\"Warning: Invalid loss detected: {loss}\")\n",
        "                    print(f\"masked_logits stats: min={masked_logits.min()}, max={masked_logits.max()}, mean={masked_logits.mean()}\")\n",
        "                    print(f\"labels stats: min={labels.min()}, max={labels.max()}\")\n",
        "                    # NaN/Infの場合は大きな損失値を設定\n",
        "                    loss = torch.tensor(100.0, requires_grad=True, device=masked_logits.device)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error computing loss: {e}\")\n",
        "                print(f\"masked_logits shape: {masked_logits.shape}, labels shape: {labels.shape}\")\n",
        "                # エラーが発生した場合は大きな損失値を設定\n",
        "                loss = torch.tensor(100.0, requires_grad=True, device=masked_logits.device)\n",
        "\n",
        "        # Accelerateライブラリ互換のため、SequenceClassifierOutputを使用\n",
        "        return SequenceClassifierOutput(loss=loss, logits=masked_logits)\n",
        "\n",
        "    def apply_question_mask(self, logits, question_ids):\n",
        "        \"\"\"QuestionIdごとに無効なラベルをマスクする\"\"\"\n",
        "        batch_size = logits.size(0)\n",
        "        device = logits.device\n",
        "\n",
        "        # マスクを作成（初期値は全てマスク）\n",
        "        mask = torch.full_like(logits, self.mask_value)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            q_id = question_ids[i].item() if torch.is_tensor(question_ids[i]) else question_ids[i]\n",
        "\n",
        "            if q_id in self.question_label_map:\n",
        "                valid_labels = self.question_label_map[q_id]\n",
        "                # 有効なラベルのみ元のlogitsを保持\n",
        "                for label_idx in valid_labels:\n",
        "                    mask[i, label_idx] = 0\n",
        "\n",
        "        # マスクを適用（無効なラベルには大きな負の値を加算）\n",
        "        masked_logits = logits + mask\n",
        "\n",
        "        return masked_logits\n",
        "\n",
        "\n",
        "def create_question_label_mapping(train_df):\n",
        "    \"\"\"QuestionIdごとの有効なラベル（誤概念）のマッピングを作成\"\"\"\n",
        "    question_label_map = defaultdict(set)\n",
        "\n",
        "    for _, row in train_df.iterrows():\n",
        "        question_id = row['QuestionId']\n",
        "        label = row['label']\n",
        "        question_label_map[question_id].add(label)\n",
        "\n",
        "    # setをlistに変換\n",
        "    question_label_map = {q_id: list(labels) for q_id, labels in question_label_map.items()}\n",
        "\n",
        "    # 統計情報を表示\n",
        "    label_counts = [len(labels) for labels in question_label_map.values()]\n",
        "    print(f\"\\n=== QuestionId-Label Mapping Statistics ===\")\n",
        "    print(f\"Total unique questions: {len(question_label_map)}\")\n",
        "    print(f\"Average labels per question: {np.mean(label_counts):.2f}\")\n",
        "    print(f\"Min labels per question: {np.min(label_counts)}\")\n",
        "    print(f\"Max labels per question: {np.max(label_counts)}\")\n",
        "    print(f\"Median labels per question: {np.median(label_counts):.1f}\")\n",
        "\n",
        "    # ヒストグラムを作成\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(label_counts, bins=30, edgecolor='black')\n",
        "    plt.xlabel('Number of labels per question')\n",
        "    plt.ylabel('Number of questions')\n",
        "    plt.title('Distribution of Labels per QuestionId')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('question_label_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    return dict(question_label_map)\n",
        "\n",
        "\n",
        "def tokenize_dataset_with_question_id(dataset, tokenizer, max_len):\n",
        "    \"\"\"データセットのトークナイズ（QuestionId付き）\"\"\"\n",
        "    def tokenize(batch):\n",
        "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
        "        tokenized = tokenizer(\n",
        "            batch['text'],\n",
        "            padding=False,  # パディングはDataCollatorに任せる\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            return_tensors=None  # map時は'None'を使用\n",
        "        )\n",
        "        # QuestionIdをそのまま保持\n",
        "        tokenized['question_ids'] = batch['QuestionId']\n",
        "        return tokenized\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize,\n",
        "        batched=True,\n",
        "        remove_columns=['text', 'QuestionId']  # textとQuestionIdを削除、labelは保持\n",
        "    )\n",
        "\n",
        "    return tokenized_dataset\n",
        "\n",
        "\n",
        "class DataCollatorWithQuestionId:\n",
        "    \"\"\"QuestionIdを含むカスタムデータコレーター\"\"\"\n",
        "    def __init__(self, tokenizer, max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # # デバッグ: 入力データの構造を確認\n",
        "        # print(f\"[COLLATOR DEBUG] Features keys: {list(features[0].keys())}\")\n",
        "        # print(f\"[COLLATOR DEBUG] First feature sample: {features[0]}\")\n",
        "\n",
        "        # バッチの最大長を取得\n",
        "        max_length = max(len(feature[\"input_ids\"]) for feature in features)\n",
        "\n",
        "        # パディング\n",
        "        batch = {}\n",
        "        for key in features[0].keys():\n",
        "            if key in [\"label\", \"labels\"]:\n",
        "                # ラベルはパディング不要（labelをlabelsに変換）\n",
        "                labels = [f[key] for f in features]\n",
        "                # print(f\"[COLLATOR DEBUG] Processing labels: {labels[:5]}...\")\n",
        "                batch[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
        "            elif key in [\"input_ids\", \"attention_mask\"]:\n",
        "                # input_idsとattention_maskをパディング\n",
        "                padded = []\n",
        "                for feature in features:\n",
        "                    # tensorをlistに変換\n",
        "                    if torch.is_tensor(feature[key]):\n",
        "                        feature_list = feature[key].tolist()\n",
        "                    else:\n",
        "                        feature_list = feature[key]\n",
        "\n",
        "                    remainder = [self.tokenizer.pad_token_id if key == \"input_ids\" else 0] * (max_length - len(feature_list))\n",
        "                    padded_feature = feature_list + remainder\n",
        "                    padded.append(padded_feature)\n",
        "                batch[key] = torch.tensor(padded, dtype=torch.long)\n",
        "            elif key == \"question_ids\":\n",
        "                # question_idsはパディング不要\n",
        "                batch[key] = torch.tensor([f[key] for f in features], dtype=torch.long)\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# WandBの初期化\n",
        "if USE_WANDB:\n",
        "    wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        name=WANDB_RUN_NAME + \"_masked_loss\",\n",
        "        entity=WANDB_ENTITY,\n",
        "        config={\n",
        "            \"model_name\": MODEL_NAME,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"max_len\": MAX_LEN,\n",
        "            \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "            \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
        "            \"learning_rate\": LEARNING_RATE,\n",
        "            \"early_stopping_patience\": EARLY_STOPPING_PATIENCE if USE_EARLY_STOPPING else None,\n",
        "            \"lora_rank\": LORA_RANK,\n",
        "            \"lora_alpha\": LORA_ALPHA,\n",
        "            \"lora_target_modules\": LORA_TARGET_MODULES,\n",
        "            \"lora_dropout\": LORA_DROPOUT,\n",
        "            \"lora_bias\": LORA_BIAS,\n",
        "            \"with_choices\": True,\n",
        "            \"masked_loss\": True,  # マスク付き損失関数を使用\n",
        "        }\n",
        "    )\n",
        "\n",
        "# GPU設定\n",
        "if CUDA_VISIBLE_DEVICES is not None:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
        "    print(f\"Using CUDA device(s): {CUDA_VISIBLE_DEVICES}\")\n",
        "\n",
        "# 出力ディレクトリの作成\n",
        "output_dir_masked = OUTPUT_DIR + \"_masked_loss\"\n",
        "os.makedirs(output_dir_masked, exist_ok=True)\n",
        "\n",
        "# --- データの読み込みと前処理 ---\n",
        "print(\"Loading and preprocessing training data...\")\n",
        "le = LabelEncoder()\n",
        "train = pd.read_csv(TRAIN_DATA_PATH)\n",
        "\n",
        "# --- QuestionId 32835のQuestionTextを更新 ---\n",
        "print(\"Updating QuestionId 32835...\")\n",
        "new_question_text = \"Which number is the greatest? Options: 6.0000 6.2 6.079 6.0001\"\n",
        "mask_32835 = train['QuestionId'] == 32835\n",
        "update_count = mask_32835.sum()\n",
        "\n",
        "if update_count > 0:\n",
        "    original_text = train[mask_32835]['QuestionText'].iloc[0]\n",
        "    print(f\"Found {update_count} rows with QuestionId 32835\")\n",
        "    print(f\"Original: {original_text[:80]}...\")\n",
        "    print(f\"Updated to: {new_question_text}\")\n",
        "    train.loc[mask_32835, 'QuestionText'] = new_question_text\n",
        "else:\n",
        "    print(\"No rows found with QuestionId 32835\")\n",
        "\n",
        "# フィルタリングを行わず全データを使用\n",
        "print(f\"Using all data without filtering: {train.shape[0]} rows\")\n",
        "\n",
        "train.Misconception = train.Misconception.fillna('NA')\n",
        "train['target'] = train.Category + \":\" + train.Misconception\n",
        "train['label'] = le.fit_transform(train['target'])\n",
        "n_classes = len(le.classes_)\n",
        "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
        "\n",
        "# --- 特徴量エンジニアリング ---\n",
        "print(\"Performing feature engineering...\")\n",
        "correct = prepare_correct_answers(train)\n",
        "train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
        "train.is_correct = train.is_correct.fillna(0)\n",
        "\n",
        "# --- 選択肢データの準備 ---\n",
        "print(\"Preparing answer choices for each question...\")\n",
        "choices = prepare_answer_choices(train)\n",
        "train = train.merge(choices[['QuestionId', 'answer_choices_str']], on='QuestionId', how='left')\n",
        "\n",
        "# --- MC_Answerを選択肢ラベルに変換 ---\n",
        "print(\"Converting MC_Answer to choice labels...\")\n",
        "def get_choice_label(row):\n",
        "    question_id = row['QuestionId']\n",
        "    mc_answer = row['MC_Answer']\n",
        "    # 該当するchoice_mappingを取得\n",
        "    choice_mapping = choices[choices['QuestionId'] == question_id]['choice_mapping'].iloc[0]\n",
        "    return choice_mapping.get(mc_answer, mc_answer)  # マッピングがない場合は元の値\n",
        "\n",
        "train['choice_label'] = train.apply(get_choice_label, axis=1)\n",
        "\n",
        "# --- 入力テキストのフォーマット ---\n",
        "print(\"Formatting input text with answer choices...\")\n",
        "train['text'] = train.apply(format_input, axis=1)\n",
        "print(\"Example prompt for our LLM with choices:\")\n",
        "print(train.text.values[0])\n",
        "\n",
        "# --- QuestionId-Labelマッピングの作成 ---\n",
        "print(\"\\nCreating QuestionId-Label mapping for masked loss...\")\n",
        "question_label_map = create_question_label_mapping(train)\n",
        "\n",
        "# マッピングを保存\n",
        "mapping_path = f\"{output_dir_masked}/question_label_mapping.pkl\"\n",
        "with open(mapping_path, 'wb') as f:\n",
        "    pickle.dump(question_label_map, f)\n",
        "print(f\"Question-Label mapping saved to: {mapping_path}\")\n",
        "\n",
        "# --- トークナイザーの初期化 ---\n",
        "print(\"Initializing tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# パディングトークンの設定\n",
        "# Qwen3モデルの場合、特別なトークンIDを使用\n",
        "if tokenizer.pad_token is None:\n",
        "    # 語彙内の安全なトークンIDを使用\n",
        "    # Qwenモデルでは、0番のトークンがUNKNOWNトークンとして使われることが多い\n",
        "    tokenizer.pad_token_id = 0\n",
        "    tokenizer.pad_token = tokenizer.decode([0])\n",
        "\n",
        "# --- トークン長の分析 ---\n",
        "print(\"Analyzing token lengths...\")\n",
        "lengths = [len(tokenizer.encode(t, truncation=False)) for t in train['text']]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(lengths, bins=50)\n",
        "plt.title(\"Token Length Distribution (With Choices and Masked Loss)\")\n",
        "plt.xlabel(\"Number of tokens\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.savefig(f'{output_dir_masked}/token_length_distribution_masked.png')\n",
        "plt.close()\n",
        "\n",
        "over_limit = (np.array(lengths) > MAX_LEN).sum()\n",
        "print(f\"There are {over_limit} train sample(s) with more than {MAX_LEN} tokens\")\n",
        "\n",
        "# 選択肢が追加されたことによるトークン長の増加を表示\n",
        "avg_length = np.mean(lengths)\n",
        "max_length = np.max(lengths)\n",
        "print(f\"Average token length: {avg_length:.1f}, Max token length: {max_length}\")\n",
        "\n",
        "# --- データの分割 ---\n",
        "if VALIDATION_SPLIT > 0:\n",
        "    print(\"Splitting data into train and validation sets...\")\n",
        "    train_df, val_df = train_test_split(train, test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "    # QuestionIdを含むカラムを選択\n",
        "    COLS = ['text', 'label', 'QuestionId']\n",
        "    train_ds = Dataset.from_pandas(train_df[COLS])\n",
        "    val_ds = Dataset.from_pandas(val_df[COLS])\n",
        "\n",
        "    # --- データセットのトークナイズ ---\n",
        "    print(\"Tokenizing datasets with QuestionIds...\")\n",
        "    train_ds = tokenize_dataset_with_question_id(train_ds, tokenizer, MAX_LEN)\n",
        "    val_ds = tokenize_dataset_with_question_id(val_ds, tokenizer, MAX_LEN)\n",
        "else:\n",
        "    print(\"Using all data for training (no validation split)...\")\n",
        "    train_df = train\n",
        "    val_df = None\n",
        "\n",
        "    # QuestionIdを含むカラムを選択\n",
        "    COLS = ['text', 'label', 'QuestionId']\n",
        "    train_ds = Dataset.from_pandas(train_df[COLS])\n",
        "    val_ds = None\n",
        "\n",
        "    # --- データセットのトークナイズ ---\n",
        "    print(\"Tokenizing training dataset with QuestionIds...\")\n",
        "    train_ds = tokenize_dataset_with_question_id(train_ds, tokenizer, MAX_LEN)\n",
        "\n",
        "# --- Label Encoderの保存 ---\n",
        "label_encoder_path = f\"{output_dir_masked}/label_encoder.joblib\"\n",
        "print(f\"Saving label encoder to: {label_encoder_path}\")\n",
        "joblib.dump(le, label_encoder_path)\n",
        "\n",
        "\n",
        "# --- モデルの初期化 ---\n",
        "print(\"Initializing model with masked loss...\")\n",
        "# カスタムクラスを直接使用（マスク付き損失関数のため）\n",
        "model = Qwen2ForSequenceClassificationWithMaskedLoss(MODEL_NAME, n_classes, question_label_map)\n",
        "\n",
        "# パディングトークンIDを設定\n",
        "if hasattr(model.config, 'pad_token_id'):\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "# --- LoRAアダプターの設定 ---\n",
        "print(\"Configuring LoRA adapter...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,  # LoRAのランク\n",
        "    lora_alpha=LORA_ALPHA,  # LoRAのスケーリングパラメータ\n",
        "    target_modules=LORA_TARGET_MODULES,  # 対象モジュール\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=LORA_BIAS,\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n",
        "\n",
        "# PEFTモデルの作成\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"Number of trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# シングルGPUに設定\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_ratio=0.0,  # ウォームアップを無効化\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"map@3\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    bf16=True,  # BF16を使用\n",
        "    gradient_checkpointing=False,  # メモリ効率化のため有効化\n",
        "    dataloader_num_workers=4,\n",
        "    remove_unused_columns=False,  # カラムを削除しない\n",
        "    lr_scheduler_type=\"cosine\",  # コサインスケジューラーを使用\n",
        "    max_grad_norm=MAX_GRAD_NORM,  # Gradient clipping\n",
        "    optim=\"adamw_bnb_8bit\" if USE_8BIT_ADAM else \"adamw_torch\",  # 8-bit Adam optimizer\n",
        "    report_to=\"wandb\" if USE_WANDB else \"none\",\n",
        "    run_name=WANDB_RUN_NAME + \"_v2\" if USE_WANDB else None,\n",
        ")\n",
        "# --- トレーナーのセットアップとトレーニング ---\n",
        "print(\"Setting up trainer...\")\n",
        "\n",
        "# エポックあたりのステップ数を計算\n",
        "steps_per_epoch = len(train_ds) // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)  # gradient_accumulation_stepsを考慮\n",
        "total_steps = steps_per_epoch * EPOCHS\n",
        "print(f\"\\nDataset statistics:\")\n",
        "print(f\"Training samples: {len(train_ds)}\")\n",
        "print(f\"Validation samples: {len(val_ds) if val_ds is not None else 0}\")\n",
        "print(f\"Batch size: {TRAIN_BATCH_SIZE} (with gradient accumulation: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "if val_ds is not None:\n",
        "    print(f\"Evaluation interval: every {EVAL_STEPS} steps (~{EVAL_STEPS/steps_per_epoch:.2f} epochs)\")\n",
        "    print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} evaluations without improvement\")\n",
        "else:\n",
        "    print(\"No validation - training without evaluation\")\n",
        "\n",
        "# カスタムデータコレーターを使用（QuestionId付き）\n",
        "data_collator = DataCollatorWithQuestionId(tokenizer=tokenizer, max_length=MAX_LEN)\n",
        "\n",
        "# コールバックの設定\n",
        "callbacks = []\n",
        "\n",
        "# SaveBestMap3Callbackを追加（validationがある場合のみ）\n",
        "if val_ds is not None:\n",
        "    save_best_callback = SaveBestMap3Callback(save_dir=output_dir_masked, tokenizer=tokenizer)\n",
        "    callbacks.append(save_best_callback)\n",
        "    print(f\"SaveBestMap3Callback enabled - モデルは {output_dir_masked}/best_map3 に保存されます\")\n",
        "\n",
        "    if USE_EARLY_STOPPING:\n",
        "        # EARLY_STOPPING_PATIENCEは評価回数として直接使用\n",
        "        early_stopping_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "            early_stopping_threshold=EARLY_STOPPING_THRESHOLD\n",
        "        )\n",
        "        callbacks.append(early_stopping_callback)\n",
        "        print(f\"Early stopping enabled:\")\n",
        "        print(f\"  - Patience (evaluations without improvement): {EARLY_STOPPING_PATIENCE}\")\n",
        "        print(f\"  - Threshold: {EARLY_STOPPING_THRESHOLD}\")\n",
        "else:\n",
        "    print(\"No validation - callbacks disabled\")\n",
        "\n",
        "# デバッグ: compute_map3関数の確認\n",
        "print(f\"[DEBUG] compute_map3 function: {compute_map3}\")\n",
        "print(f\"[DEBUG] val_ds is not None: {val_ds is not None}\")\n",
        "compute_metrics_func = compute_map3 if val_ds is not None else None\n",
        "print(f\"[DEBUG] compute_metrics will be set to: {compute_metrics_func}\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics_func,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "# Trainerが初期化された後にcompute_metricsが正しく設定されているか確認\n",
        "# print(f\"[DEBUG] Trainer.compute_metrics: {trainer.compute_metrics}\")\n",
        "# if hasattr(trainer, 'compute_metrics') and trainer.compute_metrics:\n",
        "#     print(\"[DEBUG] compute_metrics is properly set in Trainer\")\n",
        "# else:\n",
        "#     print(\"[DEBUG] WARNING: compute_metrics is NOT set in Trainer\")\n",
        "\n",
        "print(\"\\nStarting training with masked loss...\")\n",
        "print(\"Note: Invalid labels for each QuestionId will be masked during training.\")\n",
        "trainer.train()\n",
        "\n",
        "# # --- 最終的なMAP@3スコアを表示 ---\n",
        "# if val_ds is not None:\n",
        "#     print(\"\\nEvaluating on validation set...\")\n",
        "#     eval_results = trainer.evaluate()\n",
        "#     print(f\"\\nValidation MAP@3: {eval_results.get('eval_map@3', 'N/A'):.4f}\")\n",
        "\n",
        "# # --- モデルの保存 ---\n",
        "# best_model_path = f\"{output_dir_masked}/best\"\n",
        "# print(\"\\nSaving model...\")\n",
        "# # LoRAアダプターのみを保存\n",
        "# model.save_pretrained(best_model_path)\n",
        "# # トークナイザーも保存\n",
        "# tokenizer.save_pretrained(best_model_path)\n",
        "\n",
        "# print(\"Training completed successfully!\")\n",
        "# print(f\"Model saved to: {best_model_path}\")\n",
        "# print(f\"Label encoder saved to: {label_encoder_path}\")\n",
        "# print(f\"Question-Label mapping saved to: {mapping_path}\")\n",
        "\n",
        "# # WandBの終了\n",
        "# if USE_WANDB:\n",
        "#     wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "1RnHUkUsBRgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "訓練直後のモデルを直接マージして保存するスクリプト\n",
        "train_mask_loss.pyの訓練完了後に実行する想定\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def save_trained_model_directly(\n",
        "    trained_peft_model,  # trainer.train()後のPEFTモデル（既に学習済み）\n",
        "    tokenizer,\n",
        "    output_dir,\n",
        "    label_encoder_path,\n",
        "    question_label_mapping_path\n",
        "):\n",
        "    \"\"\"\n",
        "    訓練済みのPEFTモデルを直接マージして保存\n",
        "\n",
        "    Args:\n",
        "        trained_peft_model: 訓練済みのPEFTモデル（trainer.model）\n",
        "        tokenizer: 使用したトークナイザー\n",
        "        output_dir: 保存先ディレクトリ\n",
        "        label_encoder_path: ラベルエンコーダーのパス\n",
        "        question_label_mapping_path: Question-Labelマッピングのパス\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Saving trained model directly...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. PEFTモデルをマージ\n",
        "    print(\"\\n1. Merging LoRA adapter with base model...\")\n",
        "    # merge_and_unload()でLoRAアダプターをベースモデルに統合\n",
        "    merged_model = trained_peft_model.merge_and_unload()\n",
        "\n",
        "    # 2. 出力ディレクトリの作成\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 3. マージ済みモデル（qwen部分）の保存\n",
        "    print(f\"\\n2. Saving merged base model (qwen) to: {output_dir}\")\n",
        "    # qwen部分を保存（AutoModelで読み込み可能な形式）\n",
        "    merged_model.qwen.save_pretrained(output_dir, safe_serialization=True)\n",
        "\n",
        "    # 4. classifierレイヤーの重みを保存\n",
        "    classifier_path = os.path.join(output_dir, \"classifier_weights.pt\")\n",
        "    print(f\"\\n3. Saving classifier weights to: {classifier_path}\")\n",
        "    torch.save({\n",
        "        'classifier_state_dict': merged_model.classifier.state_dict(),\n",
        "        'hidden_size': merged_model.qwen.config.hidden_size,\n",
        "        'num_labels': merged_model.num_labels,\n",
        "        'dropout': 0.1,\n",
        "    }, classifier_path)\n",
        "\n",
        "    # 5. トークナイザーも保存\n",
        "    print(f\"\\n4. Saving tokenizer to: {output_dir}\")\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # 6. Question-Labelマッピングをコピー\n",
        "    question_label_map = question_label_mapping_path\n",
        "\n",
        "    output_mapping_path = os.path.join(output_dir, \"question_label_mapping.pkl\")\n",
        "    print(f\"\\n5. Saving question-label mapping to: {output_mapping_path}\")\n",
        "    with open(output_mapping_path, 'wb') as f:\n",
        "        pickle.dump(question_label_map, f)\n",
        "\n",
        "    # 7. ラベルエンコーダーをコピー\n",
        "    le = label_encoder_path\n",
        "    output_encoder_path = os.path.join(output_dir, \"label_encoder.joblib\")\n",
        "    print(f\"\\n6. Saving label encoder to: {output_encoder_path}\")\n",
        "    joblib.dump(le, output_encoder_path)\n",
        "\n",
        "    # 8. モデル設定情報を保存\n",
        "    config_path = os.path.join(output_dir, \"model_config.pkl\")\n",
        "    print(f\"\\n7. Saving model configuration to: {config_path}\")\n",
        "    with open(config_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'n_classes': merged_model.num_labels,\n",
        "            'mask_value': -1e10,\n",
        "            'dropout': 0.1,\n",
        "        }, f)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Model saved successfully!\")\n",
        "    print(f\"All files saved to: {output_dir}\")\n",
        "    print(\"\\nSaved files:\")\n",
        "    print(f\"  - Merged base model: {output_dir}/\")\n",
        "    print(f\"  - Classifier weights: {classifier_path}\")\n",
        "    print(f\"  - Question-label mapping: {output_mapping_path}\")\n",
        "    print(f\"  - Label encoder: {output_encoder_path}\")\n",
        "    print(f\"  - Model config: {config_path}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    return merged_model\n",
        "\n",
        "\n",
        "# train_mask_loss.pyの最後に追加する使用例\n",
        "\"\"\"\n",
        "使用例（train_mask_loss.pyの訓練完了後に追加）:\n",
        "\n",
        "from save_trained_model_directly import save_trained_model_directly\n",
        "\n",
        "# trainer.train()の後で実行\n",
        "merged_model = save_trained_model_directly(\n",
        "    trained_peft_model=model,  # trainer.train()後のモデル\n",
        "    tokenizer=tokenizer,\n",
        "    output_dir=\"./phi4-merged-complete\",\n",
        "    label_encoder_path=label_encoder_path,\n",
        "    question_label_mapping_path=mapping_path\n",
        ")\n",
        "\n",
        "# Kaggleにアップロードする際はこのディレクトリをzipして使用\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6ZSB61fSitic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = save_trained_model_directly(\n",
        "    trained_peft_model=model,  # trainer.train()後のモデル\n",
        "    tokenizer=tokenizer,\n",
        "    output_dir=\"./phi4-merged\",\n",
        "    label_encoder_path=le,\n",
        "    question_label_mapping_path=question_label_map\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVyoLw_gitel",
        "outputId": "8068e2b8-d9f6-44c9-a5f8-e971be7b2783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Saving trained model directly...\n",
            "==================================================\n",
            "\n",
            "1. Merging LoRA adapter with base model...\n",
            "\n",
            "2. Saving merged base model (qwen) to: ./phi4-merged\n",
            "\n",
            "3. Saving classifier weights to: ./phi4-merged/classifier_weights.pt\n",
            "\n",
            "4. Saving tokenizer to: ./phi4-merged\n",
            "\n",
            "5. Saving question-label mapping to: ./phi4-merged/question_label_mapping.pkl\n",
            "\n",
            "6. Saving label encoder to: ./phi4-merged/label_encoder.joblib\n",
            "\n",
            "7. Saving model configuration to: ./phi4-merged/model_config.pkl\n",
            "\n",
            "==================================================\n",
            "Model saved successfully!\n",
            "All files saved to: ./phi4-merged\n",
            "\n",
            "Saved files:\n",
            "  - Merged base model: ./phi4-merged/\n",
            "  - Classifier weights: ./phi4-merged/classifier_weights.pt\n",
            "  - Question-label mapping: ./phi4-merged/question_label_mapping.pkl\n",
            "  - Label encoder: ./phi4-merged/label_encoder.joblib\n",
            "  - Model config: ./phi4-merged/model_config.pkl\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Google ColabでKaggleにモデルをアップロードするスクリプト\n",
        "/content/phi4-reasoning-merged ディレクトリをKaggleデータセットとしてアップロード\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Google Colabで実行するコード\n",
        "# このスクリプトをColabにコピーして実行してください\n",
        "\n",
        "# ========================================\n",
        "# 1. Kaggle APIのセットアップ\n",
        "# ========================================\n",
        "print(\"=\" * 50)\n",
        "print(\"Step 1: Setting up Kaggle API\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# kaggle.json をアップロード（既にある場合はスキップ）\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    print(\"Please upload your kaggle.json file...\")\n",
        "    uploaded = files.upload()  # kaggle.json を選ぶ\n",
        "\n",
        "    # Kaggle APIのインストールと設定\n",
        "    !pip -q install kaggle\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(\"Kaggle API setup completed!\")\n",
        "else:\n",
        "    print(\"kaggle.json already exists. Skipping upload.\")\n",
        "    !pip -q install kaggle\n",
        "\n",
        "# 動作確認\n",
        "print(\"\\nTesting Kaggle API connection...\")\n",
        "!kaggle datasets list -s \"titanic\" | head -n 3\n",
        "\n",
        "# ========================================\n",
        "# 2. データセットのメタデータ作成\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Step 2: Creating dataset metadata\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import json\n",
        "\n",
        "# 設定（必要に応じて変更）\n",
        "USERNAME = \"masasato1999\"           # あなたのKaggleユーザー名\n",
        "DATASET_SLUG = \"phi4-merged-masked\"  # データセット名（英数字とハイフンのみ）\n",
        "DST = \"/content/phi4-merged\"  # アップロードするディレクトリ\n",
        "\n",
        "# ディレクトリの存在確認\n",
        "if not os.path.exists(DST):\n",
        "    print(f\"ERROR: Directory {DST} not found!\")\n",
        "    print(\"Please make sure the model is saved at the correct location.\")\n",
        "else:\n",
        "    print(f\"Model directory found: {DST}\")\n",
        "\n",
        "    # ディレクトリの内容を確認\n",
        "    print(\"\\nDirectory contents:\")\n",
        "    !ls -la $DST\n",
        "\n",
        "    # メタデータの作成\n",
        "    meta = {\n",
        "        \"title\": \"Phi-4 Reasoning Merged with Masked Loss\",\n",
        "        \"id\": f\"{USERNAME}/{DATASET_SLUG}\",\n",
        "        \"licenses\": [{\"name\": \"other\"}],\n",
        "        \"isPrivate\": True  # まずは非公開でアップロード\n",
        "    }\n",
        "\n",
        "    metadata_path = f\"{DST}/dataset-metadata.json\"\n",
        "    with open(metadata_path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    print(f\"\\nMetadata created at: {metadata_path}\")\n",
        "    print(\"Metadata content:\")\n",
        "    print(json.dumps(meta, indent=2))\n",
        "\n",
        "# ========================================\n",
        "# 3. Kaggleへのアップロード\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Step 3: Uploading to Kaggle\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"Uploading directory: {DST}\")\n",
        "print(f\"Dataset will be available at: kaggle.com/datasets/{USERNAME}/{DATASET_SLUG}\")\n",
        "print(\"\\nThis may take several minutes depending on the model size...\")\n",
        "\n",
        "# アップロード実行\n",
        "!kaggle datasets create -p $DST --dir-mode zip\n",
        "\n",
        "# # ========================================\n",
        "# # 4. アップロード確認\n",
        "# # ========================================\n",
        "# print(\"\\n\" + \"=\" * 50)\n",
        "# print(\"Step 4: Verifying upload\")\n",
        "# print(\"=\" * 50)\n",
        "\n",
        "# # アップロードされたデータセットを確認\n",
        "# print(\"\\nChecking if dataset was uploaded successfully...\")\n",
        "# !kaggle datasets list -u $USERNAME | grep $DATASET_SLUG\n",
        "\n",
        "# print(\"\\n\" + \"=\" * 50)\n",
        "# print(\"Upload completed!\")\n",
        "# print(\"=\" * 50)\n",
        "# print(f\"\\n📦 Dataset URL: https://www.kaggle.com/datasets/{USERNAME}/{DATASET_SLUG}\")\n",
        "# print(\"\\n⚠️  Note: The dataset is currently PRIVATE.\")\n",
        "# print(\"To make it public, go to the dataset page and change the visibility settings.\")\n",
        "# print(\"\\n🔧 To use in Kaggle notebook:\")\n",
        "# print(f\"   Input path: /kaggle/input/{DATASET_SLUG}/\")\n",
        "\n",
        "# # ========================================\n",
        "# # 5. データセットの更新が必要な場合\n",
        "# # ========================================\n",
        "# print(\"\\n\" + \"=\" * 50)\n",
        "# print(\"For future updates:\")\n",
        "# print(\"=\" * 50)\n",
        "# print(\"If you need to update this dataset later, use:\")\n",
        "# print(f\"!kaggle datasets version -p {DST} -m 'Update message'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VrGGEHjrlu1X",
        "outputId": "39ca8139-b727-45bb-ea2f-dfe028aaf71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Step 1: Setting up Kaggle API\n",
            "==================================================\n",
            "Please upload your kaggle.json file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-820b59f8-e9c5-43e5-8669-7dfaf6fcf2a9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-820b59f8-e9c5-43e5-8669-7dfaf6fcf2a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "Kaggle API setup completed!\n",
            "\n",
            "Testing Kaggle API connection...\n",
            "ref                                  title                                                size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "-----------------------------------  ---------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "heptapod/titanic                     Titanic                                             11090  2017-05-16 08:14:22.210000         129925       1730  0.7058824        \n",
            "\n",
            "==================================================\n",
            "Step 2: Creating dataset metadata\n",
            "==================================================\n",
            "Model directory found: /content/phi4-merged\n",
            "\n",
            "Directory contents:\n",
            "total 27639236\n",
            "drwxr-xr-x 2 root root       4096 Oct  2 12:15 .\n",
            "drwxr-xr-x 1 root root       4096 Oct  2 12:18 ..\n",
            "-rw-r--r-- 1 root root        462 Oct  2 12:10 chat_template.jinja\n",
            "-rw-r--r-- 1 root root    1333565 Oct  2 12:10 classifier_weights.pt\n",
            "-rw-r--r-- 1 root root       3514 Oct  2 12:09 config.json\n",
            "-rw-r--r-- 1 root root        175 Oct  2 12:15 dataset-metadata.json\n",
            "-rw-r--r-- 1 root root       2664 Oct  2 12:10 label_encoder.joblib\n",
            "-rw-r--r-- 1 root root     916646 Oct  2 12:10 merges.txt\n",
            "-rw-r--r-- 1 root root 4933656240 Oct  2 12:09 model-00001-of-00006.safetensors\n",
            "-rw-r--r-- 1 root root 4954690400 Oct  2 12:09 model-00002-of-00006.safetensors\n",
            "-rw-r--r-- 1 root root 4902241040 Oct  2 12:10 model-00003-of-00006.safetensors\n",
            "-rw-r--r-- 1 root root 4771168824 Oct  2 12:10 model-00004-of-00006.safetensors\n",
            "-rw-r--r-- 1 root root 4771168824 Oct  2 12:10 model-00005-of-00006.safetensors\n",
            "-rw-r--r-- 1 root root 3958511376 Oct  2 12:10 model-00006-of-00006.safetensors\n",
            "-rw-r--r-- 1 root root         71 Oct  2 12:10 model_config.pkl\n",
            "-rw-r--r-- 1 root root      18891 Oct  2 12:10 model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root        381 Oct  2 12:10 question_label_mapping.pkl\n",
            "-rw-r--r-- 1 root root        463 Oct  2 12:10 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      17314 Oct  2 12:10 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    7153181 Oct  2 12:10 tokenizer.json\n",
            "-rw-r--r-- 1 root root    1612637 Oct  2 12:10 vocab.json\n",
            "\n",
            "Metadata created at: /content/phi4-merged/dataset-metadata.json\n",
            "Metadata content:\n",
            "{\n",
            "  \"title\": \"Phi-4 Reasoning Merged with Masked Loss\",\n",
            "  \"id\": \"masasato1999/phi4-merged-masked\",\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"other\"\n",
            "    }\n",
            "  ],\n",
            "  \"isPrivate\": true\n",
            "}\n",
            "\n",
            "==================================================\n",
            "Step 3: Uploading to Kaggle\n",
            "==================================================\n",
            "Uploading directory: /content/phi4-merged\n",
            "Dataset will be available at: kaggle.com/datasets/masasato1999/phi4-merged-masked\n",
            "\n",
            "This may take several minutes depending on the model size...\n",
            "Starting upload for file question_label_mapping.pkl\n",
            "100% 381/381 [00:00<00:00, 1.21kB/s]\n",
            "Upload successful: question_label_mapping.pkl (381B)\n",
            "Starting upload for file chat_template.jinja\n",
            "100% 462/462 [00:00<00:00, 1.50kB/s]\n",
            "Upload successful: chat_template.jinja (462B)\n",
            "Starting upload for file model_config.pkl\n",
            "100% 71.0/71.0 [00:00<00:00, 221B/s]\n",
            "Upload successful: model_config.pkl (71B)\n",
            "Starting upload for file classifier_weights.pt\n",
            "100% 1.27M/1.27M [00:00<00:00, 1.35MB/s]\n",
            "Upload successful: classifier_weights.pt (1MB)\n",
            "Starting upload for file vocab.json\n",
            "100% 1.54M/1.54M [00:01<00:00, 1.56MB/s]\n",
            "Upload successful: vocab.json (2MB)\n",
            "Starting upload for file tokenizer_config.json\n",
            "100% 16.9k/16.9k [00:00<00:00, 53.4kB/s]\n",
            "Upload successful: tokenizer_config.json (17KB)\n",
            "Starting upload for file merges.txt\n",
            "100% 895k/895k [00:00<00:00, 928kB/s]\n",
            "Upload successful: merges.txt (895KB)\n",
            "Starting upload for file tokenizer.json\n",
            "100% 6.82M/6.82M [00:01<00:00, 5.53MB/s]\n",
            "Upload successful: tokenizer.json (7MB)\n",
            "Starting upload for file label_encoder.joblib\n",
            "100% 2.60k/2.60k [00:00<00:00, 8.61kB/s]\n",
            "Upload successful: label_encoder.joblib (3KB)\n",
            "Starting upload for file model-00005-of-00006.safetensors\n",
            "100% 4.44G/4.44G [01:52<00:00, 42.3MB/s]\n",
            "Upload successful: model-00005-of-00006.safetensors (4GB)\n",
            "Starting upload for file model-00003-of-00006.safetensors\n",
            "100% 4.57G/4.57G [01:58<00:00, 41.5MB/s]\n",
            "Upload successful: model-00003-of-00006.safetensors (5GB)\n",
            "Starting upload for file special_tokens_map.json\n",
            "100% 463/463 [00:00<00:00, 1.32kB/s]\n",
            "Upload successful: special_tokens_map.json (463B)\n",
            "Starting upload for file model-00002-of-00006.safetensors\n",
            "100% 4.61G/4.61G [01:56<00:00, 42.7MB/s]\n",
            "Upload successful: model-00002-of-00006.safetensors (5GB)\n",
            "Starting upload for file model.safetensors.index.json\n",
            "100% 18.4k/18.4k [00:00<00:00, 54.2kB/s]\n",
            "Upload successful: model.safetensors.index.json (18KB)\n",
            "Starting upload for file model-00001-of-00006.safetensors\n",
            " 18% 825M/4.59G [00:20<01:41, 40.0MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbrr0Hl5lux9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "eNza9KRslutZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JF3f051qluo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}