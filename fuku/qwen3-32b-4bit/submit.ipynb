{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "設定ファイル - Deberta モデルのトレーニングと推論用設定\n",
    "\"\"\"\n",
    "\n",
    "# Model configuration\n",
    "VER = \"4bit\"\n",
    "MODEL_NAME = \"/kaggle/input/models/Qwen3-32B\"\n",
    "MODEL_TYPE = \"qwen2\"  # Add model type for proper handling\n",
    "EPOCHS = 3.1  # Reduce epochs for initial testing\n",
    "MAX_LEN = 300  # Increase max length for better context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"ver_{VER}\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 8  # Batch size 2 for RTX 5090 with 31GB VRAM\n",
    "EVAL_BATCH_SIZE = 8  # Eval can use larger batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Reduced to 32 for faster training\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/best_map3\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"qwen3-32b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"qwen3-32b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 128  # LoRAのランク - reduced for memory efficiency\n",
    "LORA_ALPHA = 256  # LoRAのスケーリングパラメータ - reduced proportionally\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "# 4-bit quantization settings\n",
    "USE_4BIT_QUANTIZATION = True  # Enable 4-bit quantization\n",
    "BNB_4BIT_COMPUTE_DTYPE = \"bfloat16\"  # Compute dtype for 4-bit base models\n",
    "BNB_4BIT_QUANT_TYPE = \"nf4\"  # Quantization type (fp4 or nf4)\n",
    "BNB_4BIT_USE_DOUBLE_QUANT = True  # Use double quantization\n",
    "BNB_4BIT_QUANT_STORAGE_DTYPE = \"uint8\"  # Storage dtype for quantized weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-index --no-deps /kaggle/input/bitsandbytes-20250725/bitsandbytes/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55522fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "共通ユーティリティ関数\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Qwen2.5-Math用の数学タスクに特化したプロンプト\n",
    "    prompt = (\n",
    "        \"<|im_start|>user\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder):\n",
    "    \"\"\"予測結果から提出用ファイルを作成\"\"\"\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    flat = top3.flatten()\n",
    "    decoded = label_encoder.inverse_transform(flat)\n",
    "    top3_labels = decoded.reshape(top3.shape)\n",
    "    pred_strings = [\" \".join(r) for r in top3_labels]\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'row_id': test_data.row_id.values,\n",
    "        'Category:Misconception': pred_strings\n",
    "    })\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f82676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qwen-3-0.6B モデル推論スクリプト - 提出用予測ファイルの生成\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "import gc\n",
    "# PEFTのインポートをオプショナルにする\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "\n",
    "\n",
    "# Kaggle環境ではカスタムクラスは不要\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（4bit量子化で読み込み）\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # 4bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
