{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":252296453,"sourceType":"kernelVersion"},{"sourceId":170579,"sourceType":"modelInstanceVersion","modelInstanceId":145133,"modelId":164048},{"sourceId":601080,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":450411,"modelId":466241}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n設定ファイル - Qwen2.5-32B モデルのトレーニングと推論用設定\n\"\"\"\n\n# Model configuration\nVER = \"4bit\"\nMODEL_NAME = \"/kaggle/input/qwen2.5/transformers/32b-instruct/1\"\nMODEL_TYPE = \"qwen2\"  # Qwen2.5 uses qwen2 architecture\nEPOCHS = 3  # Reduce epochs for initial testing\nMAX_LEN = 300  # Increase max length for better context\n\n# Directory settings\nOUTPUT_DIR = f\"/kaggle/input/qwen2.5-32b-0.9485/transformers/fulltrain/1/09485ft\"\n\n# Training parameters\nTRAIN_BATCH_SIZE = 8  # Batch size 2 for RTX 5090 with 31GB VRAM\nEVAL_BATCH_SIZE = 4  # Eval can use larger batch size\nGRADIENT_ACCUMULATION_STEPS = 8  # Reduced to 32 for faster training\nLEARNING_RATE = 2e-4\nLOGGING_STEPS = 50\nSAVE_STEPS = 229\nEVAL_STEPS = 229\n\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\nTEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n\n# Model save paths\nBEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\nLABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n\n# Other settings\nRANDOM_SEED = 42\nVALIDATION_SPLIT = 0.2\n\n# GPU settings\nCUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n\n# Submission settings\nSUBMISSION_OUTPUT_PATH = 'submission.csv'\n\n# WandB settings\nUSE_WANDB = True  # Set to False to disable WandB\nWANDB_PROJECT = \"qwen2.5-32b-math-misconceptions\"\nWANDB_RUN_NAME = f\"qwen2.5-32b-ver{VER}\"\nWANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n\n# Early stopping settings\nUSE_EARLY_STOPPING = True\nEARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\nEARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n\n# Evaluation/Save strategy\nEVALUATION_STRATEGY = \"steps\"\nSAVE_STRATEGY = \"steps\"\n\n# LoRA configuration\nLORA_RANK = 128  # LoRAのランク - reduced for memory efficiency\nLORA_ALPHA = 128  # LoRAのスケーリングパラメータ - reduced proportionally\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\nLORA_DROPOUT = 0.1  # LoRAのドロップアウト率\nLORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n\n# Memory optimization settings\nUSE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\nUSE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\nMAX_GRAD_NORM = 1.0  # Gradient clipping value\n\n# 4-bit quantization settings\n# 4-bit quantization settings\nUSE_4BIT_QUANTIZATION = True  # Enable 4-bit quantization\n# GPUとbf16対応状況に応じて自動選択\n\nBNB_4BIT_COMPUTE_DTYPE = \"float16\"\nBNB_4BIT_QUANT_TYPE = \"nf4\"  # Quantization type (fp4 or nf4)\nBNB_4BIT_USE_DOUBLE_QUANT = True  # Use double quantization\nBNB_4BIT_QUANT_STORAGE_DTYPE = \"uint8\"  # Storage dtype for quantized weights\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:39:07.740092Z","iopub.execute_input":"2025-10-08T05:39:07.740411Z","iopub.status.idle":"2025-10-08T05:39:07.748140Z","shell.execute_reply.started":"2025-10-08T05:39:07.740389Z","shell.execute_reply":"2025-10-08T05:39:07.747403Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!pip install --no-index --no-deps /kaggle/input/bitsandbytes-20250725/bitsandbytes/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:39:07.749382Z","iopub.execute_input":"2025-10-08T05:39:07.749617Z","iopub.status.idle":"2025-10-08T05:39:09.442707Z","shell.execute_reply.started":"2025-10-08T05:39:07.749601Z","shell.execute_reply":"2025-10-08T05:39:09.441941Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/bitsandbytes-20250725/bitsandbytes/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl\nbitsandbytes is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\"\"\"\n共通ユーティリティ関数\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nimport torch\n\n\ndef prepare_correct_answers(train_data):\n    \"\"\"正解答案データを準備\"\"\"\n    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n    correct = train_data.loc[idx].copy()\n    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n    correct = correct.sort_values('c', ascending=False)\n    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n    correct['is_correct'] = 1\n    return correct\n\n\ndef format_input(row):\n    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n    if row[\"is_correct\"]:\n        status = \"Yes\"\n    else:\n        status = \"No\"\n\n    # Qwen2.5用の数学誤解分析タスクに特化したプロンプト\n    prompt = (\n        \"<|im_start|>user\"\n        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct?: {status}\\n\"\n        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n        \"<|im_end|>\\n<|im_start|>assistant\\n\"\n        \"<think>\\n\"\n        \"Let me analyze this mathematical misconception...\\n\"\n        \"</think>\\n\\n\"\n        \"<|im_end|>\"\n    )\n    return prompt\n\n\ndef tokenize_dataset(dataset, tokenizer, max_len):\n    \"\"\"データセットをトークナイズ\"\"\"\n    def tokenize(batch):\n        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n        return tokenizer(\n            batch['text'],\n            padding=False,  # パディングはDataCollatorに任せる\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None  # map時は'None'を使用\n        )\n\n    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n    # columnsの設定時にlabelを保持\n    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n    dataset.set_format(type='torch', columns=columns)\n    return dataset\n\n\ndef compute_map3(eval_pred):\n    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n    logits, labels = eval_pred\n    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n    top3 = np.argsort(-probs, axis=1)[:, :3]\n    score = 0.0\n    for i, label in enumerate(labels):\n        ranks = top3[i]\n        if ranks[0] == label:\n            score += 1.0\n        elif ranks[1] == label:\n            score += 1.0 / 2\n        elif ranks[2] == label:\n            score += 1.0 / 3\n    return {\"map@3\": score / len(labels)}\n\n\ndef create_submission(predictions, test_data, label_encoder):\n    \"\"\"予測結果から提出用ファイルを作成\"\"\"\n    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n    top3 = np.argsort(-probs, axis=1)[:, :3]\n    flat = top3.flatten()\n    decoded = label_encoder.inverse_transform(flat)\n    top3_labels = decoded.reshape(top3.shape)\n    pred_strings = [\" \".join(r) for r in top3_labels]\n\n    submission = pd.DataFrame({\n        'row_id': test_data.row_id.values,\n        'Category:Misconception': pred_strings\n    })\n    return submission\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:39:09.443786Z","iopub.execute_input":"2025-10-08T05:39:09.444028Z","iopub.status.idle":"2025-10-08T05:39:09.457060Z","shell.execute_reply.started":"2025-10-08T05:39:09.444008Z","shell.execute_reply":"2025-10-08T05:39:09.456283Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\"\"\"\nQwen2.5-32B モデル推論スクリプト - 提出用予測ファイルの生成\n\"\"\"\n\nimport pandas as pd\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport time\nimport joblib\nimport torch\nimport gc\n# PEFT は必須。未インストール時は ImportError がそのまま表示されます。\nfrom peft import PeftModel, PeftConfig\n\n\n# Kaggle環境ではカスタムクラスは不要\n\n\ndef main():\n    \"\"\"メイン推論関数\"\"\"\n\n    # メモリキャッシュをクリア\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    # CUDAメモリ管理の最適化\n    import os\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n    # 2つのGPUを使用可能にする\n    if torch.cuda.device_count() > 1:\n        print(f\"Found {torch.cuda.device_count()} GPUs\")\n\n    print(\"Loading label encoder...\")\n    # ラベルエンコーダーの読み込み\n    le = joblib.load(LABEL_ENCODER_PATH)\n    n_classes = len(le.classes_)\n\n    print(\"Loading trained model and tokenizer...\")\n\n    # LoRAアダプターを使用する場合（PEFT は必須）\n    print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n    print(f\"Loading base model from: {MODEL_NAME}\")\n\n    # ベースモデルを読み込む（4bit量子化で読み込み）\n    from transformers import BitsAndBytesConfig\n\n    # config.pyの設定に基づいて4bit量子化を構成\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=getattr(torch, BNB_4BIT_COMPUTE_DTYPE),\n        bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,\n        bnb_4bit_use_double_quant=BNB_4BIT_USE_DOUBLE_QUANT,\n        bnb_4bit_quant_storage_dtype=getattr(torch, BNB_4BIT_QUANT_STORAGE_DTYPE)\n    )\n\n    try:\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME,\n            num_labels=n_classes,\n            trust_remote_code=True,\n            quantization_config=quantization_config,\n            device_map=\"auto\",  # 自動的に複数GPUに分散\n            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n        )\n    except ValueError as e:\n        # 4bit以外へのフォールバックは行わない方針。\n        # Kaggle等で \"dispatched on the CPU or the disk\" が発生する場合は\n        # VRAMやバッチ/シーケンス長の見直しを促し、明示的に失敗させる。\n        if \"dispatched on the CPU or the disk\" in str(e):\n            raise RuntimeError(\n                \"4bitロードに失敗しました。8bitフォールバックは無効です。\"\n                \"VRAMを増やすか、MAX_LEN/EVAL_BATCH_SIZEなどを下げて再実行してください。\"\n            ) from e\n        else:\n            raise\n\n    # LoRAアダプターを適用\n    model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n    # 注意: 4bit量子化モデル + LoRA を Trainer に渡す際、\n    # LoRA をベースへマージすると「純量子化モデル」扱いになり Trainer 初期化で失敗する。\n    # そのため推論では必ず PEFT ラッパーを保持し、マージしない。\n\n    # 推論モードに設定（メモリ効率化）\n    model.eval()\n    # 4bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n\n    # トークナイザーはベースモデルから読み込む\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n    print(\"Successfully loaded LoRA fine-tuned model\")\n\n    # パディングトークンの設定\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n    if hasattr(model, 'base_model'):\n        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n        # 内部のモデルにも設定\n        if hasattr(model.base_model, 'model'):\n            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n    else:\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n    print(\"Loading test data...\")\n    # テストデータの読み込み\n    test = pd.read_csv(TEST_DATA_PATH)\n\n    print(\"Loading training data for correct answers...\")\n    # 正解答案データの準備（訓練データから取得）\n    train = pd.read_csv(TRAIN_DATA_PATH)\n    train.Misconception = train.Misconception.fillna('NA')\n    correct = prepare_correct_answers(train)\n\n    print(\"Preprocessing test data...\")\n    # テストデータの前処理\n    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n    test.is_correct = test.is_correct.fillna(0)\n    test['text'] = test.apply(format_input, axis=1)\n\n    print(\"Tokenizing test data...\")\n    # テストデータのトークナイズ\n    ds_test = Dataset.from_pandas(test[['text']])\n    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n\n    # パディングのためのデータコラレータの設定\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    print(\"Running inference...\")\n\n    # TF32を有効化（推論速度向上）\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n\n    # 推論の実行\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,  # tokenizer の代替\n        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n        args=TrainingArguments(\n            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n            report_to=\"none\",    # wandbを無効化\n            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n            fp16=True,  # float16を使用\n            dataloader_pin_memory=True,  # データローダーの高速化\n            dataloader_num_workers=2,  # データ読み込みの並列化\n        )\n    )\n    # inference_mode で推論を実行（さらに軽量な no_grad）\n    with torch.inference_mode():\n        # モデルロード後の1件あたり推論時間を計測（データ全体を一括推論した実効平均）\n        start = time.perf_counter()\n        predictions = trainer.predict(ds_test)\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n        except Exception:\n            pass\n        elapsed = time.perf_counter() - start\n        num_samples = len(ds_test)\n        if num_samples > 0:\n            ms_per_sample = (elapsed / num_samples) * 1000.0\n            print(f\"Average inference time: {ms_per_sample:.2f} ms/sample (batch={EVAL_BATCH_SIZE}, samples={num_samples})\")\n\n    print(\"Creating submission file...\")\n    # 提出用ファイルの作成\n    submission = create_submission(predictions, test, le)\n\n    # ファイルの保存\n    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n    print(\"\\nSubmission preview:\")\n    print(submission.head())\n    print(f\"\\nSubmission shape: {submission.shape}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:39:09.458510Z","iopub.execute_input":"2025-10-08T05:39:09.458702Z","iopub.status.idle":"2025-10-08T05:39:09.958397Z","shell.execute_reply.started":"2025-10-08T05:39:09.458688Z","shell.execute_reply":"2025-10-08T05:39:09.957497Z"}},"outputs":[{"name":"stdout","text":"Found 2 GPUs\nLoading label encoder...\nLoading trained model and tokenizer...\nLoading fine-tuned LoRA model from: /kaggle/input/qwen2.5-32b-0.9485/transformers/fulltrain/1/09485ft/checkpoint-1722\nLoading base model from: /kaggle/input/qwen2.5/transformers/32b-instruct/1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1345021491.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/1345021491.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4648\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4649\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4650\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_library_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"],"ename":"ImportError","evalue":"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`","output_type":"error"}],"execution_count":11}]}