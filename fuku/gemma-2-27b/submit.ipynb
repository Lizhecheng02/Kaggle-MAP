{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T16:51:45.663764Z",
     "iopub.status.busy": "2025-07-23T16:51:45.663562Z",
     "iopub.status.idle": "2025-07-23T16:51:45.674490Z",
     "shell.execute_reply": "2025-07-23T16:51:45.673845Z",
     "shell.execute_reply.started": "2025-07-23T16:51:45.663746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "設定ファイル - Deberta モデルのトレーニングと推論用設定\n",
    "\"\"\"\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/gemma-3-4b\"\n",
    "MODEL_TYPE = \"qwen2\"  # Add model type for proper handling\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 512  # Increase max length for better context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"ver_{VER}\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 4  # Further reduced to avoid CUDA errors\n",
    "EVAL_BATCH_SIZE = 4  # Further reduced to avoid CUDA errors\n",
    "GRADIENT_ACCUMULATION_STEPS=16\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/best\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.00000001\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"gemma-3-4b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"gemma-3-4b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # Number of evaluations with no improvement after which training will be stopped\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # Minimum change in the monitored metric to qualify as an improvement\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16  # LoRAのランク\n",
    "LORA_ALPHA = 32  # LoRAのスケーリングパラメータ\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Gemma用の対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # バイアスの設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T16:51:45.676339Z",
     "iopub.status.busy": "2025-07-23T16:51:45.675782Z",
     "iopub.status.idle": "2025-07-23T16:51:56.789182Z",
     "shell.execute_reply": "2025-07-23T16:51:56.788585Z",
     "shell.execute_reply.started": "2025-07-23T16:51:45.676318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "共通ユーティリティ関数\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row['is_correct']:\n",
    "        status = \"This answer is correct.\"\n",
    "    else:\n",
    "        status = \"This answer is incorrect.\"\n",
    "\n",
    "    # 選択肢となる誤概念リストを定義\n",
    "    misconceptions = [\n",
    "        'Adding_across', 'Adding_terms', 'Additive', 'Base_rate', 'Certainty', 'Definition',\n",
    "        'Denominator-only_change', 'Division', 'Duplication', 'Firstterm', 'FlipChange',\n",
    "        'Ignores_zeroes', 'Incomplete', 'Incorrect_equivalent_fraction_addition', 'Interior',\n",
    "        'Inverse_operation', 'Inversion', 'Irrelevant', 'Longer_is_bigger', 'Mult',\n",
    "        'Multiplying_by_4', 'NA', 'Not_variable', 'Positive', 'Scale', 'Shorter_is_bigger',\n",
    "        'Subtraction', 'SwapDividend', 'Tacking', 'Unknowable', 'WNB', 'Whole_numbers_larger',\n",
    "        'Wrong_Fraction', 'Wrong_Operation', 'Wrong_fraction', 'Wrong_term'\n",
    "    ]\n",
    "    # リストをカンマ区切りの文字列に変換\n",
    "    choices = \", \".join(misconceptions)\n",
    "    # Gemma-3用のプロンプトフォーマット（選択肢を含む）\n",
    "    prompt = (\n",
    "        f\"<bos><start_of_turn>user\\n\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Student's Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Status: {status}\\n\"\n",
    "        f\"Student's Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        f\"Misconception choices: {choices}\\n\\n\"\n",
    "        f\"Task: Identify the mathematical misconception in this student's reasoning from the above list.\"\n",
    "        f\"<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder):\n",
    "    \"\"\"予測結果から提出用ファイルを作成\"\"\"\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    flat = top3.flatten()\n",
    "    decoded = label_encoder.inverse_transform(flat)\n",
    "    top3_labels = decoded.reshape(top3.shape)\n",
    "    pred_strings = [\" \".join(r) for r in top3_labels]\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'row_id': test_data.row_id.values,\n",
    "        'Category:Misconception': pred_strings\n",
    "    })\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T16:51:56.791238Z",
     "iopub.status.busy": "2025-07-23T16:51:56.790501Z",
     "iopub.status.idle": "2025-07-23T16:52:15.879775Z",
     "shell.execute_reply": "2025-07-23T16:52:15.878894Z",
     "shell.execute_reply.started": "2025-07-23T16:51:56.791216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gemma-3-1B モデル推論スクリプト - 提出用予測ファイルの生成\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "# PEFTのインポートをオプショナルにする\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# カスタムモジュールのインポート\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # GPUメモリをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # 複数GPU対応\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "    # メモリ効率化のための設定\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "\n",
    "        # カスタムGemmaモデルクラスを修正して、効率的に読み込む\n",
    "        import torch.nn as nn\n",
    "        from transformers import AutoModelForCausalLM\n",
    "\n",
    "        class GemmaForSequenceClassificationOptimized(nn.Module):\n",
    "            def __init__(self, model_name, num_labels):\n",
    "                super().__init__()\n",
    "                # device_mapを使って自動的に複数GPUに分散\n",
    "                self.gemma = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,  # 半精度で読み込み\n",
    "                    device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "                    low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "                )\n",
    "                self.config = self.gemma.config\n",
    "                self.dropout = nn.Dropout(0.1)\n",
    "                hidden_size = self.config.text_config.hidden_size if hasattr(self.config, 'text_config') else self.config.hidden_size\n",
    "                self.classifier = nn.Linear(hidden_size, num_labels).half()  # 分類器も半精度\n",
    "\n",
    "            def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, labels=None, **kwargs):\n",
    "                if input_ids is None and inputs_embeds is None:\n",
    "                    raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "                outputs = self.gemma(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "                hidden_states = outputs.hidden_states[-1] if hasattr(outputs, 'hidden_states') else outputs.last_hidden_state\n",
    "                pooled_output = hidden_states[:, -1, :]\n",
    "                pooled_output = self.dropout(pooled_output)\n",
    "                logits = self.classifier(pooled_output)\n",
    "\n",
    "                loss = None\n",
    "                if labels is not None:\n",
    "                    loss_fct = nn.CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "                if loss is not None:\n",
    "                    return {'loss': loss, 'logits': logits}\n",
    "                else:\n",
    "                    return {'logits': logits}\n",
    "\n",
    "        # 最適化されたモデルを使用\n",
    "        model = GemmaForSequenceClassificationOptimized(MODEL_NAME, num_labels=n_classes)\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # device_map=\"auto\"を使用しているため、DataParallelは不要\n",
    "        print(f\"Model loaded with automatic device mapping across {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=1,  # メモリ節約のため1に固定\n",
    "            fp16=True,  # 半精度を使用してメモリ節約\n",
    "            dataloader_num_workers=0,  # メモリ節約\n",
    "            gradient_checkpointing=False,  # 推論時は不要\n",
    "        )\n",
    "    )\n",
    "    predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 222398,
     "modelInstanceId": 239470,
     "sourceId": 282751,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 406927,
     "modelInstanceId": 387899,
     "sourceId": 486330,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
