{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Deberta Starter\n",
    "This notebook is a Deberta starter for Kaggle's \"MAP - Charting Student Math Misunderstandings\" competition. Currently the top public LB scoring notebooks use tf-idf. However using encoder LLM like Deberta can achieve a better CV score!\n",
    "\n",
    "This notebook demonstrates that using a small simple `Deberta-v3-xsmall` achieves CV 0.920+. If we use `Deberta-v3-base` instead, we achieve CV 0.930+ wow! Furthermore we can improve the text input to our LLM and improve CV score even more! We can also train 5-KFold or train with 100% data to boost LB score. (For deberta-small and xsmall `epochs=10` works best, for deberta-base use `epochs=6`).\n",
    "\n",
    "This notebook does both training and inference. However the most efficient way to submit to Kaggle competitions is to train models in one Kaggle notebook or train offline. Then save the model folder to a Kaggle dataset. Then make a second notebook that only loads the saved model and infers the test data. We submit the second notebook to the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "VER=1\n",
    "model_name = \"/root/kaggle/map-charting-student-math-misunderstandings/models/deberta-v3-xsmall\"\n",
    "EPOCHS = 10\n",
    "\n",
    "DIR = f\"ver_{VER}\"\n",
    "os.makedirs(DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "train.Misconception = train.Misconception.fillna('NA')\n",
    "train['target'] = train.Category+\":\"+train.Misconception\n",
    "train['label'] = le.fit_transform(train['target'])\n",
    "n_classes = len(le.classes_)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Powerful Feature Engineer\n",
    "We engineer one feature which we will use when formatting the input text for our LLM. Consider using more feature engineering and/or modifying the input text to our LLM. There is a discussion about this feature [here][1]\n",
    "\n",
    "[1]: https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings/discussion/589400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "train.is_correct = train.is_correct.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question EDA\n",
    "The train.csv has 15 multiple choice math questions. Below we display each of the questions and the 4 MC choices. The choices are sorted from (A) most popular selected to (D) least popular selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "# GET ANSWER CHOICES\n",
    "tmp = train.groupby(['QuestionId','MC_Answer']).size().reset_index(name='count')\n",
    "tmp['rank'] = tmp.groupby('QuestionId')['count'].rank(method='dense', ascending=False).astype(int) - 1\n",
    "tmp = tmp.drop('count',axis=1)\n",
    "tmp = tmp.sort_values(['QuestionId','rank'])\n",
    "\n",
    "# DISPLAY QUESTION AND ANSWER CHOICES\n",
    "Q = tmp.QuestionId.unique()\n",
    "for q in Q:\n",
    "    question = train.loc[train.QuestionId==q].iloc[0].QuestionText\n",
    "    choices = tmp.loc[tmp.QuestionId==q].MC_Answer.values\n",
    "    labels=\"ABCD\"\n",
    "    choice_str = \" \".join([f\"({labels[i]}) {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "    print()\n",
    "    display(Latex(f\"QuestionId {q}: {question}\") )\n",
    "    display(Latex(f\"MC Answers: {choice_str}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Transformers\n",
    "We will train our Deberta model using Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Train Data\n",
    "First we must tokenizer our data. Before we can tokenizer, we need to decide how to convert the multiple text columns into a single prompt. We will show our model the `QuestionText`, then the `MC_Answer` response, then use our `powerful feature engineer` to say whether this answer is `correct or incorrect`. Finally we will show our LLM the `StudentExplanation`.\n",
    "\n",
    "Consider changing the prompt below. Modifying the prompt can significantly improve our CV score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_input(row):\n",
    "    x = \"This answer is correct.\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"This is answer is incorrect.\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"{x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "train['text'] = train.apply(format_input,axis=1)\n",
    "print(\"Example prompt for our LLM:\")\n",
    "print()\n",
    "print( train.text.values[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lengths = [len(tokenizer.encode(t, truncation=False)) for t in train[\"text\"]]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "L = (np.array(lengths)>MAX_LEN).sum()\n",
    "print(f\"There are {L} train sample(s) with more than {MAX_LEN} tokens\")\n",
    "np.sort( lengths )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 20% Validation Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "COLS = ['text','label']\n",
    "train_ds = Dataset.from_pandas(train_df[COLS])\n",
    "val_ds = Dataset.from_pandas(val_df[COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "columns = ['input_ids', 'attention_mask', 'label']\n",
    "train_ds.set_format(type='torch', columns=columns)\n",
    "val_ds.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Train Model\n",
    "Let's initialize and train our model with HuggingFace trainer. We also define a custom metric of MAP@3 which is the competition metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DebertaV2ForSequenceClassification\n",
    "\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=n_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"./{DIR}\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\", #no for no saving\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=16*2,\n",
    "    per_device_eval_batch_size=32*2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"map@3\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CUSTOM MAP@3 METRIC\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]  # Top 3 predictions\n",
    "    match = (top3 == labels[:, None])\n",
    "\n",
    "    # Compute MAP@3 manually\n",
    "    map3 = 0\n",
    "    for i in range(len(labels)):\n",
    "        if match[i, 0]:\n",
    "            map3 += 1.0\n",
    "        elif match[i, 1]:\n",
    "            map3 += 1.0 / 2\n",
    "        elif match[i, 2]:\n",
    "            map3 += 1.0 / 3\n",
    "    return {\"map@3\": map3 / len(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_map3,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model\n",
    "This is how to save the files we need to upload to a Kaggle dataset for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "trainer.save_model(f\"{DIR}/best\")\n",
    "_ = joblib.dump(le, f\"{DIR}/label_encoder.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "This is how we load a Kaggle dataset for our inference notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{DIR}/best\")\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "    f\"{DIR}/best\",\n",
    "    num_labels=n_classes\n",
    ")\n",
    "training_args = TrainingArguments(report_to=\"none\")\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args)\n",
    "le = joblib.load(f\"{DIR}/label_encoder.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Predict Test \n",
    "We load test data, then engineer our powerful feature, then create prompt, then tokenize. Finally we infer test and generate probabilities for all 65 multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "print( test.shape )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "\n",
    "test['text'] = test.apply(format_input,axis=1)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds_test = Dataset.from_pandas(test[['text']])\n",
    "ds_test = ds_test.map(tokenize, batched=True)\n",
    "\n",
    "predictions = trainer.predict(ds_test)\n",
    "probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission CSV\n",
    "We create submission.csv by converting our top3 test preds into their class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get top 3 predicted class indices\n",
    "top3 = np.argsort(-probs, axis=1)[:, :3]   # shape: [num_samples, 3]\n",
    "\n",
    "# Decode numeric class indices to original string labels\n",
    "flat_top3 = top3.flatten()\n",
    "decoded_labels = le.inverse_transform(flat_top3)\n",
    "top3_labels = decoded_labels.reshape(top3.shape)\n",
    "\n",
    "# Join 3 labels per row with space\n",
    "joined_preds = [\" \".join(row) for row in top3_labels]\n",
    "\n",
    "# Save submission\n",
    "sub = pd.DataFrame({\n",
    "    \"row_id\": test.row_id.values,\n",
    "    \"Category:Misconception\": joined_preds\n",
    "})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 2663421,
     "sourceId": 4620664,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
