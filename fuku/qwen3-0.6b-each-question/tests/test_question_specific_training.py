#!/usr/bin/env python3
"""
QuestionIdåˆ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™:
1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
2. QuestionIdã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
3. ãƒ‘ã‚¹ç®¡ç†æ©Ÿèƒ½
4. ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
5. åŸºæœ¬çš„ãªå­¦ç¿’ãƒ»æ¨è«–ãƒ•ãƒ­ãƒ¼
"""

import os
import sys
import unittest
import tempfile
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import QUESTION_IDS, get_question_output_dir, get_question_model_path, get_question_label_encoder_path
from utils import (
    filter_data_by_question_id, 
    get_question_specific_labels,
    print_question_data_summary,
    save_question_results
)
from prompts import prompt_registry
from transformers import AutoTokenizer


class TestQuestionSpecificSystem(unittest.TestCase):
    """QuestionIdåˆ¥ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def setUp(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        self.sample_data = pd.DataFrame({
            'QuestionId': ['31772', '31774', '31772', '31777', '31774'],
            'Category': ['True_Correct', 'False_Misconception', 'True_Neither', 'True_Correct', 'False_Correct'],
            'Misconception': ['NA', 'SwapDividend', 'NA', 'NA', 'NA'],
            'QuestionText': [
                'What fraction of the shape is not shaded?',
                'Calculate 1/2 Ã· 6',
                'What fraction of the shape is not shaded?',
                'A box contains 120 counters',
                'Calculate 1/2 Ã· 6'
            ],
            'MC_Answer': ['A', 'B', 'C', 'A', 'D'],
            'StudentExplanation': [
                'I counted 3 out of 9 parts',
                'I multiplied instead of dividing',
                'Not sure about this',
                'I calculated 3/5 of 120',
                'I got the right answer'
            ],
            'is_correct': [1, 0, 1, 1, 1],
            'row_id': [1, 2, 3, 4, 5]
        })
        
        # targetã‚«ãƒ©ãƒ ã‚’ä½œæˆ
        self.sample_data['target'] = self.sample_data['Category'] + ':' + self.sample_data['Misconception']
    
    def test_config_loading(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
        # QuestionIDsãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒ†ã‚¹ãƒˆ
        self.assertIsInstance(QUESTION_IDS, list)
        self.assertGreater(len(QUESTION_IDS), 0)
        self.assertIn('31772', QUESTION_IDS)
        print(f"âœ… Config loading test passed. Found {len(QUESTION_IDS)} questions.")
    
    def test_path_management(self):
        """ãƒ‘ã‚¹ç®¡ç†æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        question_id = '31772'
        
        # ãƒ‘ã‚¹ç”Ÿæˆé–¢æ•°ã®ãƒ†ã‚¹ãƒˆ
        output_dir = get_question_output_dir(question_id)
        model_path = get_question_model_path(question_id)
        label_encoder_path = get_question_label_encoder_path(question_id)
        
        self.assertIn(question_id, output_dir)
        self.assertIn(question_id, model_path)
        self.assertIn(question_id, label_encoder_path)
        
        print(f"âœ… Path management test passed.")
        print(f"  Output dir: {output_dir}")
        print(f"  Model path: {model_path}")
        print(f"  Label encoder path: {label_encoder_path}")
    
    def test_data_filtering(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        question_id = '31772'
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ
        filtered_data = filter_data_by_question_id(self.sample_data, question_id)
        
        self.assertEqual(len(filtered_data), 2)  # 31772ã¯2ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹
        self.assertTrue(all(filtered_data['QuestionId'] == question_id))
        
        print(f"âœ… Data filtering test passed. Filtered {len(filtered_data)} samples for Question {question_id}")
    
    def test_label_processing(self):
        """ãƒ©ãƒ™ãƒ«å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""
        question_id = '31772'
        filtered_data = filter_data_by_question_id(self.sample_data, question_id)
        
        # ãƒ©ãƒ™ãƒ«ã®å–å¾—ã¨å‡¦ç†
        labels = get_question_specific_labels(filtered_data)
        le = LabelEncoder()
        encoded_labels = le.fit_transform(filtered_data['target'])
        
        self.assertGreater(len(labels), 0)
        self.assertEqual(len(encoded_labels), len(filtered_data))
        
        print(f"âœ… Label processing test passed. Found {len(labels)} unique labels for Question {question_id}")
        print(f"  Labels: {labels}")
    
    def test_prompt_generation(self):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–ï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«è»½é‡ãªã‚‚ã®ã‚’ä½¿ç”¨ï¼‰
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
        except:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            print("âš ï¸  Tokenizer not available, skipping prompt generation test")
            return
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–¢æ•°ã®å–å¾—
        prompt_function = prompt_registry.get('create_prompt_v2')
        if prompt_function is None:
            print("âš ï¸  Prompt function not available, skipping test")
            return
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã‚’ãƒ†ã‚¹ãƒˆ
        sample_row = self.sample_data.iloc[0]
        try:
            prompt = prompt_function(tokenizer, sample_row)
            self.assertIsInstance(prompt, str)
            self.assertGreater(len(prompt), 0)
            print(f"âœ… Prompt generation test passed.")
            print(f"  Sample prompt length: {len(prompt)} characters")
        except Exception as e:
            print(f"âš ï¸  Prompt generation test failed: {str(e)}")
    
    def test_results_saving(self):
        """çµæœä¿å­˜æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        question_id = '31772'
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®çµæœãƒ‡ãƒ¼ã‚¿
        test_results = {
            'question_id': question_id,
            'n_classes': 3,
            'n_samples': 10,
            'final_map3': 0.85,
            'test_flag': True
        }
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        with tempfile.TemporaryDirectory() as temp_dir:
            save_question_results(question_id, test_results, temp_dir)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
            results_file = os.path.join(temp_dir, f'question_{question_id}_results.json')
            self.assertTrue(os.path.exists(results_file))
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
            import json
            with open(results_file, 'r') as f:
                saved_results = json.load(f)
            
            self.assertEqual(saved_results['question_id'], question_id)
            self.assertEqual(saved_results['final_map3'], 0.85)
        
        print(f"âœ… Results saving test passed.")
    
    def test_data_summary(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼è¡¨ç¤ºã®ãƒ†ã‚¹ãƒˆ"""
        question_id = '31772'
        filtered_data = filter_data_by_question_id(self.sample_data, question_id)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆå‡ºåŠ›ã®ã‚­ãƒ£ãƒ—ãƒãƒ£ã¯é›£ã—ã„ã®ã§ã€ã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã„ã“ã¨ã‚’ç¢ºèªï¼‰
        try:
            print_question_data_summary(filtered_data, question_id)
            print(f"âœ… Data summary test passed.")
        except Exception as e:
            self.fail(f"Data summary failed: {str(e)}")


class TestSystemIntegration(unittest.TestCase):
    """ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    def test_question_ids_consistency(self):
        """QuestionIDsã®æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ"""
        from prompt_utils import questions
        
        # prompt_utilsã®questionsã¨config.pyã®QUESTION_IDSãŒä¸€è‡´ã™ã‚‹ã‹ãƒ†ã‚¹ãƒˆ
        prompt_utils_ids = set(questions.keys())
        config_ids = set(QUESTION_IDS)
        
        self.assertEqual(prompt_utils_ids, config_ids, 
                        "QuestionIDs in prompt_utils.py and config.py should match")
        
        print(f"âœ… QuestionIDs consistency test passed. {len(QUESTION_IDS)} questions found.")
    
    def test_directory_structure(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ç¢ºèª
        with tempfile.TemporaryDirectory() as temp_dir:
            # å…ƒã®OUTPUT_DIRã‚’ä¸€æ™‚çš„ã«å¤‰æ›´
            original_output_dir = globals().get('OUTPUT_DIR', 'ver_2')
            
            try:
                # config.pyã®OUTPUT_DIRã‚’ä¸€æ™‚çš„ã«å¤‰æ›´
                import config
                config.OUTPUT_DIR = temp_dir
                
                # å„QuestionIdã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ç¢ºèª
                for question_id in QUESTION_IDS[:3]:  # æœ€åˆã®3ã¤ã ã‘ãƒ†ã‚¹ãƒˆ
                    output_dir = get_question_output_dir(question_id)
                    self.assertIn(temp_dir, output_dir)
                    self.assertIn(question_id, output_dir)
                
                print(f"âœ… Directory structure test passed.")
                
            finally:
                # å…ƒã®è¨­å®šã‚’å¾©å…ƒ
                config.OUTPUT_DIR = original_output_dir


def run_all_tests():
    """å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("="*80)
    print("QuestionIdåˆ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æ¨è«–ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("="*80)
    
    # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®ä½œæˆ
    test_suite = unittest.TestSuite()
    
    # åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
    test_suite.addTest(TestQuestionSpecificSystem('test_config_loading'))
    test_suite.addTest(TestQuestionSpecificSystem('test_path_management'))
    test_suite.addTest(TestQuestionSpecificSystem('test_data_filtering'))
    test_suite.addTest(TestQuestionSpecificSystem('test_label_processing'))
    test_suite.addTest(TestQuestionSpecificSystem('test_prompt_generation'))
    test_suite.addTest(TestQuestionSpecificSystem('test_results_saving'))
    test_suite.addTest(TestQuestionSpecificSystem('test_data_summary'))
    
    # çµ±åˆãƒ†ã‚¹ãƒˆ
    test_suite.addTest(TestSystemIntegration('test_question_ids_consistency'))
    test_suite.addTest(TestSystemIntegration('test_directory_structure'))
    
    # ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã®å®Ÿè¡Œ
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    print("\n" + "="*80)
    if result.wasSuccessful():
        print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ!")
        print("QuestionIdåˆ¥ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã™ã‚‹æº–å‚™ãŒã§ãã¦ã„ã¾ã™ã€‚")
    else:
        print(f"âŒ {len(result.failures)} ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã€{len(result.errors)} ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        print("ã‚·ã‚¹ãƒ†ãƒ ã‚’ä¿®æ­£ã—ã¦ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
    print("="*80)
    
    return result.wasSuccessful()


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)