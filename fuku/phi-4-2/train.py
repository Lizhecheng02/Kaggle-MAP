"""
Phi-4 ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    AutoConfig
)
from datasets import Dataset
import joblib
import torch
import torch.nn as nn
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from transformers import AutoModel
import wandb
from transformers import EarlyStoppingCallback, TrainerCallback
import gc

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config import *
from utils import prepare_correct_answers, format_input, tokenize_dataset, compute_map3
from models import HierPhi4ForSequenceClassification
from data_collator import DataCollatorWithPadding


class GradientCheckCallback(TrainerCallback):
    """æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã§å‹¾é…ãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    def __init__(self):
        self.checked = False

    def on_step_end(self, args, state, control, model=None, **kwargs):
        if not self.checked and state.global_step == 1:
            self.checked = True
            print("\n" + "="*60)
            print("Gradient Check (Step 1)")
            print("="*60)

            # trainable ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’ç¢ºèª
            params_with_grad = []
            params_without_grad = []
            for name, param in model.named_parameters():
                if param.requires_grad:
                    if param.grad is not None:
                        grad_norm = param.grad.norm().item()
                        params_with_grad.append((name, grad_norm))
                    else:
                        params_without_grad.append(name)

            print(f"Parameters with gradients: {len(params_with_grad)}")
            if params_with_grad:
                print("Sample gradient norms:")
                for name, norm in params_with_grad[:5]:
                    print(f"  {name}: {norm:.6f}")

            if params_without_grad:
                print(f"\nâš  Warning: {len(params_without_grad)} trainable parameters have no gradients!")
                print("Sample parameters without gradients:")
                for name in params_without_grad[:5]:
                    print(f"  {name}")
            else:
                print("âœ“ All trainable parameters have gradients!")

            print("="*60 + "\n")

        return control


class SaveBestMap3Callback(TrainerCallback):
    """eval_{METRIC_NAME} ãŒæœ€é«˜å€¤ã‚’æ›´æ–°ã—ãŸéš›ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    def __init__(self, save_dir, tokenizer, metric_name: str = METRIC_NAME):
        self.save_dir = save_dir
        self.tokenizer = tokenizer
        self.best_map3 = 0.0
        self.metric_name = metric_name
        self.metric_key = f"eval_{self.metric_name}"

    def on_evaluate(self, args, state, control, metrics, model=None, **kwargs):
        # ãƒ‡ãƒãƒƒã‚°: åˆå›ã®ã¿ metrics ã®ã‚­ãƒ¼ä¸€è¦§ã‚’å‡ºåŠ›
        if not hasattr(self, "_printed_keys"):
            try:
                print(f"[Debug] on_evaluate metrics keys: {sorted(list(metrics.keys()))}")
            except Exception:
                pass
            self._printed_keys = True

        current_map3 = metrics.get(self.metric_key, None)
        current_step = state.global_step
        total_steps = state.max_steps if state.max_steps else "N/A"

        if current_map3 is None:
            print(f"\n[Step {current_step}/{total_steps}] æ³¨æ„: metrics ã« '{self.metric_key}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆcompute_metricsæœªå®Ÿè¡Œã®å¯èƒ½æ€§ï¼‰ã€‚")
            current_map3 = 0.0
        print(f"[Step {current_step}/{total_steps}] è©•ä¾¡å®Ÿè¡Œ - MAP@{MAP_K}ã‚¹ã‚³ã‚¢: {current_map3:.4f}")

        if current_map3 > self.best_map3:
            self.best_map3 = current_map3

            # å°‚ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            best_map3_path = os.path.join(self.save_dir, 'best_map3')
            os.makedirs(best_map3_path, exist_ok=True)

            # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜
            model.save_pretrained(best_map3_path)
            self.tokenizer.save_pretrained(best_map3_path)

            print(f"ğŸ‰ æ–°ã—ã„ãƒ™ã‚¹ãƒˆMAP@{MAP_K}ã‚¹ã‚³ã‚¢æ›´æ–°: {current_map3:.4f} (Step {current_step}) - ãƒ¢ãƒ‡ãƒ«ã‚’ {best_map3_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        else:
            print(f"ç¾åœ¨ã®ãƒ™ã‚¹ãƒˆMAP@{MAP_K}ã‚¹ã‚³ã‚¢: {self.best_map3:.4f} (å¤‰æ›´ãªã—)")

        return control


class Phi4ForSequenceClassification(nn.Module):
    """Phi-4ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º"""
    def __init__(self, backbone, num_labels):
        """
        Args:
            backbone: äº‹å‰ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPhi-4 base model
            num_labels: åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°
        """
        super().__init__()
        self.phi = backbone
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.phi.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.phi(input_ids=input_ids, attention_mask=attention_mask)
        # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
        pooled_output = outputs.last_hidden_state[:, -1, :]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        return type('Output', (), {'loss': loss, 'logits': logits})()


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°"""

    # config.pyã®å†…å®¹ã‚’å‡ºåŠ›
    print("=" * 80)
    print("Configuration Settings (config.py):")
    print("=" * 80)
    with open('config.py', 'r', encoding='utf-8') as f:
        print(f.read())
    print("=" * 80)
    print()

    # WandBã®åˆæœŸåŒ–
    if USE_WANDB:
        wandb.init(
            project=WANDB_PROJECT,
            name=WANDB_RUN_NAME,
            entity=WANDB_ENTITY,
            config={
                "model_name": MODEL_NAME,
                "epochs": EPOCHS,
                "max_len": MAX_LEN,
                "train_batch_size": TRAIN_BATCH_SIZE,
                "eval_batch_size": EVAL_BATCH_SIZE,
                "learning_rate": LEARNING_RATE,
                "early_stopping_patience": EARLY_STOPPING_PATIENCE if USE_EARLY_STOPPING else None,
                "lora_rank": LORA_RANK,
                "lora_alpha": LORA_ALPHA,
                "lora_target_modules": LORA_TARGET_MODULES,
                "lora_dropout": LORA_DROPOUT,
                "lora_bias": LORA_BIAS,
                "use_dora": USE_DORA,
                "attention_implementation": ATTENTION_IMPLEMENTATION,
            }
        )

    # GPUè¨­å®š
    if CUDA_VISIBLE_DEVICES is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES
        print(f"Using CUDA device(s): {CUDA_VISIBLE_DEVICES}")

    # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
    torch.cuda.empty_cache()
    gc.collect()

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç† ---
    print("Loading and preprocessing training data...")
    le = LabelEncoder()
    train = pd.read_csv(TRAIN_DATA_PATH)
    train.Misconception = train.Misconception.fillna('NA')
    train['target'] = train.Category + ":" + train.Misconception
    train['label'] = le.fit_transform(train['target'])
    n_classes = len(le.classes_)
    print(f"Train shape: {train.shape} with {n_classes} target classes")

    # --- éšå±¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆCategory / Misconception:NAå«ã‚€ï¼‰---
    from sklearn.preprocessing import LabelEncoder as _LE
    le_cat = _LE()
    le_mc = _LE()
    train['label_cat'] = le_cat.fit_transform(train['Category'].astype(str))
    train['label_mc'] = le_mc.fit_transform(train['Misconception'].astype(str))

    # joint -> (cat_idx, mc_idx) ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    classes_joint = list(le.classes_)
    cat_for_joint = []
    mc_for_joint = []
    for t in classes_joint:
        c, m = t.split(":", 1)
        cat_for_joint.append(c)
        mc_for_joint.append(m)
    joint_to_cat = torch.tensor(le_cat.transform(cat_for_joint), dtype=torch.long)
    joint_to_mc = torch.tensor(le_mc.transform(mc_for_joint), dtype=torch.long)

    # Category ãŒ *_Misconception ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
    cat_is_misconc = pd.Series(le_cat.classes_).astype(str).str.endswith('_Misconception').values
    cat_is_misconc = torch.tensor(cat_is_misconc, dtype=torch.bool)

    # Misconception ã® NA index
    if 'NA' not in set(le_mc.classes_):
        raise RuntimeError("Misconception ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã« 'NA' ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    mc_na_index = int(le_mc.transform(['NA'])[0])

    # --- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    print("Performing feature engineering...")
    correct = prepare_correct_answers(train)
    train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')
    train.is_correct = train.is_correct.fillna(0)

    # --- å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ ---
    print("Formatting input text...")
    train['text'] = train.apply(format_input, axis=1)
    print("Example prompt for our LLM:")
    print(train.text.values[0])

    # --- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ– ---
    print("Initializing tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    # Phi-4ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã®è¨­å®š
    if tokenizer.pad_token is None:
        # Phi-4ã§ã¯ç‰¹åˆ¥ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨
        tokenizer.pad_token = "<|finetune_right_pad_id|>"
        tokenizer.pad_token_id = 100257

    # --- ãƒˆãƒ¼ã‚¯ãƒ³é•·ã®åˆ†æ ---
    print("Analyzing token lengths...")
    lengths = [len(tokenizer.encode(t, truncation=False)) for t in train['text']]
    plt.figure(figsize=(10, 6))
    plt.hist(lengths, bins=50)
    plt.title("Token Length Distribution")
    plt.xlabel("Number of tokens")
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.savefig(f'{OUTPUT_DIR}/token_length_distribution.png')
    plt.close()

    over_limit = (np.array(lengths) > MAX_LEN).sum()
    print(f"There are {over_limit} train sample(s) with more than {MAX_LEN} tokens")

    # --- ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰² ---
    print("Splitting data into train and validation sets...")
    train_df, val_df = train_test_split(train, test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED)
    COLS = ['text','label','label_cat','label_mc']
    train_ds = Dataset.from_pandas(train_df[COLS])
    val_ds = Dataset.from_pandas(val_df[COLS])

    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ---
    print("Tokenizing datasets...")
    train_ds = tokenize_dataset(train_ds, tokenizer, MAX_LEN)
    val_ds = tokenize_dataset(val_ds, tokenizer, MAX_LEN)

    # --- Label Encoderã®ä¿å­˜ ---
    print(f"Saving label encoder (joint) to: {LABEL_ENCODER_PATH}")
    joblib.dump(le, LABEL_ENCODER_PATH)
    # éšå±¤ãƒ¡ã‚¿ã®ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    print(f"Saving hierarchical metadata to: {HIER_META_PATH}")
    hier_meta = {
        'le_cat': le_cat,
        'le_mc': le_mc,
        'joint_to_cat': joint_to_cat,
        'joint_to_mc': joint_to_mc,
        'cat_is_misconc': cat_is_misconc,
        'mc_na_index': mc_na_index,
        'joint_classes': classes_joint,
    }
    joblib.dump(hier_meta, HIER_META_PATH)

    # --- ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– ---
    print("Initializing model...")
    print(f"Using attention implementation: {ATTENTION_IMPLEMENTATION}")

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã®è¨ˆæ¸¬é–‹å§‹
    import time
    load_start_time = time.time()

    # ãƒ™ãƒ¼ã‚¹ï¼ˆLLMæœ¬ä½“ï¼‰ã‚’èª­ã¿è¾¼ã‚€
    # device_map="auto"ã§ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥GPUã«åŠ¹ç‡çš„ã«ãƒ­ãƒ¼ãƒ‰
    base_model = AutoModel.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        device_map="auto",  # ä¿®æ­£: Noneã‹ã‚‰"auto"ã«å¤‰æ›´ã—ã€åŠ¹ç‡çš„ãªGPUãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿç¾
        torch_dtype=torch.bfloat16,  # BF16ã§èª­ã¿è¾¼ã¿
        low_cpu_mem_usage=True,
        attn_implementation=ATTENTION_IMPLEMENTATION
    )

    load_end_time = time.time()
    load_duration = load_end_time - load_start_time
    print(f"Model loaded in {load_duration:.2f} seconds")

    # GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç¢ºèª
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB")

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¨­å®š
    base_model.config.pad_token_id = tokenizer.pad_token_id

    if HIERARCHICAL_TRAINING and COMPOSE_JOINT_FROM_HEADS:
        print("Using Hierarchical multi-task model (cat/mc heads + composed joint)")
        model_core = HierPhi4ForSequenceClassification(
            backbone=base_model,
            hidden_size=base_model.config.hidden_size,
            n_joint=n_classes,
            n_cat=len(le_cat.classes_),
            n_mc=len(le_mc.classes_),
            joint_to_cat=joint_to_cat,
            joint_to_mc=joint_to_mc,
            cat_is_misconc=cat_is_misconc,
            mc_na_index=mc_na_index,
            lambda_cat=LAMBDA_CAT,
            lambda_mc=LAMBDA_MC,
            lambda_constraint=LAMBDA_CONSTRAINT,
        )
    else:
        print("Fallback: using simple single-head classifier for joint labels")
        # äº’æ›ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
        # ä¿®æ­£: backboneã‚’ç›´æ¥æ¸¡ã™ã“ã¨ã§ã€ä¸è¦ãªå†ãƒ­ãƒ¼ãƒ‰ã‚’å›é¿
        model_core = Phi4ForSequenceClassification(backbone=base_model, num_labels=n_classes)

    # --- LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨­å®š ---
    print("Configuring LoRA adapter...")
    modules_to_save = None
    if HIERARCHICAL_TRAINING and COMPOSE_JOINT_FROM_HEADS:
        modules_to_save = ["fc_cat", "fc_mc"]

    lora_config = LoraConfig(
        r=LORA_RANK,  # LoRAã®ãƒ©ãƒ³ã‚¯
        lora_alpha=LORA_ALPHA,  # LoRAã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        target_modules=LORA_TARGET_MODULES,  # å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        lora_dropout=LORA_DROPOUT,
        bias=LORA_BIAS,
        task_type=TaskType.SEQ_CLS,
        use_dora=USE_DORA,  # DoRAã®ä½¿ç”¨
        modules_to_save=modules_to_save,
    )

    # PEFTãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = get_peft_model(model_core, lora_config)
    print("Number of trainable parameters:")
    model.print_trainable_parameters()

    # --- Gradient checkpointing ã¨ input gradients ã®è¨­å®š ---
    print("Configuring gradient checkpointing and input gradients...")

    # 1. enable_input_require_grads() ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
    if hasattr(model, 'enable_input_require_grads'):
        model.enable_input_require_grads()
        print("âœ“ enable_input_require_grads() called on PeftModel")
    else:
        print("âš  Warning: model does not have enable_input_require_grads()")

    # 2. base_model ã«å¯¾ã—ã¦ã‚‚æ˜ç¤ºçš„ã«è¨­å®šï¼ˆå¿µã®ãŸã‚ï¼‰
    if hasattr(model, 'base_model') and hasattr(model.base_model, 'enable_input_require_grads'):
        model.base_model.enable_input_require_grads()
        print("âœ“ enable_input_require_grads() called on base_model")

    # 3. gradient checkpointing ã‚’ backbone ã«å¯¾ã—ã¦æœ‰åŠ¹åŒ–
    try:
        # HierPhi4 ã®å ´åˆ: model.base_model.backbone
        if hasattr(model.base_model, 'backbone') and hasattr(model.base_model.backbone, 'gradient_checkpointing_enable'):
            model.base_model.backbone.gradient_checkpointing_enable()
            print("âœ“ gradient_checkpointing_enable() called on backbone")
        # å˜ç´”ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ: model.base_model
        elif hasattr(model.base_model, 'gradient_checkpointing_enable'):
            model.base_model.gradient_checkpointing_enable()
            print("âœ“ gradient_checkpointing_enable() called on base_model")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        elif hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            print("âœ“ gradient_checkpointing_enable() called on model")
        else:
            print("âš  Warning: could not find gradient_checkpointing_enable() method")
    except Exception as e:
        print(f"âš  Warning: failed to enable gradient checkpointing: {e}")

    # 4. requires_grad çŠ¶æ…‹ã‚’ç¢ºèª
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    print(f"Trainable parameters count: {len(trainable_params)}")
    if len(trainable_params) > 0:
        print(f"Sample trainable parameter requires_grad: {trainable_params[0].requires_grad}")
    else:
        print("âš  Warning: No trainable parameters found!")

    # device_map="auto"ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«GPUã«é…ç½®ã•ã‚Œã¦ã„ã¾ã™
    # æ˜ç¤ºçš„ãªcuda()å‘¼ã³å‡ºã—ã¯ä¸è¦
    # ï¼ˆdevice_map="auto"ãŒbase_modelã‚’è‡ªå‹•çš„ã«æœ€é©é…ç½®ï¼‰
    print(f"Model device: {next(model.parameters()).device}")

    # è¿½åŠ ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # --- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®è¨­å®š ---
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        do_train=True,
        do_eval=True,
        eval_strategy="steps",
        save_strategy="steps",
        eval_steps=EVAL_STEPS,
        save_steps=SAVE_STEPS,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=EVAL_BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        logging_dir=f"{OUTPUT_DIR}/logs",
        logging_steps=LOGGING_STEPS,
        metric_for_best_model=None,
        greater_is_better=True,
        # Trainerã®ãƒ™ã‚¹ãƒˆåˆ¤å®šã«ä¾å­˜ã—ãªã„ï¼ˆç‹¬è‡ªã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ä¿å­˜ï¼‰
        load_best_model_at_end=False,
        # äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ã‚’ä¿æŒã—ã¦compute_metricsã‚’å¿…ãšå®Ÿè¡Œ
        prediction_loss_only=False,
        report_to="wandb" if USE_WANDB else "none",
        bf16=True,  # BF16ã‚’ä½¿ç”¨
        gradient_checkpointing=True,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚æœ‰åŠ¹åŒ–
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šã®ãŸã‚è¿½åŠ 
        remove_unused_columns=False,  # ã‚«ãƒ©ãƒ ã‚’å‰Šé™¤ã—ãªã„
        lr_scheduler_type="cosine",  # ã‚³ã‚µã‚¤ãƒ³ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨
        warmup_ratio=0.0,  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’ç„¡åŠ¹åŒ–
        save_total_limit=2,
        max_grad_norm=MAX_GRAD_NORM,  # Gradient clipping
        optim="adamw_bnb_8bit" if USE_8BIT_ADAM else "adamw_torch",  # 8-bit Adam optimizer
    )

    # --- ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ---
    print("Setting up trainer...")
    # ãƒ‡ãƒãƒƒã‚°: prediction_loss_only ã‚’æ˜ç¤ºçš„ã«ç¢ºèª
    print(f"TrainingArguments.prediction_loss_only = {training_args.prediction_loss_only}")

    # ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—
    steps_per_epoch = len(train_ds) // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)  # gradient_accumulation_stepsã‚’è€ƒæ…®
    total_steps = steps_per_epoch * EPOCHS
    print(f"\nDataset statistics:")
    print(f"Training samples: {len(train_ds)}")
    print(f"Validation samples: {len(val_ds)}")
    print(f"Batch size: {TRAIN_BATCH_SIZE} (with gradient accumulation: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})")
    print(f"Steps per epoch: {steps_per_epoch}")
    print(f"Total training steps: {total_steps}")
    print(f"Evaluation interval: every {EVAL_STEPS} steps (~{EVAL_STEPS/steps_per_epoch:.2f} epochs)")
    print(f"Early stopping after {EARLY_STOPPING_PATIENCE} evaluations without improvement")

    # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=MAX_LEN)

    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š
    callbacks = []

    # GradientCheckCallbackã‚’è¿½åŠ ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    gradient_check_callback = GradientCheckCallback()
    callbacks.append(gradient_check_callback)
    print("GradientCheckCallback enabled - æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã§å‹¾é…ã‚’ç¢ºèªã—ã¾ã™")

    # SaveBestMap3Callbackã‚’è¿½åŠ 
    save_best_callback = SaveBestMap3Callback(save_dir=OUTPUT_DIR, tokenizer=tokenizer, metric_name=METRIC_NAME)
    callbacks.append(save_best_callback)
    print(f"SaveBestMap3Callback enabled - ãƒ¢ãƒ‡ãƒ«ã¯ {OUTPUT_DIR}/best_map3 ã«ä¿å­˜ã•ã‚Œã¾ã™")

    if USE_EARLY_STOPPING:
        # EARLY_STOPPING_PATIENCEã¯è©•ä¾¡å›æ•°ã¨ã—ã¦ç›´æ¥ä½¿ç”¨
        early_stopping_callback = EarlyStoppingCallback(
            early_stopping_patience=EARLY_STOPPING_PATIENCE,
            early_stopping_threshold=EARLY_STOPPING_THRESHOLD
        )
        callbacks.append(early_stopping_callback)
        print(f"Early stopping enabled:")
        print(f"  - Patience (evaluations without improvement): {EARLY_STOPPING_PATIENCE}")
        print(f"  - Threshold: {EARLY_STOPPING_THRESHOLD}")

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_map3,
        callbacks=callbacks,
    )

    print("Starting training...")
    # ãƒ‡ãƒãƒƒã‚°: ãƒ©ãƒ™ãƒ«å
    try:
        print(f"Trainer.label_names = {getattr(trainer, 'label_names', None)}")
    except Exception:
        pass
    trainer.train()

    # --- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµ‚äº†å¾Œã®æœ€çµ‚è©•ä¾¡ ---
    print("\n" + "="*60)
    print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº† - æœ€çµ‚è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...")
    print("="*60)
    final_eval_results = trainer.evaluate()
    final_map3 = final_eval_results.get(f"eval_{METRIC_NAME}", 0.0)
    print(f"\nğŸ æœ€çµ‚è©•ä¾¡çµæœ:")
    print(f"   æœ€çµ‚MAP@{MAP_K}ã‚¹ã‚³ã‚¢: {final_map3:.4f}")
    print(f"   å…¨ä½“ã®ãƒ™ã‚¹ãƒˆMAP@{MAP_K}ã‚¹ã‚³ã‚¢: {save_best_callback.best_map3:.4f}")

    # æœ€çµ‚è©•ä¾¡ãŒæ–°ã—ã„ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã®å ´åˆã€æ˜ç¤ºçš„ã«ä¿å­˜
    if final_map3 > save_best_callback.best_map3:
        print(f"ğŸ‰ æœ€çµ‚è©•ä¾¡ã§æ–°ã—ã„ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢é”æˆï¼ {final_map3:.4f} > {save_best_callback.best_map3:.4f}")
        save_best_callback.best_map3 = final_map3
        best_map3_path = os.path.join(OUTPUT_DIR, 'best_map3')
        os.makedirs(best_map3_path, exist_ok=True)
        model.save_pretrained(best_map3_path)
        tokenizer.save_pretrained(best_map3_path)
        print(f"   æœ€çµ‚ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ {best_map3_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # --- ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ ---
    print("\nSaving model...")
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜
    model.save_pretrained(BEST_MODEL_PATH)
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚‚ä¿å­˜
    tokenizer.save_pretrained(BEST_MODEL_PATH)

    print("Training completed successfully!")
    print(f"Model saved to: {BEST_MODEL_PATH}")
    print(f"Label encoder saved to: {LABEL_ENCODER_PATH}")

    # WandBã®çµ‚äº†
    if USE_WANDB:
        wandb.finish()


if __name__ == "__main__":
    main()
