{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-index --no-deps /kaggle/input/bitsandbytes-20250725/bitsandbytes/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:19:49.851636Z",
     "iopub.status.busy": "2025-09-17T12:19:49.851310Z",
     "iopub.status.idle": "2025-09-17T12:19:49.866376Z",
     "shell.execute_reply": "2025-09-17T12:19:49.865773Z",
     "shell.execute_reply.started": "2025-09-17T12:19:49.851613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deepseek_0_946.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deepseek_0_946.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2\"\n",
    "MODEL_TYPE = \"qwen2\"  # DeepSeek-R1 is based on Qwen2 architecture\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Increase for DeepSeek model's better long context handling\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/deepseek-r1-distill-qwen-14b-cv0.9455-fulltrain/transformers/default/1/ver_2\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 8  # Batch size 2 for RTX 5090 with 31GB VRAM\n",
    "EVAL_BATCH_SIZE = 8  # Eval can use larger batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Reduced to 32 for faster training\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.0000001\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'deepseek_r1_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"deepseek-r1-14b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"deepseek-r1-14b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 32  # LoRAのランク - reduced for memory efficiency\n",
    "LORA_ALPHA = 64  # LoRAのスケーリングパラメータ - reduced proportionally\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # DeepSeek-R1用のプロンプト - シンプルな形式\n",
    "    prompt = (\n",
    "        f\"User: [Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        \"Assistant: <think>\\n\\n</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（float16で読み込み）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # float16モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:19:49.867441Z",
     "iopub.status.busy": "2025-09-17T12:19:49.867199Z",
     "iopub.status.idle": "2025-09-17T12:19:49.889180Z",
     "shell.execute_reply": "2025-09-17T12:19:49.888474Z",
     "shell.execute_reply.started": "2025-09-17T12:19:49.867414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_14b_0_946.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_14b_0_946.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/qwen-3/transformers/14b/1\"\n",
    "MODEL_TYPE = \"qwen2\"  # Add model type for proper handling\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Increase max length for better context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/qwen3-14b-lb0.945-fulltrain/transformers/default/1/ver_2\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 4  # Batch size 2 for RTX 5090 with 31GB VRAM\n",
    "EVAL_BATCH_SIZE = 4  # Eval can use larger batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Reduced to 32 for faster training\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.00000001\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'qwen3_14b_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"qwen3-14b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"qwen3-14b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 128  # LoRAのランク - reduced for memory efficiency\n",
    "LORA_ALPHA = 256  # LoRAのスケーリングパラメータ - reduced proportionally\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Qwen2.5-Math用の数学タスクに特化したプロンプト\n",
    "    prompt = (\n",
    "        \"<|im_start|>user\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（float16で読み込み）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # float16モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:19:49.890369Z",
     "iopub.status.busy": "2025-09-17T12:19:49.890093Z",
     "iopub.status.idle": "2025-09-17T12:19:49.911852Z",
     "shell.execute_reply": "2025-09-17T12:19:49.911199Z",
     "shell.execute_reply.started": "2025-09-17T12:19:49.890339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_32b_0_947.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_32b_0_947.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/qwen-3/transformers/32b/1\"\n",
    "MODEL_TYPE = \"qwen2\"  # Add model type for proper handling\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 300  # Increase max length for better context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/qwen3-32b-9468/transformers/default/1/ver_2\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 16  # Batch size 2 for RTX 5090 with 31GB VRAM\n",
    "EVAL_BATCH_SIZE = 16  # Eval can use larger batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Reduced to 32 for faster training\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/best\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'qwen3_32b_0_947_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"qwen3-32b-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"qwen3-32b-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_RANK = 16  # LoRAのランク - reduced for memory efficiency\n",
    "LORA_ALPHA = 32  # LoRAのスケーリングパラメータ - reduced proportionally\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = True  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Qwen2.5-Math用の数学タスクに特化したプロンプト\n",
    "    prompt = (\n",
    "        \"<|im_start|>user\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n",
    "        \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（4bit量子化で読み込み）\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # 4bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:19:49.913672Z",
     "iopub.status.busy": "2025-09-17T12:19:49.912985Z",
     "iopub.status.idle": "2025-09-17T12:19:49.932603Z",
     "shell.execute_reply": "2025-09-17T12:19:49.932038Z",
     "shell.execute_reply.started": "2025-09-17T12:19:49.913653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing phi_4_0_948_fulltrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phi_4_0_948_fulltrain.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/ms-phi4/transformers/default/1/phi-4\"\n",
    "MODEL_TYPE = \"phi\"  # Phi-4 model type\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Phi-4 supports longer context\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/phi-4-cv0965-fulltrain/transformers/default/1/ver_2_0965ft\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 4  # Smaller batch size for Phi-4\n",
    "EVAL_BATCH_SIZE = 4  # Eval batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Increased for effective batch size\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'phi_4_0_948_fulltrain_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"phi-4-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"phi-4-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration for Phi-4\n",
    "LORA_RANK = 64  # LoRAのランク - optimized for Phi-4\n",
    "LORA_ALPHA = 128  # LoRAのスケーリングパラメータ - 1:1 ratio with rank\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Phi-4 target modules\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率 - reduced for Phi-4\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n",
    "    prompt = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        \"<think>\\n\"\n",
    "        \"Let me analyze this mathematical misconception...\\n\"\n",
    "        \"</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（量子化なしでフルプレシジョン）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            torch_dtype=torch.float16,  # float16を使用（メモリ効率とパフォーマンスのバランス）\n",
    "            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # モデルは既にdevice_mapでGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "        tokenizer.pad_token_id = 100257\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:19:49.935434Z",
     "iopub.status.busy": "2025-09-17T12:19:49.935206Z",
     "iopub.status.idle": "2025-09-17T12:19:49.950620Z",
     "shell.execute_reply": "2025-09-17T12:19:49.949750Z",
     "shell.execute_reply.started": "2025-09-17T12:19:49.935411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing phi_4_reasoning_0_948.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phi_4_reasoning_0_948.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available, will use base model only\")\n",
    "\n",
    "# Model configuration\n",
    "VER = 2\n",
    "MODEL_NAME = \"/kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus\"\n",
    "MODEL_TYPE = \"phi\"  # Phi-4 model type\n",
    "EPOCHS = 3  # Reduce epochs for initial testing\n",
    "MAX_LEN = 250  # Phi-4-reasoning-plus supports 32k context, but we use 1024 for efficiency\n",
    "\n",
    "# Directory settings\n",
    "OUTPUT_DIR = f\"/kaggle/input/phi-4-reasoning-plus09476-ft/transformers/default/1/ver_2_9476ft\"\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_BATCH_SIZE = 4  # Smaller batch size for Phi-4\n",
    "EVAL_BATCH_SIZE = 4  # Eval batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Increased for effective batch size\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n",
    "TEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\n",
    "LABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n",
    "\n",
    "# Other settings\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SPLIT = 0.0000001\n",
    "\n",
    "# GPU settings\n",
    "CUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n",
    "\n",
    "# Submission settings\n",
    "SUBMISSION_OUTPUT_PATH = 'phi_4_reasoning_0_948_submission.csv'\n",
    "\n",
    "# WandB settings\n",
    "USE_WANDB = True  # Set to False to disable WandB\n",
    "WANDB_PROJECT = \"phi-4-reasoning-math-misconceptions\"\n",
    "WANDB_RUN_NAME = f\"phi-4-reasoning-ver{VER}\"\n",
    "WANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n",
    "\n",
    "# Early stopping settings\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\n",
    "EARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n",
    "\n",
    "# LoRA configuration for Phi-4\n",
    "LORA_RANK = 64  # LoRAのランク - optimized for Phi-4\n",
    "LORA_ALPHA = 128  # LoRAのスケーリングパラメータ - 1:1 ratio with rank\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Phi-4 target modules\n",
    "LORA_DROPOUT = 0.1  # LoRAのドロップアウト率 - reduced for Phi-4\n",
    "LORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\n",
    "USE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping value\n",
    "\n",
    "def prepare_correct_answers(train_data):\n",
    "    \"\"\"正解答案データを準備\"\"\"\n",
    "    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n",
    "    correct = train_data.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def format_input(row):\n",
    "    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n",
    "    if row[\"is_correct\"]:\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "\n",
    "    # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n",
    "    prompt = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct?: {status}\\n\"\n",
    "        f\"Explanation: {row['StudentExplanation']}\\n\"\n",
    "        \"<|end|>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        \"<think>\\n\"\n",
    "        \"Let me analyze this mathematical misconception...\\n\"\n",
    "        \"</think>\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "    \"\"\"データセットをトークナイズ\"\"\"\n",
    "    def tokenize(batch):\n",
    "        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            padding=False,  # パディングはDataCollatorに任せる\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=None  # map時は'None'を使用\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n",
    "    # columnsの設定時にlabelを保持\n",
    "    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "    score = 0.0\n",
    "    for i, label in enumerate(labels):\n",
    "        ranks = top3[i]\n",
    "        if ranks[0] == label:\n",
    "            score += 1.0\n",
    "        elif ranks[1] == label:\n",
    "            score += 1.0 / 2\n",
    "        elif ranks[2] == label:\n",
    "            score += 1.0 / 3\n",
    "    return {\"map@3\": score / len(labels)}\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n",
    "\n",
    "    question_label_choices = {\n",
    "        31772: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Incomplete',\n",
    "            'True_Misconception:WNB',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        31774: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:SwapDividend',\n",
    "            'False_Misconception:Mult',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:FlipChange',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:SwapDividend',\n",
    "            'True_Misconception:Mult',\n",
    "            'True_Misconception:FlipChange'\n",
    "        ],\n",
    "        31777: [\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Misconception:Wrong_Fraction',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ],\n",
    "        31778: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Additive',\n",
    "            'False_Misconception:Irrelevant',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:WNB',\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Irrelevant',\n",
    "            'True_Misconception:Additive'\n",
    "        ],\n",
    "        32829: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Not_variable',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Adding_terms',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Not_variable',\n",
    "            'False_Misconception:Inverse_operation'\n",
    "        ],\n",
    "        32833: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Inversion',\n",
    "            'True_Misconception:Duplication',\n",
    "            'False_Misconception:Duplication',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Inversion',\n",
    "            'False_Misconception:Wrong_Operation'\n",
    "        ],\n",
    "        32835: [\n",
    "            'False_Misconception:Whole_numbers_larger',\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Longer_is_bigger',\n",
    "            'False_Misconception:Ignores_zeroes',\n",
    "            'False_Misconception:Shorter_is_bigger',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Whole_numbers_larger',\n",
    "            'True_Misconception:Shorter_is_bigger',\n",
    "            'True_Misconception:Longer_is_bigger'\n",
    "        ],\n",
    "        33471: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_fraction',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Incomplete',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_fraction'\n",
    "        ],\n",
    "        33472: [\n",
    "            'True_Neither:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Adding_across',\n",
    "            'True_Misconception:Denominator-only_change',\n",
    "            'True_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Denominator-only_change',\n",
    "            'False_Misconception:Incorrect_equivalent_fraction_addition',\n",
    "            'False_Misconception:Adding_across'\n",
    "        ],\n",
    "        33474: [\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Division',\n",
    "            'True_Misconception:Subtraction',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Subtraction',\n",
    "            'False_Misconception:Division',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        76870: [\n",
    "            'False_Misconception:Unknowable',\n",
    "            'False_Correct:NA',\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Definition',\n",
    "            'False_Misconception:Interior',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Definition'\n",
    "        ],\n",
    "        89443: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Positive',\n",
    "            'False_Misconception:Tacking',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Tacking',\n",
    "            'True_Misconception:Positive',\n",
    "            'False_Correct:NA'\n",
    "        ],\n",
    "        91695: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Wrong_term',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Firstterm',\n",
    "            'True_Correct:NA',\n",
    "            'True_Misconception:Wrong_term',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Firstterm'\n",
    "        ],\n",
    "        104665: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Misconception:Base_rate',\n",
    "            'False_Correct:NA',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA',\n",
    "            'True_Misconception:Base_rate',\n",
    "            'True_Misconception:Multiplying_by_4',\n",
    "            'False_Misconception:Multiplying_by_4'\n",
    "        ],\n",
    "        109465: [\n",
    "            'False_Neither:NA',\n",
    "            'False_Correct:NA',\n",
    "            'False_Misconception:Certainty',\n",
    "            'False_Misconception:Scale',\n",
    "            'True_Correct:NA',\n",
    "            'True_Neither:NA'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify which are True/False classes\n",
    "    true_classes = {}\n",
    "    false_classes = {}\n",
    "    for idx, c in enumerate(label_encoder.classes_):\n",
    "\n",
    "        if 'True' in c:\n",
    "            true_classes[idx] = c\n",
    "        else:\n",
    "            false_classes[idx] = c\n",
    "\n",
    "\n",
    "    # Normalize for Label Encoder\n",
    "    question_label_choice_ids = {}\n",
    "    for qid, choices in question_label_choices.items():\n",
    "        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n",
    "\n",
    "        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n",
    "\n",
    "\n",
    "    test_probabilities = []\n",
    "    test_predictions = []\n",
    "    test_top3_predictions = []\n",
    "\n",
    "    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n",
    "\n",
    "        candidate_idx = question_label_choice_ids[qid]\n",
    "\n",
    "        # If filter candidates using True/False information\n",
    "        if filter_true_false:\n",
    "            if correct == 1:\n",
    "                # use true_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in true_classes]\n",
    "            if correct == 0:\n",
    "                # use false_classes to filter candidate_idx\n",
    "                candidate_idx = [c for c in candidate_idx if c in false_classes]\n",
    "\n",
    "        candidate_logits = row[candidate_idx]\n",
    "\n",
    "        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n",
    "\n",
    "        top_k = np.argsort(-candidate_probs)\n",
    "\n",
    "        # Have to convert back to the original label encoder space\n",
    "        topk_idx = np.array(candidate_idx)[top_k]\n",
    "\n",
    "        # Keep the probabilities\n",
    "        topk_probs = candidate_probs[top_k].tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n",
    "\n",
    "        test_probabilities.append(topk_probs)\n",
    "        test_predictions.append(topk_preds)\n",
    "        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n",
    "\n",
    "    test_submission_data = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.tolist(),\n",
    "        \"QuestionId\": test_data.QuestionId.tolist(),\n",
    "        \"is_correct\": test_data.is_correct.tolist(),\n",
    "        \"probs\": test_probabilities,\n",
    "        \"preds\": test_predictions,\n",
    "        'Category:Misconception': test_top3_predictions\n",
    "    })\n",
    "\n",
    "    return test_submission_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン推論関数\"\"\"\n",
    "\n",
    "    # メモリキャッシュをクリア\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDAメモリ管理の最適化\n",
    "    import os\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # 2つのGPUを使用可能にする\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "    print(\"Loading label encoder...\")\n",
    "    # ラベルエンコーダーの読み込み\n",
    "    le = joblib.load(LABEL_ENCODER_PATH)\n",
    "    n_classes = len(le.classes_)\n",
    "\n",
    "    print(\"Loading trained model and tokenizer...\")\n",
    "\n",
    "    if PEFT_AVAILABLE:\n",
    "        # LoRAアダプターを使用する場合\n",
    "        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n",
    "        print(f\"Loading base model from: {MODEL_NAME}\")\n",
    "\n",
    "        # ベースモデルを読み込む（量子化なし）\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=n_classes,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",  # 自動的に複数GPUに分散\n",
    "            low_cpu_mem_usage=True,  # CPUメモリ使用量を削減\n",
    "            torch_dtype=torch.float16  # FP16を使用してメモリ効率を改善\n",
    "        )\n",
    "\n",
    "        # LoRAアダプターを適用\n",
    "        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n",
    "\n",
    "        # 推論モードに設定（メモリ効率化）\n",
    "        model.eval()\n",
    "        # 8bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n",
    "\n",
    "        # トークナイザーはベースモデルから読み込む\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"Successfully loaded LoRA fine-tuned model\")\n",
    "    else:\n",
    "        # PEFTが利用できない場合はエラー\n",
    "        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n",
    "\n",
    "    # パディングトークンの設定\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "        tokenizer.pad_token_id = 100349  # Phi-4-reasoning-plusのPADトークンID\n",
    "\n",
    "    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n",
    "    if hasattr(model, 'base_model'):\n",
    "        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # 内部のモデルにも設定\n",
    "        if hasattr(model.base_model, 'model'):\n",
    "            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    # テストデータの読み込み\n",
    "    test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    print(\"Loading training data for correct answers...\")\n",
    "    # 正解答案データの準備（訓練データから取得）\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    train.Misconception = train.Misconception.fillna('NA')\n",
    "    correct = prepare_correct_answers(train)\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # テストデータの前処理\n",
    "    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "    test.is_correct = test.is_correct.fillna(0)\n",
    "    test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "    print(\"Tokenizing test data...\")\n",
    "    # テストデータのトークナイズ\n",
    "    ds_test = Dataset.from_pandas(test[['text']])\n",
    "    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    # パディングのためのデータコラレータの設定\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "\n",
    "    # TF32を有効化（推論速度向上）\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # 推論の実行\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,  # tokenizer の代替\n",
    "        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n",
    "            report_to=\"none\",    # wandbを無効化\n",
    "            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n",
    "            fp16=True,  # float16を使用\n",
    "            dataloader_pin_memory=True,  # データローダーの高速化\n",
    "            dataloader_num_workers=2,  # データ読み込みの並列化\n",
    "        )\n",
    "    )\n",
    "    # no_gradコンテキストで推論を実行（メモリ効率化）\n",
    "    with torch.no_grad():\n",
    "        predictions = trainer.predict(ds_test)\n",
    "\n",
    "    print(\"Creating submission file...\")\n",
    "    # 提出用ファイルの作成\n",
    "    submission = create_submission(predictions, test, le)\n",
    "\n",
    "    # ファイルの保存\n",
    "    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:19:49.951488Z",
     "iopub.status.busy": "2025-09-17T12:19:49.951267Z",
     "iopub.status.idle": "2025-09-17T12:29:13.070868Z",
     "shell.execute_reply": "2025-09-17T12:29:13.070114Z",
     "shell.execute_reply.started": "2025-09-17T12:19:49.951473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 12:20:02.632008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758111602.776594      76 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758111602.817692      76 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Found 2 GPUs\n",
      "Loading label encoder...\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Loading trained model and tokenizer...\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/qwen3-32b-9468/transformers/default/1/ver_2/best\n",
      "Loading base model from: /kaggle/input/qwen-3/transformers/32b/1\n",
      "Loading checkpoint shards: 100%|████████████████| 17/17 [08:36<00:00, 30.40s/it]\n",
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/qwen-3/transformers/32b/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "Successfully loaded LoRA fine-tuned model\n",
      "Loading test data...\n",
      "Loading training data for correct answers...\n",
      "Preprocessing test data...\n",
      "Tokenizing test data...\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 105.47 examples/s]\n",
      "Running inference...\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1253.53it/s]\n",
      "Creating submission file...\n",
      "Submission file saved to: qwen3_32b_0_947_submission.csv\n",
      "\n",
      "Submission preview:\n",
      "   row_id  ...                             Category:Misconception\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  ...  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "\n",
      "[3 rows x 6 columns]\n",
      "\n",
      "Submission shape: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "!python qwen3_32b_0_947.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:29:13.072098Z",
     "iopub.status.busy": "2025-09-17T12:29:13.071859Z",
     "iopub.status.idle": "2025-09-17T12:32:54.068248Z",
     "shell.execute_reply": "2025-09-17T12:32:54.067356Z",
     "shell.execute_reply.started": "2025-09-17T12:29:13.072066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 12:29:20.471363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758112160.498803     123 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758112160.507581     123 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Found 2 GPUs\n",
      "Loading label encoder...\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Loading trained model and tokenizer...\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/phi-4-reasoning-plus09476-ft/transformers/default/1/ver_2_9476ft/checkpoint-1722\n",
      "Loading base model from: /kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [03:11<00:00, 31.95s/it]\n",
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "Successfully loaded LoRA fine-tuned model\n",
      "Loading test data...\n",
      "Loading training data for correct answers...\n",
      "Preprocessing test data...\n",
      "Tokenizing test data...\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 102.58 examples/s]\n",
      "Running inference...\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1243.86it/s]\n",
      "Creating submission file...\n",
      "Submission file saved to: phi_4_reasoning_0_948_submission.csv\n",
      "\n",
      "Submission preview:\n",
      "   row_id  ...                             Category:Misconception\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\n",
      "\n",
      "[3 rows x 6 columns]\n",
      "\n",
      "Submission shape: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "!python phi_4_reasoning_0_948.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:32:54.069677Z",
     "iopub.status.busy": "2025-09-17T12:32:54.069421Z",
     "iopub.status.idle": "2025-09-17T12:36:47.047122Z",
     "shell.execute_reply": "2025-09-17T12:36:47.046422Z",
     "shell.execute_reply.started": "2025-09-17T12:32:54.069641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 12:33:00.925993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758112380.949742     154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758112380.956819     154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Found 2 GPUs\n",
      "Loading label encoder...\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Loading trained model and tokenizer...\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/phi-4-cv0965-fulltrain/transformers/default/1/ver_2_0965ft/checkpoint-1722\n",
      "Loading base model from: /kaggle/input/ms-phi4/transformers/default/1/phi-4\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [03:29<00:00, 34.91s/it]\n",
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/ms-phi4/transformers/default/1/phi-4 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "Successfully loaded LoRA fine-tuned model\n",
      "Loading test data...\n",
      "Loading training data for correct answers...\n",
      "Preprocessing test data...\n",
      "Tokenizing test data...\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 142.54 examples/s]\n",
      "Running inference...\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1272.16it/s]\n",
      "Creating submission file...\n",
      "Submission file saved to: phi_4_0_948_fulltrain_submission.csv\n",
      "\n",
      "Submission preview:\n",
      "   row_id  ...                             Category:Misconception\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\n",
      "\n",
      "[3 rows x 6 columns]\n",
      "\n",
      "Submission shape: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "!python phi_4_0_948_fulltrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:36:47.048335Z",
     "iopub.status.busy": "2025-09-17T12:36:47.048100Z",
     "iopub.status.idle": "2025-09-17T12:39:56.718815Z",
     "shell.execute_reply": "2025-09-17T12:39:56.717802Z",
     "shell.execute_reply.started": "2025-09-17T12:36:47.048312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 12:36:53.585857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758112613.608315     185 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758112613.615213     185 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Found 2 GPUs\n",
      "Loading label encoder...\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Loading trained model and tokenizer...\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/deepseek-r1-distill-qwen-14b-cv0.9455-fulltrain/transformers/default/1/ver_2/checkpoint-1722\n",
      "Loading base model from: /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:44<00:00, 41.21s/it]\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "Successfully loaded LoRA fine-tuned model\n",
      "Loading test data...\n",
      "Loading training data for correct answers...\n",
      "Preprocessing test data...\n",
      "Tokenizing test data...\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 394.88 examples/s]\n",
      "Running inference...\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1381.07it/s]\n",
      "Creating submission file...\n",
      "Submission file saved to: deepseek_r1_submission.csv\n",
      "\n",
      "Submission preview:\n",
      "   row_id  ...                             Category:Misconception\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\n",
      "\n",
      "[3 rows x 6 columns]\n",
      "\n",
      "Submission shape: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "!python deepseek_0_946.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:39:56.720862Z",
     "iopub.status.busy": "2025-09-17T12:39:56.720188Z",
     "iopub.status.idle": "2025-09-17T12:42:37.943258Z",
     "shell.execute_reply": "2025-09-17T12:42:37.942545Z",
     "shell.execute_reply.started": "2025-09-17T12:39:56.720817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-17 12:40:02.414435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758112802.436444     216 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758112802.443282     216 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Found 2 GPUs\n",
      "Loading label encoder...\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Loading trained model and tokenizer...\n",
      "Loading fine-tuned LoRA model from: /kaggle/input/qwen3-14b-lb0.945-fulltrain/transformers/default/1/ver_2/checkpoint-1722\n",
      "Loading base model from: /kaggle/input/qwen-3/transformers/14b/1\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:20<00:00, 17.55s/it]\n",
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/qwen-3/transformers/14b/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "Successfully loaded LoRA fine-tuned model\n",
      "Loading test data...\n",
      "Loading training data for correct answers...\n",
      "Preprocessing test data...\n",
      "Tokenizing test data...\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 403.97 examples/s]\n",
      "Running inference...\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1422.76it/s]\n",
      "Creating submission file...\n",
      "Submission file saved to: qwen3_14b_submission.csv\n",
      "\n",
      "Submission preview:\n",
      "   row_id  ...                             Category:Misconception\n",
      "0   36696  ...  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  ...  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  ...  True_Neither:NA True_Correct:NA True_Misconcep...\n",
      "\n",
      "[3 rows x 6 columns]\n",
      "\n",
      "Submission shape: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "!python qwen3_14b_0_946.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:42:37.944387Z",
     "iopub.status.busy": "2025-09-17T12:42:37.944170Z",
     "iopub.status.idle": "2025-09-17T12:42:37.951954Z",
     "shell.execute_reply": "2025-09-17T12:42:37.951150Z",
     "shell.execute_reply.started": "2025-09-17T12:42:37.944364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c b a\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_k_ensemble(ll, k=3):\n",
    "\n",
    "    lists = [l.split(' ') for l in ll]\n",
    "    weights = [4 for l in lists]\n",
    "    score = defaultdict(int)\n",
    "\n",
    "    for i, lst in enumerate(lists):\n",
    "        weight = weights[i]\n",
    "        for rank, item in enumerate(lst):\n",
    "            score[item] += (len(lst) - rank) * weight\n",
    "\n",
    "    sorted_items = sorted(score.items(), key=lambda x: -x[1])\n",
    "    return ' '.join([item for item, _ in sorted_items[:k]])\n",
    "\n",
    "list1 = 'a b d f'\n",
    "list2 = 'b c a e'\n",
    "list3 = 'c e b'\n",
    "list4 = 'c e d'\n",
    "\n",
    "print(get_top_k_ensemble([list1, list2, list3, list4], k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:42:37.952916Z",
     "iopub.status.busy": "2025-09-17T12:42:37.952560Z",
     "iopub.status.idle": "2025-09-17T12:42:38.290300Z",
     "shell.execute_reply": "2025-09-17T12:42:38.289499Z",
     "shell.execute_reply.started": "2025-09-17T12:42:37.952899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv('/kaggle/working/deepseek_r1_submission.csv')\n",
    "df2 = pd.read_csv('/kaggle/working/qwen3_14b_submission.csv')\n",
    "df3 = pd.read_csv('/kaggle/working/phi_4_reasoning_0_948_submission.csv')\n",
    "df4 = pd.read_csv('/kaggle/working/phi_4_0_948_fulltrain_submission.csv')\n",
    "df5 = pd.read_csv('/kaggle/working/qwen3_32b_0_947_submission.csv')\n",
    "# Model order mapping (used by ensemble weights below):\n",
    "#   df1: deepseek_r1_submission.csv        -> 'deepseek_r1'\n",
    "#   df2: qwen3_14b_submission.csv          -> 'qwen3_14b'\n",
    "#   df3: phi_4_reasoning_0_948_submission  -> 'phi4_reasoning_plus'\n",
    "#   df4: phi_4_0_948_fulltrain_submission  -> 'phi4_base_fulltrain'\n",
    "#   df5: qwen3_32b_0_947_submission        -> 'qwen3_32b'\n",
    "\n",
    "df1 = df1.sort_values('row_id').reset_index(drop=True)\n",
    "df2 = df2.sort_values('row_id').reset_index(drop=True)\n",
    "df3 = df3.sort_values('row_id').reset_index(drop=True)\n",
    "df4 = df4.sort_values('row_id').reset_index(drop=True)\n",
    "df5 = df5.sort_values('row_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:42:38.291467Z",
     "iopub.status.busy": "2025-09-17T12:42:38.291219Z",
     "iopub.status.idle": "2025-09-17T12:42:38.303583Z",
     "shell.execute_reply": "2025-09-17T12:42:38.302956Z",
     "shell.execute_reply.started": "2025-09-17T12:42:38.291445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# Ensemble weights for each model in the order of df1..df5 above.\n",
    "# Increase a specific position to give that model more influence:\n",
    "#   0: deepseek_r1, 1: qwen3_14b, 2: phi4_reasoning_plus, 3: phi4_base_fulltrain, 4: qwen3_32b\n",
    "weights = np.array([0.1, 0.1, 0.3, 0.3, 0.2], dtype=float)\n",
    "# Note: np.average normalizes by sum(weights), so they need not sum to 1.\n",
    "# The normalization below is optional (scaling all weights equally has no effect).\n",
    "weights = weights / weights.sum()\n",
    "for r1, r2, r3, r4, r5 in zip(df1.itertuples(), df2.itertuples(), df3.itertuples(), df4.itertuples(), df5.itertuples()):\n",
    "\n",
    "    prob_preds_1 = sorted([(pb, pr) for pb, pr in zip(eval(r1.probs), eval(r1.preds))], key=lambda x: x[1])\n",
    "    prob_preds_2 = sorted([(pb, pr) for pb, pr in zip(eval(r2.probs), eval(r2.preds))], key=lambda x: x[1])\n",
    "    prob_preds_3 = sorted([(pb, pr) for pb, pr in zip(eval(r3.probs), eval(r3.preds))], key=lambda x: x[1])\n",
    "    prob_preds_4 = sorted([(pb, pr) for pb, pr in zip(eval(r4.probs), eval(r4.preds))], key=lambda x: x[1])\n",
    "    prob_preds_5 = sorted([(pb, pr) for pb, pr in zip(eval(r5.probs), eval(r5.preds))], key=lambda x: x[1])\n",
    "\n",
    "    # Should be same for all row_ids\n",
    "    choices = [x[1] for x in prob_preds_1]\n",
    "\n",
    "    mean_probs = np.average([\n",
    "        [x[0] for x in prob_preds_1],\n",
    "        [x[0] for x in prob_preds_2],\n",
    "        [x[0] for x in prob_preds_3],\n",
    "        [x[0] for x in prob_preds_4],\n",
    "        [x[0] for x in prob_preds_5],\n",
    "    ],\n",
    "    axis=0,\n",
    "    weights=weights)\n",
    "\n",
    "    final_prob_preds = sorted([(l, p) for l, p in zip(choices, mean_probs)], key=lambda x: -x[1])\n",
    "\n",
    "    row = {\n",
    "        \"row_id\": r1.row_id,\n",
    "        \"Category:Misconception\": \" \".join([x[0] for x in final_prob_preds[:3]])\n",
    "    }\n",
    "\n",
    "    ensemble_predictions.append(row)\n",
    "\n",
    "ensemble_predictions = pd.DataFrame(ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:42:38.304496Z",
     "iopub.status.busy": "2025-09-17T12:42:38.304296Z",
     "iopub.status.idle": "2025-09-17T12:42:38.317964Z",
     "shell.execute_reply": "2025-09-17T12:42:38.317324Z",
     "shell.execute_reply.started": "2025-09-17T12:42:38.304473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ensemble_predictions.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:42:38.319069Z",
     "iopub.status.busy": "2025-09-17T12:42:38.318734Z",
     "iopub.status.idle": "2025-09-17T12:42:38.337829Z",
     "shell.execute_reply": "2025-09-17T12:42:38.337213Z",
     "shell.execute_reply.started": "2025-09-17T12:42:38.319027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Category:Misconception</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36696</td>\n",
       "      <td>True_Correct:NA True_Neither:NA True_Misconcep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36697</td>\n",
       "      <td>False_Misconception:WNB False_Neither:NA False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36698</td>\n",
       "      <td>True_Neither:NA True_Correct:NA True_Misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                             Category:Misconception\n",
       "0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n",
       "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
       "2   36698  True_Neither:NA True_Correct:NA True_Misconcep..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_predictions"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "sourceId": 252296453,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 225262,
     "modelInstanceId": 204048,
     "sourceId": 256580,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301527,
     "sourceId": 363149,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301540,
     "sourceId": 363168,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 410342,
     "modelInstanceId": 391645,
     "sourceId": 492735,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411100,
     "modelInstanceId": 392435,
     "sourceId": 493918,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411495,
     "modelInstanceId": 392864,
     "sourceId": 494692,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411517,
     "modelInstanceId": 392888,
     "sourceId": 494731,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 411554,
     "modelInstanceId": 392927,
     "sourceId": 494800,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 414003,
     "modelInstanceId": 395433,
     "sourceId": 497710,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 414910,
     "modelInstanceId": 396413,
     "sourceId": 498993,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
