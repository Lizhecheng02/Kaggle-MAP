{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":252296453,"sourceType":"kernelVersion"},{"sourceId":256580,"sourceType":"modelInstanceVersion","modelInstanceId":204048,"modelId":225262},{"sourceId":363149,"sourceType":"modelInstanceVersion","modelInstanceId":301527,"modelId":322000},{"sourceId":363168,"sourceType":"modelInstanceVersion","modelInstanceId":301540,"modelId":322000},{"sourceId":492735,"sourceType":"modelInstanceVersion","modelInstanceId":391645,"modelId":410342},{"sourceId":493918,"sourceType":"modelInstanceVersion","modelInstanceId":392435,"modelId":411100},{"sourceId":494692,"sourceType":"modelInstanceVersion","modelInstanceId":392864,"modelId":411495},{"sourceId":494731,"sourceType":"modelInstanceVersion","modelInstanceId":392888,"modelId":411517},{"sourceId":494800,"sourceType":"modelInstanceVersion","modelInstanceId":392927,"modelId":411554},{"sourceId":497710,"sourceType":"modelInstanceVersion","modelInstanceId":395433,"modelId":414003},{"sourceId":498993,"sourceType":"modelInstanceVersion","modelInstanceId":396413,"modelId":414910}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index --no-deps /kaggle/input/bitsandbytes-20250725/bitsandbytes/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:31:21.58499Z","iopub.execute_input":"2025-09-04T03:31:21.585196Z","iopub.status.idle":"2025-09-04T03:31:26.439428Z","shell.execute_reply.started":"2025-09-04T03:31:21.585177Z","shell.execute_reply":"2025-09-04T03:31:26.43837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile deepseek_0_946.py\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport joblib\nimport torch\n\ntry:\n    from peft import PeftModel, PeftConfig\n    PEFT_AVAILABLE = True\nexcept ImportError:\n    PEFT_AVAILABLE = False\n    print(\"Warning: PEFT not available, will use base model only\")\n\n\n# Model configuration\nVER = 2\nMODEL_NAME = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2\"\nMODEL_TYPE = \"qwen2\"  # DeepSeek-R1 is based on Qwen2 architecture\nEPOCHS = 3  # Reduce epochs for initial testing\nMAX_LEN = 250  # Increase for DeepSeek model's better long context handling\n\n# Directory settings\nOUTPUT_DIR = f\"/kaggle/input/deepseek-r1-distill-qwen-14b-cv0.9455-fulltrain/transformers/default/1/ver_2\"\n\n# Training parameters\nTRAIN_BATCH_SIZE = 8  # Batch size 2 for RTX 5090 with 31GB VRAM\nEVAL_BATCH_SIZE = 8  # Eval can use larger batch size\nGRADIENT_ACCUMULATION_STEPS = 8  # Reduced to 32 for faster training\nLEARNING_RATE = 2e-4\nLOGGING_STEPS = 50\nSAVE_STEPS = 200\nEVAL_STEPS = 200\n\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\nTEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n\n# Model save paths\nBEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\nLABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n\n# Other settings\nRANDOM_SEED = 42\nVALIDATION_SPLIT = 0.0000001\n\n# GPU settings\nCUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n\n# Submission settings\nSUBMISSION_OUTPUT_PATH = 'deepseek_r1_submission.csv'\n\n# WandB settings\nUSE_WANDB = True  # Set to False to disable WandB\nWANDB_PROJECT = \"deepseek-r1-14b-math-misconceptions\"\nWANDB_RUN_NAME = f\"deepseek-r1-14b-ver{VER}\"\nWANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n\n# Early stopping settings\nUSE_EARLY_STOPPING = True\nEARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\nEARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n\n# LoRA configuration\nLORA_RANK = 32  # LoRAのランク - reduced for memory efficiency\nLORA_ALPHA = 64  # LoRAのスケーリングパラメータ - reduced proportionally\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\nLORA_DROPOUT = 0.1  # LoRAのドロップアウト率\nLORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n\n# Memory optimization settings\nUSE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\nUSE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\nMAX_GRAD_NORM = 1.0  # Gradient clipping value\n\ndef prepare_correct_answers(train_data):\n    \"\"\"正解答案データを準備\"\"\"\n    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n    correct = train_data.loc[idx].copy()\n    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n    correct = correct.sort_values('c', ascending=False)\n    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n    correct['is_correct'] = 1\n    return correct\n\n\ndef format_input(row):\n    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n    if row[\"is_correct\"]:\n        status = \"Yes\"\n    else:\n        status = \"No\"\n\n    # DeepSeek-R1用のプロンプト - シンプルな形式\n    prompt = (\n        f\"User: [Mathematical Misconception Analysis Task]\\n\\n\"\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct?: {status}\\n\"\n        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n        \"Assistant: <think>\\n\\n</think>\\n\\n\"\n    )\n    return prompt\n\n\ndef tokenize_dataset(dataset, tokenizer, max_len):\n    \"\"\"データセットをトークナイズ\"\"\"\n    def tokenize(batch):\n        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n        return tokenizer(\n            batch['text'],\n            padding=False,  # パディングはDataCollatorに任せる\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None  # map時は'None'を使用\n        )\n\n    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n    # columnsの設定時にlabelを保持\n    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n    dataset.set_format(type='torch', columns=columns)\n    return dataset\n\n\ndef compute_map3(eval_pred):\n    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n    logits, labels = eval_pred\n    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n    top3 = np.argsort(-probs, axis=1)[:, :3]\n    score = 0.0\n    for i, label in enumerate(labels):\n        ranks = top3[i]\n        if ranks[0] == label:\n            score += 1.0\n        elif ranks[1] == label:\n            score += 1.0 / 2\n        elif ranks[2] == label:\n            score += 1.0 / 3\n    return {\"map@3\": score / len(labels)}\n\n\ndef create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n\n    question_label_choices = {\n        31772: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Incomplete',\n            'True_Misconception:WNB',\n            'False_Neither:NA',\n            'False_Misconception:WNB',\n            'False_Misconception:Incomplete',\n            'False_Correct:NA'\n        ],\n        31774: [\n            'False_Neither:NA',\n            'False_Misconception:SwapDividend',\n            'False_Misconception:Mult',\n            'False_Correct:NA',\n            'False_Misconception:FlipChange',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:SwapDividend',\n            'True_Misconception:Mult',\n            'True_Misconception:FlipChange'\n        ],\n        31777: [\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Irrelevant',\n            'False_Misconception:Wrong_Fraction',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ],\n        31778: [\n            'False_Neither:NA',\n            'False_Misconception:Additive',\n            'False_Misconception:Irrelevant',\n            'False_Correct:NA',\n            'False_Misconception:WNB',\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Irrelevant',\n            'True_Misconception:Additive'\n        ],\n        32829: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Not_variable',\n            'False_Neither:NA',\n            'False_Misconception:Adding_terms',\n            'False_Correct:NA',\n            'False_Misconception:Not_variable',\n            'False_Misconception:Inverse_operation'\n        ],\n        32833: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Inversion',\n            'True_Misconception:Duplication',\n            'False_Misconception:Duplication',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Inversion',\n            'False_Misconception:Wrong_Operation'\n        ],\n        32835: [\n            'False_Misconception:Whole_numbers_larger',\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Longer_is_bigger',\n            'False_Misconception:Ignores_zeroes',\n            'False_Misconception:Shorter_is_bigger',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Whole_numbers_larger',\n            'True_Misconception:Shorter_is_bigger',\n            'True_Misconception:Longer_is_bigger'\n        ],\n        33471: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_fraction',\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Wrong_fraction'\n        ],\n        33472: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Adding_across',\n            'True_Misconception:Denominator-only_change',\n            'True_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Denominator-only_change',\n            'False_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Misconception:Adding_across'\n        ],\n        33474: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Division',\n            'True_Misconception:Subtraction',\n            'False_Neither:NA',\n            'False_Misconception:Subtraction',\n            'False_Misconception:Division',\n            'False_Correct:NA'\n        ],\n        76870: [\n            'False_Misconception:Unknowable',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Definition',\n            'False_Misconception:Interior',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Definition'\n        ],\n        89443: [\n            'False_Neither:NA',\n            'False_Misconception:Positive',\n            'False_Misconception:Tacking',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Tacking',\n            'True_Misconception:Positive',\n            'False_Correct:NA'\n        ],\n        91695: [\n            'False_Neither:NA',\n            'False_Misconception:Wrong_term',\n            'False_Correct:NA',\n            'False_Misconception:Firstterm',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_term',\n            'True_Neither:NA',\n            'True_Misconception:Firstterm'\n        ],\n        104665: [\n            'False_Neither:NA',\n            'False_Misconception:Base_rate',\n            'False_Correct:NA',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Base_rate',\n            'True_Misconception:Multiplying_by_4',\n            'False_Misconception:Multiplying_by_4'\n        ],\n        109465: [\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Certainty',\n            'False_Misconception:Scale',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ]\n    }\n\n    # Identify which are True/False classes\n    true_classes = {}\n    false_classes = {}\n    for idx, c in enumerate(label_encoder.classes_):\n    \n        if 'True' in c:\n            true_classes[idx] = c\n        else:\n            false_classes[idx] = c\n\n    \n    # Normalize for Label Encoder\n    question_label_choice_ids = {}\n    for qid, choices in question_label_choices.items():\n        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n        \n        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n        \n    \n    test_probabilities = []\n    test_predictions = []\n    test_top3_predictions = []\n\n    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n    \n        candidate_idx = question_label_choice_ids[qid]\n\n        # If filter candidates using True/False information\n        if filter_true_false:\n            if correct == 1:\n                # use true_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in true_classes]\n            if correct == 0:\n                # use false_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in false_classes]\n    \n        candidate_logits = row[candidate_idx]\n    \n        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n    \n        top_k = np.argsort(-candidate_probs)\n    \n        # Have to convert back to the original label encoder space\n        topk_idx = np.array(candidate_idx)[top_k]\n    \n        # Keep the probabilities\n        topk_probs = candidate_probs[top_k].tolist()\n    \n        # Get the predicted labels\n        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n    \n        test_probabilities.append(topk_probs)\n        test_predictions.append(topk_preds)\n        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n    \n    test_submission_data = pd.DataFrame({\n        \"row_id\": test_data.row_id.tolist(),\n        \"QuestionId\": test_data.QuestionId.tolist(),\n        \"is_correct\": test_data.is_correct.tolist(),\n        \"probs\": test_probabilities,\n        \"preds\": test_predictions,\n        'Category:Misconception': test_top3_predictions\n    })\n\n    return test_submission_data\n\n\ndef main():\n    \"\"\"メイン推論関数\"\"\"\n    \n    # メモリキャッシュをクリア\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # CUDAメモリ管理の最適化\n    import os\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n    \n    # 2つのGPUを使用可能にする\n    if torch.cuda.device_count() > 1:\n        print(f\"Found {torch.cuda.device_count()} GPUs\")\n\n    print(\"Loading label encoder...\")\n    # ラベルエンコーダーの読み込み\n    le = joblib.load(LABEL_ENCODER_PATH)\n    n_classes = len(le.classes_)\n\n    print(\"Loading trained model and tokenizer...\")\n\n    if PEFT_AVAILABLE:\n        # LoRAアダプターを使用する場合\n        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n        print(f\"Loading base model from: {MODEL_NAME}\")\n\n        # ベースモデルを読み込む（float16で読み込み）\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME,\n            num_labels=n_classes,\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",  # 自動的に複数GPUに分散\n            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n        )\n\n        # LoRAアダプターを適用\n        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n        \n        # 推論モードに設定（メモリ効率化）\n        model.eval()\n        # float16モデルは既にGPUに配置されているのでto('cuda')は不要\n\n        # トークナイザーはベースモデルから読み込む\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n        print(\"Successfully loaded LoRA fine-tuned model\")\n    else:\n        # PEFTが利用できない場合はエラー\n        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n\n    # パディングトークンの設定\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    \n    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n    if hasattr(model, 'base_model'):\n        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n        # 内部のモデルにも設定\n        if hasattr(model.base_model, 'model'):\n            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n    else:\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n    print(\"Loading test data...\")\n    # テストデータの読み込み\n    test = pd.read_csv(TEST_DATA_PATH)\n\n    print(\"Loading training data for correct answers...\")\n    # 正解答案データの準備（訓練データから取得）\n    train = pd.read_csv(TRAIN_DATA_PATH)\n    train.Misconception = train.Misconception.fillna('NA')\n    correct = prepare_correct_answers(train)\n\n    print(\"Preprocessing test data...\")\n    # テストデータの前処理\n    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n    test.is_correct = test.is_correct.fillna(0)\n    test['text'] = test.apply(format_input, axis=1)\n\n    print(\"Tokenizing test data...\")\n    # テストデータのトークナイズ\n    ds_test = Dataset.from_pandas(test[['text']])\n    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n\n    # パディングのためのデータコラレータの設定\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    print(\"Running inference...\")\n    \n    # TF32を有効化（推論速度向上）\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # 推論の実行\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,  # tokenizer の代替\n        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n        args=TrainingArguments(\n            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n            report_to=\"none\",    # wandbを無効化\n            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n            fp16=True,  # float16を使用\n            dataloader_pin_memory=True,  # データローダーの高速化\n            dataloader_num_workers=2,  # データ読み込みの並列化\n        )\n    )\n    # no_gradコンテキストで推論を実行（メモリ効率化）\n    with torch.no_grad():\n        predictions = trainer.predict(ds_test)\n\n    print(\"Creating submission file...\")\n    # 提出用ファイルの作成\n    submission = create_submission(predictions, test, le)\n\n    # ファイルの保存\n    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n    print(\"\\nSubmission preview:\")\n    print(submission.head())\n    print(f\"\\nSubmission shape: {submission.shape}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:31:26.441561Z","iopub.execute_input":"2025-09-04T03:31:26.441849Z","iopub.status.idle":"2025-09-04T03:31:26.461913Z","shell.execute_reply.started":"2025-09-04T03:31:26.441817Z","shell.execute_reply":"2025-09-04T03:31:26.461124Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile qwen3_14b_0_946.py\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport joblib\nimport torch\n\ntry:\n    from peft import PeftModel, PeftConfig\n    PEFT_AVAILABLE = True\nexcept ImportError:\n    PEFT_AVAILABLE = False\n    print(\"Warning: PEFT not available, will use base model only\")\n\n# Model configuration\nVER = 2\nMODEL_NAME = \"/kaggle/input/qwen-3/transformers/14b/1\"\nMODEL_TYPE = \"qwen2\"  # Add model type for proper handling\nEPOCHS = 3  # Reduce epochs for initial testing\nMAX_LEN = 250  # Increase max length for better context\n\n# Directory settings\nOUTPUT_DIR = f\"/kaggle/input/qwen3-14b-lb0.945-fulltrain/transformers/default/1/ver_2\"\n\n# Training parameters\nTRAIN_BATCH_SIZE = 4  # Batch size 2 for RTX 5090 with 31GB VRAM\nEVAL_BATCH_SIZE = 4  # Eval can use larger batch size\nGRADIENT_ACCUMULATION_STEPS = 16  # Reduced to 32 for faster training\nLEARNING_RATE = 2e-4\nLOGGING_STEPS = 50\nSAVE_STEPS = 200\nEVAL_STEPS = 200\n\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\nTEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n\n# Model save paths\nBEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\nLABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n\n# Other settings\nRANDOM_SEED = 42\nVALIDATION_SPLIT = 0.00000001\n\n# GPU settings\nCUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n\n# Submission settings\nSUBMISSION_OUTPUT_PATH = 'qwen3_14b_submission.csv'\n\n# WandB settings\nUSE_WANDB = True  # Set to False to disable WandB\nWANDB_PROJECT = \"qwen3-14b-math-misconceptions\"\nWANDB_RUN_NAME = f\"qwen3-14b-ver{VER}\"\nWANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n\n# Early stopping settings\nUSE_EARLY_STOPPING = True\nEARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\nEARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n\n# LoRA configuration\nLORA_RANK = 128  # LoRAのランク - reduced for memory efficiency\nLORA_ALPHA = 256  # LoRAのスケーリングパラメータ - reduced proportionally\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\nLORA_DROPOUT = 0.1  # LoRAのドロップアウト率\nLORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n\n# Memory optimization settings\nUSE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\nUSE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\nMAX_GRAD_NORM = 1.0  # Gradient clipping value\n\ndef prepare_correct_answers(train_data):\n    \"\"\"正解答案データを準備\"\"\"\n    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n    correct = train_data.loc[idx].copy()\n    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n    correct = correct.sort_values('c', ascending=False)\n    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n    correct['is_correct'] = 1\n    return correct\n\n\ndef format_input(row):\n    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n    if row[\"is_correct\"]:\n        status = \"Yes\"\n    else:\n        status = \"No\"\n\n    # Qwen2.5-Math用の数学タスクに特化したプロンプト\n    prompt = (\n        \"<|im_start|>user\"\n        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct?: {status}\\n\"\n        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n        \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n    )\n    return prompt\n\n\ndef tokenize_dataset(dataset, tokenizer, max_len):\n    \"\"\"データセットをトークナイズ\"\"\"\n    def tokenize(batch):\n        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n        return tokenizer(\n            batch['text'],\n            padding=False,  # パディングはDataCollatorに任せる\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None  # map時は'None'を使用\n        )\n\n    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n    # columnsの設定時にlabelを保持\n    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n    dataset.set_format(type='torch', columns=columns)\n    return dataset\n\n\ndef compute_map3(eval_pred):\n    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n    logits, labels = eval_pred\n    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n    top3 = np.argsort(-probs, axis=1)[:, :3]\n    score = 0.0\n    for i, label in enumerate(labels):\n        ranks = top3[i]\n        if ranks[0] == label:\n            score += 1.0\n        elif ranks[1] == label:\n            score += 1.0 / 2\n        elif ranks[2] == label:\n            score += 1.0 / 3\n    return {\"map@3\": score / len(labels)}\n\n\ndef create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n\n    question_label_choices = {\n        31772: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Incomplete',\n            'True_Misconception:WNB',\n            'False_Neither:NA',\n            'False_Misconception:WNB',\n            'False_Misconception:Incomplete',\n            'False_Correct:NA'\n        ],\n        31774: [\n            'False_Neither:NA',\n            'False_Misconception:SwapDividend',\n            'False_Misconception:Mult',\n            'False_Correct:NA',\n            'False_Misconception:FlipChange',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:SwapDividend',\n            'True_Misconception:Mult',\n            'True_Misconception:FlipChange'\n        ],\n        31777: [\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Irrelevant',\n            'False_Misconception:Wrong_Fraction',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ],\n        31778: [\n            'False_Neither:NA',\n            'False_Misconception:Additive',\n            'False_Misconception:Irrelevant',\n            'False_Correct:NA',\n            'False_Misconception:WNB',\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Irrelevant',\n            'True_Misconception:Additive'\n        ],\n        32829: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Not_variable',\n            'False_Neither:NA',\n            'False_Misconception:Adding_terms',\n            'False_Correct:NA',\n            'False_Misconception:Not_variable',\n            'False_Misconception:Inverse_operation'\n        ],\n        32833: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Inversion',\n            'True_Misconception:Duplication',\n            'False_Misconception:Duplication',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Inversion',\n            'False_Misconception:Wrong_Operation'\n        ],\n        32835: [\n            'False_Misconception:Whole_numbers_larger',\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Longer_is_bigger',\n            'False_Misconception:Ignores_zeroes',\n            'False_Misconception:Shorter_is_bigger',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Whole_numbers_larger',\n            'True_Misconception:Shorter_is_bigger',\n            'True_Misconception:Longer_is_bigger'\n        ],\n        33471: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_fraction',\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Wrong_fraction'\n        ],\n        33472: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Adding_across',\n            'True_Misconception:Denominator-only_change',\n            'True_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Denominator-only_change',\n            'False_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Misconception:Adding_across'\n        ],\n        33474: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Division',\n            'True_Misconception:Subtraction',\n            'False_Neither:NA',\n            'False_Misconception:Subtraction',\n            'False_Misconception:Division',\n            'False_Correct:NA'\n        ],\n        76870: [\n            'False_Misconception:Unknowable',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Definition',\n            'False_Misconception:Interior',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Definition'\n        ],\n        89443: [\n            'False_Neither:NA',\n            'False_Misconception:Positive',\n            'False_Misconception:Tacking',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Tacking',\n            'True_Misconception:Positive',\n            'False_Correct:NA'\n        ],\n        91695: [\n            'False_Neither:NA',\n            'False_Misconception:Wrong_term',\n            'False_Correct:NA',\n            'False_Misconception:Firstterm',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_term',\n            'True_Neither:NA',\n            'True_Misconception:Firstterm'\n        ],\n        104665: [\n            'False_Neither:NA',\n            'False_Misconception:Base_rate',\n            'False_Correct:NA',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Base_rate',\n            'True_Misconception:Multiplying_by_4',\n            'False_Misconception:Multiplying_by_4'\n        ],\n        109465: [\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Certainty',\n            'False_Misconception:Scale',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ]\n    }\n\n    # Identify which are True/False classes\n    true_classes = {}\n    false_classes = {}\n    for idx, c in enumerate(label_encoder.classes_):\n    \n        if 'True' in c:\n            true_classes[idx] = c\n        else:\n            false_classes[idx] = c\n\n    \n    # Normalize for Label Encoder\n    question_label_choice_ids = {}\n    for qid, choices in question_label_choices.items():\n        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n        \n        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n        \n    \n    test_probabilities = []\n    test_predictions = []\n    test_top3_predictions = []\n\n    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n    \n        candidate_idx = question_label_choice_ids[qid]\n\n        # If filter candidates using True/False information\n        if filter_true_false:\n            if correct == 1:\n                # use true_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in true_classes]\n            if correct == 0:\n                # use false_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in false_classes]\n    \n        candidate_logits = row[candidate_idx]\n    \n        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n    \n        top_k = np.argsort(-candidate_probs)\n    \n        # Have to convert back to the original label encoder space\n        topk_idx = np.array(candidate_idx)[top_k]\n    \n        # Keep the probabilities\n        topk_probs = candidate_probs[top_k].tolist()\n    \n        # Get the predicted labels\n        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n    \n        test_probabilities.append(topk_probs)\n        test_predictions.append(topk_preds)\n        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n    \n    test_submission_data = pd.DataFrame({\n        \"row_id\": test_data.row_id.tolist(),\n        \"QuestionId\": test_data.QuestionId.tolist(),\n        \"is_correct\": test_data.is_correct.tolist(),\n        \"probs\": test_probabilities,\n        \"preds\": test_predictions,\n        'Category:Misconception': test_top3_predictions\n    })\n\n    return test_submission_data\n\n\ndef main():\n    \"\"\"メイン推論関数\"\"\"\n    \n    # メモリキャッシュをクリア\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # CUDAメモリ管理の最適化\n    import os\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n    \n    # 2つのGPUを使用可能にする\n    if torch.cuda.device_count() > 1:\n        print(f\"Found {torch.cuda.device_count()} GPUs\")\n\n    print(\"Loading label encoder...\")\n    # ラベルエンコーダーの読み込み\n    le = joblib.load(LABEL_ENCODER_PATH)\n    n_classes = len(le.classes_)\n\n    print(\"Loading trained model and tokenizer...\")\n\n    if PEFT_AVAILABLE:\n        # LoRAアダプターを使用する場合\n        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n        print(f\"Loading base model from: {MODEL_NAME}\")\n\n        # ベースモデルを読み込む（float16で読み込み）\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME,\n            num_labels=n_classes,\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",  # 自動的に複数GPUに分散\n            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n        )\n\n        # LoRAアダプターを適用\n        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n        \n        # 推論モードに設定（メモリ効率化）\n        model.eval()\n        # float16モデルは既にGPUに配置されているのでto('cuda')は不要\n\n        # トークナイザーはベースモデルから読み込む\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n        print(\"Successfully loaded LoRA fine-tuned model\")\n    else:\n        # PEFTが利用できない場合はエラー\n        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n\n    # パディングトークンの設定\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    \n    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n    if hasattr(model, 'base_model'):\n        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n        # 内部のモデルにも設定\n        if hasattr(model.base_model, 'model'):\n            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n    else:\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n    print(\"Loading test data...\")\n    # テストデータの読み込み\n    test = pd.read_csv(TEST_DATA_PATH)\n\n    print(\"Loading training data for correct answers...\")\n    # 正解答案データの準備（訓練データから取得）\n    train = pd.read_csv(TRAIN_DATA_PATH)\n    train.Misconception = train.Misconception.fillna('NA')\n    correct = prepare_correct_answers(train)\n\n    print(\"Preprocessing test data...\")\n    # テストデータの前処理\n    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n    test.is_correct = test.is_correct.fillna(0)\n    test['text'] = test.apply(format_input, axis=1)\n\n    print(\"Tokenizing test data...\")\n    # テストデータのトークナイズ\n    ds_test = Dataset.from_pandas(test[['text']])\n    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n\n    # パディングのためのデータコラレータの設定\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    print(\"Running inference...\")\n    \n    # TF32を有効化（推論速度向上）\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # 推論の実行\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,  # tokenizer の代替\n        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n        args=TrainingArguments(\n            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n            report_to=\"none\",    # wandbを無効化\n            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n            fp16=True,  # float16を使用\n            dataloader_pin_memory=True,  # データローダーの高速化\n            dataloader_num_workers=2,  # データ読み込みの並列化\n        )\n    )\n    # no_gradコンテキストで推論を実行（メモリ効率化）\n    with torch.no_grad():\n        predictions = trainer.predict(ds_test)\n\n    print(\"Creating submission file...\")\n    # 提出用ファイルの作成\n    submission = create_submission(predictions, test, le)\n\n    # ファイルの保存\n    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n    print(\"\\nSubmission preview:\")\n    print(submission.head())\n    print(f\"\\nSubmission shape: {submission.shape}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:31:26.462934Z","iopub.execute_input":"2025-09-04T03:31:26.463191Z","iopub.status.idle":"2025-09-04T03:31:26.483899Z","shell.execute_reply.started":"2025-09-04T03:31:26.463167Z","shell.execute_reply":"2025-09-04T03:31:26.483116Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile qwen3_32b_0_947.py\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport joblib\nimport torch\n\ntry:\n    from peft import PeftModel, PeftConfig\n    PEFT_AVAILABLE = True\nexcept ImportError:\n    PEFT_AVAILABLE = False\n    print(\"Warning: PEFT not available, will use base model only\")\n\n# Model configuration\nVER = 2\nMODEL_NAME = \"/kaggle/input/qwen-3/transformers/32b/1\"\nMODEL_TYPE = \"qwen2\"  # Add model type for proper handling\nEPOCHS = 3  # Reduce epochs for initial testing\nMAX_LEN = 300  # Increase max length for better context\n\n# Directory settings\nOUTPUT_DIR = f\"/kaggle/input/qwen3-32b-9468/transformers/default/1/ver_2\"\n\n# Training parameters\nTRAIN_BATCH_SIZE = 16  # Batch size 2 for RTX 5090 with 31GB VRAM\nEVAL_BATCH_SIZE = 16  # Eval can use larger batch size\nGRADIENT_ACCUMULATION_STEPS = 2  # Reduced to 32 for faster training\nLEARNING_RATE = 2e-4\nLOGGING_STEPS = 50\nSAVE_STEPS = 200\nEVAL_STEPS = 200\n\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\nTEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n\n# Model save paths\nBEST_MODEL_PATH = f\"{OUTPUT_DIR}/best\"\nLABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n\n# Other settings\nRANDOM_SEED = 42\nVALIDATION_SPLIT = 0.2\n\n# GPU settings\nCUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n\n# Submission settings\nSUBMISSION_OUTPUT_PATH = 'qwen3_32b_0_947_submission.csv'\n\n# WandB settings\nUSE_WANDB = True  # Set to False to disable WandB\nWANDB_PROJECT = \"qwen3-32b-math-misconceptions\"\nWANDB_RUN_NAME = f\"qwen3-32b-ver{VER}\"\nWANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n\n# Early stopping settings\nUSE_EARLY_STOPPING = True\nEARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\nEARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n\n# LoRA configuration\nLORA_RANK = 16  # LoRAのランク - reduced for memory efficiency\nLORA_ALPHA = 32  # LoRAのスケーリングパラメータ - reduced proportionally\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # 対象モジュール\nLORA_DROPOUT = 0.1  # LoRAのドロップアウト率\nLORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n\n# Memory optimization settings\nUSE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\nUSE_8BIT_ADAM = True  # Use 8-bit Adam optimizer for memory efficiency\nMAX_GRAD_NORM = 1.0  # Gradient clipping value\n\ndef prepare_correct_answers(train_data):\n    \"\"\"正解答案データを準備\"\"\"\n    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n    correct = train_data.loc[idx].copy()\n    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n    correct = correct.sort_values('c', ascending=False)\n    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n    correct['is_correct'] = 1\n    return correct\n\n\ndef format_input(row):\n    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n    if row[\"is_correct\"]:\n        status = \"Yes\"\n    else:\n        status = \"No\"\n\n    # Qwen2.5-Math用の数学タスクに特化したプロンプト\n    prompt = (\n        \"<|im_start|>user\"\n        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct?: {status}\\n\"\n        f\"Explanation: {row['StudentExplanation']}\\n\\n\"\n        \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n    )\n    return prompt\n\n\ndef tokenize_dataset(dataset, tokenizer, max_len):\n    \"\"\"データセットをトークナイズ\"\"\"\n    def tokenize(batch):\n        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n        return tokenizer(\n            batch['text'],\n            padding=False,  # パディングはDataCollatorに任せる\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None  # map時は'None'を使用\n        )\n\n    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n    # columnsの設定時にlabelを保持\n    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n    dataset.set_format(type='torch', columns=columns)\n    return dataset\n\n\ndef compute_map3(eval_pred):\n    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n    logits, labels = eval_pred\n    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n    top3 = np.argsort(-probs, axis=1)[:, :3]\n    score = 0.0\n    for i, label in enumerate(labels):\n        ranks = top3[i]\n        if ranks[0] == label:\n            score += 1.0\n        elif ranks[1] == label:\n            score += 1.0 / 2\n        elif ranks[2] == label:\n            score += 1.0 / 3\n    return {\"map@3\": score / len(labels)}\n\n\ndef create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n\n    question_label_choices = {\n        31772: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Incomplete',\n            'True_Misconception:WNB',\n            'False_Neither:NA',\n            'False_Misconception:WNB',\n            'False_Misconception:Incomplete',\n            'False_Correct:NA'\n        ],\n        31774: [\n            'False_Neither:NA',\n            'False_Misconception:SwapDividend',\n            'False_Misconception:Mult',\n            'False_Correct:NA',\n            'False_Misconception:FlipChange',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:SwapDividend',\n            'True_Misconception:Mult',\n            'True_Misconception:FlipChange'\n        ],\n        31777: [\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Irrelevant',\n            'False_Misconception:Wrong_Fraction',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ],\n        31778: [\n            'False_Neither:NA',\n            'False_Misconception:Additive',\n            'False_Misconception:Irrelevant',\n            'False_Correct:NA',\n            'False_Misconception:WNB',\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Irrelevant',\n            'True_Misconception:Additive'\n        ],\n        32829: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Not_variable',\n            'False_Neither:NA',\n            'False_Misconception:Adding_terms',\n            'False_Correct:NA',\n            'False_Misconception:Not_variable',\n            'False_Misconception:Inverse_operation'\n        ],\n        32833: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Inversion',\n            'True_Misconception:Duplication',\n            'False_Misconception:Duplication',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Inversion',\n            'False_Misconception:Wrong_Operation'\n        ],\n        32835: [\n            'False_Misconception:Whole_numbers_larger',\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Longer_is_bigger',\n            'False_Misconception:Ignores_zeroes',\n            'False_Misconception:Shorter_is_bigger',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Whole_numbers_larger',\n            'True_Misconception:Shorter_is_bigger',\n            'True_Misconception:Longer_is_bigger'\n        ],\n        33471: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_fraction',\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Wrong_fraction'\n        ],\n        33472: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Adding_across',\n            'True_Misconception:Denominator-only_change',\n            'True_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Denominator-only_change',\n            'False_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Misconception:Adding_across'\n        ],\n        33474: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Division',\n            'True_Misconception:Subtraction',\n            'False_Neither:NA',\n            'False_Misconception:Subtraction',\n            'False_Misconception:Division',\n            'False_Correct:NA'\n        ],\n        76870: [\n            'False_Misconception:Unknowable',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Definition',\n            'False_Misconception:Interior',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Definition'\n        ],\n        89443: [\n            'False_Neither:NA',\n            'False_Misconception:Positive',\n            'False_Misconception:Tacking',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Tacking',\n            'True_Misconception:Positive',\n            'False_Correct:NA'\n        ],\n        91695: [\n            'False_Neither:NA',\n            'False_Misconception:Wrong_term',\n            'False_Correct:NA',\n            'False_Misconception:Firstterm',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_term',\n            'True_Neither:NA',\n            'True_Misconception:Firstterm'\n        ],\n        104665: [\n            'False_Neither:NA',\n            'False_Misconception:Base_rate',\n            'False_Correct:NA',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Base_rate',\n            'True_Misconception:Multiplying_by_4',\n            'False_Misconception:Multiplying_by_4'\n        ],\n        109465: [\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Certainty',\n            'False_Misconception:Scale',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ]\n    }\n\n    # Identify which are True/False classes\n    true_classes = {}\n    false_classes = {}\n    for idx, c in enumerate(label_encoder.classes_):\n    \n        if 'True' in c:\n            true_classes[idx] = c\n        else:\n            false_classes[idx] = c\n\n    \n    # Normalize for Label Encoder\n    question_label_choice_ids = {}\n    for qid, choices in question_label_choices.items():\n        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n        \n        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n        \n    \n    test_probabilities = []\n    test_predictions = []\n    test_top3_predictions = []\n\n    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n    \n        candidate_idx = question_label_choice_ids[qid]\n\n        # If filter candidates using True/False information\n        if filter_true_false:\n            if correct == 1:\n                # use true_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in true_classes]\n            if correct == 0:\n                # use false_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in false_classes]\n    \n        candidate_logits = row[candidate_idx]\n    \n        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n    \n        top_k = np.argsort(-candidate_probs)\n    \n        # Have to convert back to the original label encoder space\n        topk_idx = np.array(candidate_idx)[top_k]\n    \n        # Keep the probabilities\n        topk_probs = candidate_probs[top_k].tolist()\n    \n        # Get the predicted labels\n        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n    \n        test_probabilities.append(topk_probs)\n        test_predictions.append(topk_preds)\n        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n    \n    test_submission_data = pd.DataFrame({\n        \"row_id\": test_data.row_id.tolist(),\n        \"QuestionId\": test_data.QuestionId.tolist(),\n        \"is_correct\": test_data.is_correct.tolist(),\n        \"probs\": test_probabilities,\n        \"preds\": test_predictions,\n        'Category:Misconception': test_top3_predictions\n    })\n\n    return test_submission_data\n\n\ndef main():\n    \"\"\"メイン推論関数\"\"\"\n\n    # メモリキャッシュをクリア\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    # CUDAメモリ管理の最適化\n    import os\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n    # 2つのGPUを使用可能にする\n    if torch.cuda.device_count() > 1:\n        print(f\"Found {torch.cuda.device_count()} GPUs\")\n\n    print(\"Loading label encoder...\")\n    # ラベルエンコーダーの読み込み\n    le = joblib.load(LABEL_ENCODER_PATH)\n    n_classes = len(le.classes_)\n\n    print(\"Loading trained model and tokenizer...\")\n\n    if PEFT_AVAILABLE:\n        # LoRAアダプターを使用する場合\n        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n        print(f\"Loading base model from: {MODEL_NAME}\")\n\n        # ベースモデルを読み込む（4bit量子化で読み込み）\n        from transformers import BitsAndBytesConfig\n\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME,\n            num_labels=n_classes,\n            trust_remote_code=True,\n            quantization_config=quantization_config,\n            device_map=\"auto\",  # 自動的に複数GPUに分散\n            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n        )\n\n        # LoRAアダプターを適用\n        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n\n        # 推論モードに設定（メモリ効率化）\n        model.eval()\n        # 4bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n\n        # トークナイザーはベースモデルから読み込む\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n        print(\"Successfully loaded LoRA fine-tuned model\")\n    else:\n        # PEFTが利用できない場合はエラー\n        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n\n    # パディングトークンの設定\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n    if hasattr(model, 'base_model'):\n        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n        # 内部のモデルにも設定\n        if hasattr(model.base_model, 'model'):\n            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n    else:\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n    print(\"Loading test data...\")\n    # テストデータの読み込み\n    test = pd.read_csv(TEST_DATA_PATH)\n\n    print(\"Loading training data for correct answers...\")\n    # 正解答案データの準備（訓練データから取得）\n    train = pd.read_csv(TRAIN_DATA_PATH)\n    train.Misconception = train.Misconception.fillna('NA')\n    correct = prepare_correct_answers(train)\n\n    print(\"Preprocessing test data...\")\n    # テストデータの前処理\n    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n    test.is_correct = test.is_correct.fillna(0)\n    test['text'] = test.apply(format_input, axis=1)\n\n    print(\"Tokenizing test data...\")\n    # テストデータのトークナイズ\n    ds_test = Dataset.from_pandas(test[['text']])\n    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n\n    # パディングのためのデータコラレータの設定\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    print(\"Running inference...\")\n\n    # TF32を有効化（推論速度向上）\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n\n    # 推論の実行\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,  # tokenizer の代替\n        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n        args=TrainingArguments(\n            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n            report_to=\"none\",    # wandbを無効化\n            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n            fp16=True,  # float16を使用\n            dataloader_pin_memory=True,  # データローダーの高速化\n            dataloader_num_workers=2,  # データ読み込みの並列化\n        )\n    )\n    # no_gradコンテキストで推論を実行（メモリ効率化）\n    with torch.no_grad():\n        predictions = trainer.predict(ds_test)\n\n    print(\"Creating submission file...\")\n    # 提出用ファイルの作成\n    submission = create_submission(predictions, test, le)\n\n    # ファイルの保存\n    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n    print(\"\\nSubmission preview:\")\n    print(submission.head())\n    print(f\"\\nSubmission shape: {submission.shape}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:31:26.485754Z","iopub.execute_input":"2025-09-04T03:31:26.485951Z","iopub.status.idle":"2025-09-04T03:31:26.507099Z","shell.execute_reply.started":"2025-09-04T03:31:26.485934Z","shell.execute_reply":"2025-09-04T03:31:26.506387Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile phi_4_0_948_fulltrain.py\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport joblib\nimport torch\n\ntry:\n    from peft import PeftModel, PeftConfig\n    PEFT_AVAILABLE = True\nexcept ImportError:\n    PEFT_AVAILABLE = False\n    print(\"Warning: PEFT not available, will use base model only\")\n\n# Model configuration\nVER = 2\nMODEL_NAME = \"/kaggle/input/ms-phi4/transformers/default/1/phi-4\"\nMODEL_TYPE = \"phi\"  # Phi-4 model type\nEPOCHS = 3  # Reduce epochs for initial testing\nMAX_LEN = 250  # Phi-4 supports longer context\n\n# Directory settings\nOUTPUT_DIR = f\"/kaggle/input/phi-4-cv0965-fulltrain/transformers/default/1/ver_2_0965ft\"\n\n# Training parameters\nTRAIN_BATCH_SIZE = 4  # Smaller batch size for Phi-4\nEVAL_BATCH_SIZE = 4  # Eval batch size\nGRADIENT_ACCUMULATION_STEPS = 16  # Increased for effective batch size\nLEARNING_RATE = 2e-4\nLOGGING_STEPS = 50\nSAVE_STEPS = 200\nEVAL_STEPS = 200\n\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\nTEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n\n# Model save paths\nBEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\nLABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n\n# Other settings\nRANDOM_SEED = 42\nVALIDATION_SPLIT = 0.2\n\n# GPU settings\nCUDA_VISIBLE_DEVICES = \"0\"  # GPU device to use. Set to None to use all available GPUs\n\n# Submission settings\nSUBMISSION_OUTPUT_PATH = 'phi_4_0_948_fulltrain_submission.csv'\n\n# WandB settings\nUSE_WANDB = True  # Set to False to disable WandB\nWANDB_PROJECT = \"phi-4-math-misconceptions\"\nWANDB_RUN_NAME = f\"phi-4-ver{VER}\"\nWANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n\n# Early stopping settings\nUSE_EARLY_STOPPING = True\nEARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\nEARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n\n# LoRA configuration for Phi-4\nLORA_RANK = 64  # LoRAのランク - optimized for Phi-4\nLORA_ALPHA = 128  # LoRAのスケーリングパラメータ - 1:1 ratio with rank\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Phi-4 target modules\nLORA_DROPOUT = 0.1  # LoRAのドロップアウト率 - reduced for Phi-4\nLORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n\n# Memory optimization settings\nUSE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\nUSE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\nMAX_GRAD_NORM = 1.0  # Gradient clipping value\n\ndef prepare_correct_answers(train_data):\n    \"\"\"正解答案データを準備\"\"\"\n    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n    correct = train_data.loc[idx].copy()\n    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n    correct = correct.sort_values('c', ascending=False)\n    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n    correct['is_correct'] = 1\n    return correct\n\n\ndef format_input(row):\n    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n    if row[\"is_correct\"]:\n        status = \"Yes\"\n    else:\n        status = \"No\"\n\n    # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n    prompt = (\n        \"<|user|>\\n\"\n        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct?: {status}\\n\"\n        f\"Explanation: {row['StudentExplanation']}\\n\"\n        \"<|end|>\\n\"\n        \"<|assistant|>\\n\"\n        \"<think>\\n\"\n        \"Let me analyze this mathematical misconception...\\n\"\n        \"</think>\\n\\n\"\n    )\n    return prompt\n\n\ndef tokenize_dataset(dataset, tokenizer, max_len):\n    \"\"\"データセットをトークナイズ\"\"\"\n    def tokenize(batch):\n        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n        return tokenizer(\n            batch['text'],\n            padding=False,  # パディングはDataCollatorに任せる\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None  # map時は'None'を使用\n        )\n\n    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n    # columnsの設定時にlabelを保持\n    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n    dataset.set_format(type='torch', columns=columns)\n    return dataset\n\n\ndef compute_map3(eval_pred):\n    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n    logits, labels = eval_pred\n    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n    top3 = np.argsort(-probs, axis=1)[:, :3]\n    score = 0.0\n    for i, label in enumerate(labels):\n        ranks = top3[i]\n        if ranks[0] == label:\n            score += 1.0\n        elif ranks[1] == label:\n            score += 1.0 / 2\n        elif ranks[2] == label:\n            score += 1.0 / 3\n    return {\"map@3\": score / len(labels)}\n\n\ndef create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n\n    question_label_choices = {\n        31772: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Incomplete',\n            'True_Misconception:WNB',\n            'False_Neither:NA',\n            'False_Misconception:WNB',\n            'False_Misconception:Incomplete',\n            'False_Correct:NA'\n        ],\n        31774: [\n            'False_Neither:NA',\n            'False_Misconception:SwapDividend',\n            'False_Misconception:Mult',\n            'False_Correct:NA',\n            'False_Misconception:FlipChange',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:SwapDividend',\n            'True_Misconception:Mult',\n            'True_Misconception:FlipChange'\n        ],\n        31777: [\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Irrelevant',\n            'False_Misconception:Wrong_Fraction',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ],\n        31778: [\n            'False_Neither:NA',\n            'False_Misconception:Additive',\n            'False_Misconception:Irrelevant',\n            'False_Correct:NA',\n            'False_Misconception:WNB',\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Irrelevant',\n            'True_Misconception:Additive'\n        ],\n        32829: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Not_variable',\n            'False_Neither:NA',\n            'False_Misconception:Adding_terms',\n            'False_Correct:NA',\n            'False_Misconception:Not_variable',\n            'False_Misconception:Inverse_operation'\n        ],\n        32833: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Inversion',\n            'True_Misconception:Duplication',\n            'False_Misconception:Duplication',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Inversion',\n            'False_Misconception:Wrong_Operation'\n        ],\n        32835: [\n            'False_Misconception:Whole_numbers_larger',\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Longer_is_bigger',\n            'False_Misconception:Ignores_zeroes',\n            'False_Misconception:Shorter_is_bigger',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Whole_numbers_larger',\n            'True_Misconception:Shorter_is_bigger',\n            'True_Misconception:Longer_is_bigger'\n        ],\n        33471: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_fraction',\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Wrong_fraction'\n        ],\n        33472: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Adding_across',\n            'True_Misconception:Denominator-only_change',\n            'True_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Denominator-only_change',\n            'False_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Misconception:Adding_across'\n        ],\n        33474: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Division',\n            'True_Misconception:Subtraction',\n            'False_Neither:NA',\n            'False_Misconception:Subtraction',\n            'False_Misconception:Division',\n            'False_Correct:NA'\n        ],\n        76870: [\n            'False_Misconception:Unknowable',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Definition',\n            'False_Misconception:Interior',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Definition'\n        ],\n        89443: [\n            'False_Neither:NA',\n            'False_Misconception:Positive',\n            'False_Misconception:Tacking',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Tacking',\n            'True_Misconception:Positive',\n            'False_Correct:NA'\n        ],\n        91695: [\n            'False_Neither:NA',\n            'False_Misconception:Wrong_term',\n            'False_Correct:NA',\n            'False_Misconception:Firstterm',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_term',\n            'True_Neither:NA',\n            'True_Misconception:Firstterm'\n        ],\n        104665: [\n            'False_Neither:NA',\n            'False_Misconception:Base_rate',\n            'False_Correct:NA',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Base_rate',\n            'True_Misconception:Multiplying_by_4',\n            'False_Misconception:Multiplying_by_4'\n        ],\n        109465: [\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Certainty',\n            'False_Misconception:Scale',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ]\n    }\n\n    # Identify which are True/False classes\n    true_classes = {}\n    false_classes = {}\n    for idx, c in enumerate(label_encoder.classes_):\n    \n        if 'True' in c:\n            true_classes[idx] = c\n        else:\n            false_classes[idx] = c\n\n    \n    # Normalize for Label Encoder\n    question_label_choice_ids = {}\n    for qid, choices in question_label_choices.items():\n        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n        \n        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n        \n    \n    test_probabilities = []\n    test_predictions = []\n    test_top3_predictions = []\n\n    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n    \n        candidate_idx = question_label_choice_ids[qid]\n\n        # If filter candidates using True/False information\n        if filter_true_false:\n            if correct == 1:\n                # use true_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in true_classes]\n            if correct == 0:\n                # use false_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in false_classes]\n    \n        candidate_logits = row[candidate_idx]\n    \n        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n    \n        top_k = np.argsort(-candidate_probs)\n    \n        # Have to convert back to the original label encoder space\n        topk_idx = np.array(candidate_idx)[top_k]\n    \n        # Keep the probabilities\n        topk_probs = candidate_probs[top_k].tolist()\n    \n        # Get the predicted labels\n        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n    \n        test_probabilities.append(topk_probs)\n        test_predictions.append(topk_preds)\n        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n    \n    test_submission_data = pd.DataFrame({\n        \"row_id\": test_data.row_id.tolist(),\n        \"QuestionId\": test_data.QuestionId.tolist(),\n        \"is_correct\": test_data.is_correct.tolist(),\n        \"probs\": test_probabilities,\n        \"preds\": test_predictions,\n        'Category:Misconception': test_top3_predictions\n    })\n\n    return test_submission_data\n\n\ndef main():\n    \"\"\"メイン推論関数\"\"\"\n    \n    # メモリキャッシュをクリア\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # CUDAメモリ管理の最適化\n    import os\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n    \n    # 2つのGPUを使用可能にする\n    if torch.cuda.device_count() > 1:\n        print(f\"Found {torch.cuda.device_count()} GPUs\")\n\n    print(\"Loading label encoder...\")\n    # ラベルエンコーダーの読み込み\n    le = joblib.load(LABEL_ENCODER_PATH)\n    n_classes = len(le.classes_)\n\n    print(\"Loading trained model and tokenizer...\")\n\n    if PEFT_AVAILABLE:\n        # LoRAアダプターを使用する場合\n        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n        print(f\"Loading base model from: {MODEL_NAME}\")\n\n        # ベースモデルを読み込む（量子化なしでフルプレシジョン）\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME,\n            num_labels=n_classes,\n            trust_remote_code=True,\n            device_map=\"auto\",  # 自動的に複数GPUに分散\n            torch_dtype=torch.float16,  # float16を使用（メモリ効率とパフォーマンスのバランス）\n            low_cpu_mem_usage=True  # CPUメモリ使用量を削減\n        )\n\n        # LoRAアダプターを適用\n        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n        \n        # 推論モードに設定（メモリ効率化）\n        model.eval()\n        # モデルは既にdevice_mapでGPUに配置されているのでto('cuda')は不要\n\n        # トークナイザーはベースモデルから読み込む\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n        print(\"Successfully loaded LoRA fine-tuned model\")\n    else:\n        # PEFTが利用できない場合はエラー\n        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n\n    # パディングトークンの設定\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n        tokenizer.pad_token_id = 100257\n    \n    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n    if hasattr(model, 'base_model'):\n        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n        # 内部のモデルにも設定\n        if hasattr(model.base_model, 'model'):\n            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n    else:\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n    print(\"Loading test data...\")\n    # テストデータの読み込み\n    test = pd.read_csv(TEST_DATA_PATH)\n\n    print(\"Loading training data for correct answers...\")\n    # 正解答案データの準備（訓練データから取得）\n    train = pd.read_csv(TRAIN_DATA_PATH)\n    train.Misconception = train.Misconception.fillna('NA')\n    correct = prepare_correct_answers(train)\n\n    print(\"Preprocessing test data...\")\n    # テストデータの前処理\n    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n    test.is_correct = test.is_correct.fillna(0)\n    test['text'] = test.apply(format_input, axis=1)\n\n    print(\"Tokenizing test data...\")\n    # テストデータのトークナイズ\n    ds_test = Dataset.from_pandas(test[['text']])\n    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n\n    # パディングのためのデータコラレータの設定\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    print(\"Running inference...\")\n    \n    # TF32を有効化（推論速度向上）\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # 推論の実行\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,  # tokenizer の代替\n        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n        args=TrainingArguments(\n            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n            report_to=\"none\",    # wandbを無効化\n            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n            fp16=True,  # float16を使用\n            dataloader_pin_memory=True,  # データローダーの高速化\n            dataloader_num_workers=2,  # データ読み込みの並列化\n        )\n    )\n    # no_gradコンテキストで推論を実行（メモリ効率化）\n    with torch.no_grad():\n        predictions = trainer.predict(ds_test)\n\n    print(\"Creating submission file...\")\n    # 提出用ファイルの作成\n    submission = create_submission(predictions, test, le)\n\n    # ファイルの保存\n    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n    print(\"\\nSubmission preview:\")\n    print(submission.head())\n    print(f\"\\nSubmission shape: {submission.shape}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:31:26.508143Z","iopub.execute_input":"2025-09-04T03:31:26.50843Z","iopub.status.idle":"2025-09-04T03:31:26.534025Z","shell.execute_reply.started":"2025-09-04T03:31:26.508411Z","shell.execute_reply":"2025-09-04T03:31:26.533382Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile phi_4_reasoning_0_948.py\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport joblib\nimport torch\n\ntry:\n    from peft import PeftModel, PeftConfig\n    PEFT_AVAILABLE = True\nexcept ImportError:\n    PEFT_AVAILABLE = False\n    print(\"Warning: PEFT not available, will use base model only\")\n\n# Model configuration\nVER = 2\nMODEL_NAME = \"/kaggle/input/phi4-reasoning-plus/transformers/default/1/Phi-4-reasoning-plus\"\nMODEL_TYPE = \"phi\"  # Phi-4 model type\nEPOCHS = 3  # Reduce epochs for initial testing\nMAX_LEN = 250  # Phi-4-reasoning-plus supports 32k context, but we use 1024 for efficiency\n\n# Directory settings\nOUTPUT_DIR = f\"/kaggle/input/phi-4-reasoning-plus09476-ft/transformers/default/1/ver_2_9476ft\"\n\n# Training parameters\nTRAIN_BATCH_SIZE = 4  # Smaller batch size for Phi-4\nEVAL_BATCH_SIZE = 4  # Eval batch size\nGRADIENT_ACCUMULATION_STEPS = 16  # Increased for effective batch size\nLEARNING_RATE = 2e-4\nLOGGING_STEPS = 50\nSAVE_STEPS = 200\nEVAL_STEPS = 200\n\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\nTEST_DATA_PATH = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n\n# Model save paths\nBEST_MODEL_PATH = f\"{OUTPUT_DIR}/checkpoint-1722\"\nLABEL_ENCODER_PATH = f\"{OUTPUT_DIR}/label_encoder.joblib\"\n\n# Other settings\nRANDOM_SEED = 42\nVALIDATION_SPLIT = 0.0000001\n\n# GPU settings\nCUDA_VISIBLE_DEVICES = \"0,1\"  # GPU device to use. Set to None to use all available GPUs\n\n# Submission settings\nSUBMISSION_OUTPUT_PATH = 'phi_4_reasoning_0_948_submission.csv'\n\n# WandB settings\nUSE_WANDB = True  # Set to False to disable WandB\nWANDB_PROJECT = \"phi-4-reasoning-math-misconceptions\"\nWANDB_RUN_NAME = f\"phi-4-reasoning-ver{VER}\"\nWANDB_ENTITY = None  # Set your WandB entity (username or team name) if needed\n\n# Early stopping settings\nUSE_EARLY_STOPPING = True\nEARLY_STOPPING_PATIENCE = 10  # 改善が見られない評価回数の上限（評価はEVAL_STEPSごとに実行される）\nEARLY_STOPPING_THRESHOLD = 0.001  # 改善とみなす最小変化量\n\n# LoRA configuration for Phi-4\nLORA_RANK = 64  # LoRAのランク - optimized for Phi-4\nLORA_ALPHA = 128  # LoRAのスケーリングパラメータ - 1:1 ratio with rank\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Phi-4 target modules\nLORA_DROPOUT = 0.1  # LoRAのドロップアウト率 - reduced for Phi-4\nLORA_BIAS = \"none\"  # biasの扱い: \"none\", \"all\", \"lora_only\"\n\n# Memory optimization settings\nUSE_GRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing\nUSE_8BIT_ADAM = False  # Use 8-bit Adam optimizer for memory efficiency\nMAX_GRAD_NORM = 1.0  # Gradient clipping value\n\ndef prepare_correct_answers(train_data):\n    \"\"\"正解答案データを準備\"\"\"\n    idx = train_data.apply(lambda row: row.Category.split('_')[0] == 'True', axis=1)\n    correct = train_data.loc[idx].copy()\n    correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n    correct = correct.sort_values('c', ascending=False)\n    correct = correct.drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']]\n    correct['is_correct'] = 1\n    return correct\n\n\ndef format_input(row):\n    \"\"\"入力データをモデル用プロンプトにフォーマット\"\"\"\n    if row[\"is_correct\"]:\n        status = \"Yes\"\n    else:\n        status = \"No\"\n\n    # Phi-4用のプロンプトフォーマット（特別なthinkタグを含む）\n    prompt = (\n        \"<|user|>\\n\"\n        f\"[Mathematical Misconception Analysis Task]\\n\\n\"\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct?: {status}\\n\"\n        f\"Explanation: {row['StudentExplanation']}\\n\"\n        \"<|end|>\\n\"\n        \"<|assistant|>\\n\"\n        \"<think>\\n\"\n        \"Let me analyze this mathematical misconception...\\n\"\n        \"</think>\\n\\n\"\n    )\n    return prompt\n\n\ndef tokenize_dataset(dataset, tokenizer, max_len):\n    \"\"\"データセットをトークナイズ\"\"\"\n    def tokenize(batch):\n        # パディングはDataCollatorで行うため、ここではトークナイズのみ\n        return tokenizer(\n            batch['text'],\n            padding=False,  # パディングはDataCollatorに任せる\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None  # map時は'None'を使用\n        )\n\n    dataset = dataset.map(tokenize, batched=True, batch_size=100)\n    # columnsの設定時にlabelを保持\n    columns = ['input_ids', 'attention_mask', 'label'] if 'label' in dataset.column_names else ['input_ids', 'attention_mask']\n    dataset.set_format(type='torch', columns=columns)\n    return dataset\n\n\ndef compute_map3(eval_pred):\n    \"\"\"Top-3 予測に基づくMAP@3を計算\"\"\"\n    logits, labels = eval_pred\n    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n    top3 = np.argsort(-probs, axis=1)[:, :3]\n    score = 0.0\n    for i, label in enumerate(labels):\n        ranks = top3[i]\n        if ranks[0] == label:\n            score += 1.0\n        elif ranks[1] == label:\n            score += 1.0 / 2\n        elif ranks[2] == label:\n            score += 1.0 / 3\n    return {\"map@3\": score / len(labels)}\n\n\ndef create_submission(predictions, test_data, label_encoder, filter_true_false = True):\n\n    question_label_choices = {\n        31772: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Incomplete',\n            'True_Misconception:WNB',\n            'False_Neither:NA',\n            'False_Misconception:WNB',\n            'False_Misconception:Incomplete',\n            'False_Correct:NA'\n        ],\n        31774: [\n            'False_Neither:NA',\n            'False_Misconception:SwapDividend',\n            'False_Misconception:Mult',\n            'False_Correct:NA',\n            'False_Misconception:FlipChange',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:SwapDividend',\n            'True_Misconception:Mult',\n            'True_Misconception:FlipChange'\n        ],\n        31777: [\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Irrelevant',\n            'False_Misconception:Wrong_Fraction',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ],\n        31778: [\n            'False_Neither:NA',\n            'False_Misconception:Additive',\n            'False_Misconception:Irrelevant',\n            'False_Correct:NA',\n            'False_Misconception:WNB',\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Irrelevant',\n            'True_Misconception:Additive'\n        ],\n        32829: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Not_variable',\n            'False_Neither:NA',\n            'False_Misconception:Adding_terms',\n            'False_Correct:NA',\n            'False_Misconception:Not_variable',\n            'False_Misconception:Inverse_operation'\n        ],\n        32833: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Inversion',\n            'True_Misconception:Duplication',\n            'False_Misconception:Duplication',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Inversion',\n            'False_Misconception:Wrong_Operation'\n        ],\n        32835: [\n            'False_Misconception:Whole_numbers_larger',\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Longer_is_bigger',\n            'False_Misconception:Ignores_zeroes',\n            'False_Misconception:Shorter_is_bigger',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Whole_numbers_larger',\n            'True_Misconception:Shorter_is_bigger',\n            'True_Misconception:Longer_is_bigger'\n        ],\n        33471: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_fraction',\n            'False_Correct:NA',\n            'False_Misconception:Incomplete',\n            'False_Neither:NA',\n            'False_Misconception:Wrong_fraction'\n        ],\n        33472: [\n            'True_Neither:NA',\n            'True_Correct:NA',\n            'True_Misconception:Adding_across',\n            'True_Misconception:Denominator-only_change',\n            'True_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Denominator-only_change',\n            'False_Misconception:Incorrect_equivalent_fraction_addition',\n            'False_Misconception:Adding_across'\n        ],\n        33474: [\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Division',\n            'True_Misconception:Subtraction',\n            'False_Neither:NA',\n            'False_Misconception:Subtraction',\n            'False_Misconception:Division',\n            'False_Correct:NA'\n        ],\n        76870: [\n            'False_Misconception:Unknowable',\n            'False_Correct:NA',\n            'False_Neither:NA',\n            'False_Misconception:Definition',\n            'False_Misconception:Interior',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Definition'\n        ],\n        89443: [\n            'False_Neither:NA',\n            'False_Misconception:Positive',\n            'False_Misconception:Tacking',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Tacking',\n            'True_Misconception:Positive',\n            'False_Correct:NA'\n        ],\n        91695: [\n            'False_Neither:NA',\n            'False_Misconception:Wrong_term',\n            'False_Correct:NA',\n            'False_Misconception:Firstterm',\n            'True_Correct:NA',\n            'True_Misconception:Wrong_term',\n            'True_Neither:NA',\n            'True_Misconception:Firstterm'\n        ],\n        104665: [\n            'False_Neither:NA',\n            'False_Misconception:Base_rate',\n            'False_Correct:NA',\n            'True_Correct:NA',\n            'True_Neither:NA',\n            'True_Misconception:Base_rate',\n            'True_Misconception:Multiplying_by_4',\n            'False_Misconception:Multiplying_by_4'\n        ],\n        109465: [\n            'False_Neither:NA',\n            'False_Correct:NA',\n            'False_Misconception:Certainty',\n            'False_Misconception:Scale',\n            'True_Correct:NA',\n            'True_Neither:NA'\n        ]\n    }\n\n    # Identify which are True/False classes\n    true_classes = {}\n    false_classes = {}\n    for idx, c in enumerate(label_encoder.classes_):\n    \n        if 'True' in c:\n            true_classes[idx] = c\n        else:\n            false_classes[idx] = c\n\n    \n    # Normalize for Label Encoder\n    question_label_choice_ids = {}\n    for qid, choices in question_label_choices.items():\n        _label_ids = np.where(np.isin(label_encoder.classes_, question_label_choices[qid]))[0]\n        \n        question_label_choice_ids[qid] = [int(x) for x in _label_ids]\n        \n    \n    test_probabilities = []\n    test_predictions = []\n    test_top3_predictions = []\n\n    for qid, correct, row in zip(test_data.QuestionId.tolist(), test_data.is_correct.tolist(), predictions.predictions):\n    \n        candidate_idx = question_label_choice_ids[qid]\n\n        # If filter candidates using True/False information\n        if filter_true_false:\n            if correct == 1:\n                # use true_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in true_classes]\n            if correct == 0:\n                # use false_classes to filter candidate_idx\n                candidate_idx = [c for c in candidate_idx if c in false_classes]\n    \n        candidate_logits = row[candidate_idx]\n    \n        candidate_probs = torch.nn.functional.softmax(torch.tensor(candidate_logits), dim=-1).numpy()\n    \n        top_k = np.argsort(-candidate_probs)\n    \n        # Have to convert back to the original label encoder space\n        topk_idx = np.array(candidate_idx)[top_k]\n    \n        # Keep the probabilities\n        topk_probs = candidate_probs[top_k].tolist()\n    \n        # Get the predicted labels\n        topk_preds = label_encoder.inverse_transform(topk_idx).tolist()\n    \n        test_probabilities.append(topk_probs)\n        test_predictions.append(topk_preds)\n        test_top3_predictions.append(\" \".join(topk_preds[:3]))\n    \n    test_submission_data = pd.DataFrame({\n        \"row_id\": test_data.row_id.tolist(),\n        \"QuestionId\": test_data.QuestionId.tolist(),\n        \"is_correct\": test_data.is_correct.tolist(),\n        \"probs\": test_probabilities,\n        \"preds\": test_predictions,\n        'Category:Misconception': test_top3_predictions\n    })\n\n    return test_submission_data\n\n\ndef main():\n    \"\"\"メイン推論関数\"\"\"\n    \n    # メモリキャッシュをクリア\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # CUDAメモリ管理の最適化\n    import os\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n    \n    # 2つのGPUを使用可能にする\n    if torch.cuda.device_count() > 1:\n        print(f\"Found {torch.cuda.device_count()} GPUs\")\n\n    print(\"Loading label encoder...\")\n    # ラベルエンコーダーの読み込み\n    le = joblib.load(LABEL_ENCODER_PATH)\n    n_classes = len(le.classes_)\n\n    print(\"Loading trained model and tokenizer...\")\n\n    if PEFT_AVAILABLE:\n        # LoRAアダプターを使用する場合\n        print(f\"Loading fine-tuned LoRA model from: {BEST_MODEL_PATH}\")\n        print(f\"Loading base model from: {MODEL_NAME}\")\n\n        # ベースモデルを読み込む（量子化なし）\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME,\n            num_labels=n_classes,\n            trust_remote_code=True,\n            device_map=\"auto\",  # 自動的に複数GPUに分散\n            low_cpu_mem_usage=True,  # CPUメモリ使用量を削減\n            torch_dtype=torch.float16  # FP16を使用してメモリ効率を改善\n        )\n\n        # LoRAアダプターを適用\n        model = PeftModel.from_pretrained(model, BEST_MODEL_PATH)\n        \n        # 推論モードに設定（メモリ効率化）\n        model.eval()\n        # 8bit量子化モデルは既にGPUに配置されているのでto('cuda')は不要\n\n        # トークナイザーはベースモデルから読み込む\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n        print(\"Successfully loaded LoRA fine-tuned model\")\n    else:\n        # PEFTが利用できない場合はエラー\n        raise ImportError(\"PEFT is required to load the fine-tuned model. Please install peft: pip install peft\")\n\n    # パディングトークンの設定\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n        tokenizer.pad_token_id = 100349  # Phi-4-reasoning-plusのPADトークンID\n    \n    # モデルの設定を更新（PeftModelのbase_modelにアクセス）\n    if hasattr(model, 'base_model'):\n        model.base_model.config.pad_token_id = tokenizer.pad_token_id\n        # 内部のモデルにも設定\n        if hasattr(model.base_model, 'model'):\n            model.base_model.model.config.pad_token_id = tokenizer.pad_token_id\n    else:\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n    print(\"Loading test data...\")\n    # テストデータの読み込み\n    test = pd.read_csv(TEST_DATA_PATH)\n\n    print(\"Loading training data for correct answers...\")\n    # 正解答案データの準備（訓練データから取得）\n    train = pd.read_csv(TRAIN_DATA_PATH)\n    train.Misconception = train.Misconception.fillna('NA')\n    correct = prepare_correct_answers(train)\n\n    print(\"Preprocessing test data...\")\n    # テストデータの前処理\n    test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n    test.is_correct = test.is_correct.fillna(0)\n    test['text'] = test.apply(format_input, axis=1)\n\n    print(\"Tokenizing test data...\")\n    # テストデータのトークナイズ\n    ds_test = Dataset.from_pandas(test[['text']])\n    ds_test = tokenize_dataset(ds_test, tokenizer, MAX_LEN)\n\n    # パディングのためのデータコラレータの設定\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    print(\"Running inference...\")\n    \n    # TF32を有効化（推論速度向上）\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # 推論の実行\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,  # tokenizer の代替\n        data_collator=data_collator,  # バッチ時に自動でパディングを適用\n        args=TrainingArguments(\n            output_dir=\"./tmp\",  # 一時ディレクトリ（必須パラメータ）\n            report_to=\"none\",    # wandbを無効化\n            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # 設定ファイルから取得\n            fp16=True,  # float16を使用\n            dataloader_pin_memory=True,  # データローダーの高速化\n            dataloader_num_workers=2,  # データ読み込みの並列化\n        )\n    )\n    # no_gradコンテキストで推論を実行（メモリ効率化）\n    with torch.no_grad():\n        predictions = trainer.predict(ds_test)\n\n    print(\"Creating submission file...\")\n    # 提出用ファイルの作成\n    submission = create_submission(predictions, test, le)\n\n    # ファイルの保存\n    submission.to_csv(SUBMISSION_OUTPUT_PATH, index=False)\n    print(f\"Submission file saved to: {SUBMISSION_OUTPUT_PATH}\")\n    print(\"\\nSubmission preview:\")\n    print(submission.head())\n    print(f\"\\nSubmission shape: {submission.shape}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:56:27.573717Z","iopub.execute_input":"2025-09-04T03:56:27.573986Z","iopub.status.idle":"2025-09-04T03:56:27.586243Z","shell.execute_reply.started":"2025-09-04T03:56:27.573964Z","shell.execute_reply":"2025-09-04T03:56:27.585438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python qwen3_32b_0_947.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:31:26.556492Z","iopub.execute_input":"2025-09-04T03:31:26.556677Z","iopub.status.idle":"2025-09-04T03:39:39.657471Z","shell.execute_reply.started":"2025-09-04T03:31:26.556661Z","shell.execute_reply":"2025-09-04T03:39:39.656573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python phi_4_reasoning_0_948.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:56:36.155942Z","iopub.execute_input":"2025-09-04T03:56:36.156219Z","iopub.status.idle":"2025-09-04T03:58:49.975846Z","shell.execute_reply.started":"2025-09-04T03:56:36.156197Z","shell.execute_reply":"2025-09-04T03:58:49.974934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python phi_4_0_948_fulltrain.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:51:51.364185Z","iopub.execute_input":"2025-09-04T03:51:51.364452Z","iopub.status.idle":"2025-09-04T03:54:01.068084Z","shell.execute_reply.started":"2025-09-04T03:51:51.364433Z","shell.execute_reply":"2025-09-04T03:54:01.067322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python deepseek_0_946.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:44:07.273761Z","iopub.execute_input":"2025-09-04T03:44:07.274008Z","iopub.status.idle":"2025-09-04T03:47:45.987858Z","shell.execute_reply.started":"2025-09-04T03:44:07.273982Z","shell.execute_reply":"2025-09-04T03:47:45.986839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python qwen3_14b_0_946.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:47:45.989106Z","iopub.execute_input":"2025-09-04T03:47:45.989404Z","iopub.status.idle":"2025-09-04T03:50:54.170301Z","shell.execute_reply.started":"2025-09-04T03:47:45.989368Z","shell.execute_reply":"2025-09-04T03:50:54.169252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\ndef get_top_k_ensemble(ll, k=3):\n\n    lists = [l.split(' ') for l in ll]\n    weights = [4 for l in lists]\n    score = defaultdict(int)\n\n    for i, lst in enumerate(lists):\n        weight = weights[i]\n        for rank, item in enumerate(lst):\n            score[item] += (len(lst) - rank) * weight\n\n    sorted_items = sorted(score.items(), key=lambda x: -x[1])\n    return ' '.join([item for item, _ in sorted_items[:k]])\n\nlist1 = 'a b d f'\nlist2 = 'b c a e'\nlist3 = 'c e b'\nlist4 = 'c e d'\n\nprint(get_top_k_ensemble([list1, list2, list3, list4], k=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T03:54:52.051372Z","iopub.execute_input":"2025-09-04T03:54:52.051666Z","iopub.status.idle":"2025-09-04T03:54:52.058238Z","shell.execute_reply.started":"2025-09-04T03:54:52.05164Z","shell.execute_reply":"2025-09-04T03:54:52.05758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf1 = pd.read_csv('/kaggle/working/deepseek_r1_submission.csv')\ndf2 = pd.read_csv('/kaggle/working/qwen3_14b_submission.csv')\ndf3 = pd.read_csv('/kaggle/working/phi_4_reasoning_0_948_submission.csv')\ndf4 = pd.read_csv('/kaggle/working/phi_4_0_948_fulltrain_submission.csv')\ndf5 = pd.read_csv('/kaggle/working/qwen3_32b_0_947_submission.csv')\n\ndf1 = df1.sort_values('row_id').reset_index(drop=True)\ndf2 = df2.sort_values('row_id').reset_index(drop=True)\ndf3 = df3.sort_values('row_id').reset_index(drop=True)\ndf4 = df4.sort_values('row_id').reset_index(drop=True)\ndf5 = df5.sort_values('row_id').reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:06:36.481519Z","iopub.execute_input":"2025-09-04T04:06:36.481796Z","iopub.status.idle":"2025-09-04T04:06:36.495713Z","shell.execute_reply.started":"2025-09-04T04:06:36.481775Z","shell.execute_reply":"2025-09-04T04:06:36.495141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_predictions = []\nfor r1, r2, r3, r4, r5 in zip(df1.itertuples(), df2.itertuples(), df3.itertuples(), df4.itertuples(), df5.itertuples()):\n\n    prob_preds_1 = sorted([(pb, pr) for pb, pr in zip(eval(r1.probs), eval(r1.preds))], key=lambda x: x[1])\n    prob_preds_2 = sorted([(pb, pr) for pb, pr in zip(eval(r2.probs), eval(r2.preds))], key=lambda x: x[1])\n    prob_preds_3 = sorted([(pb, pr) for pb, pr in zip(eval(r3.probs), eval(r3.preds))], key=lambda x: x[1])\n    prob_preds_4 = sorted([(pb, pr) for pb, pr in zip(eval(r4.probs), eval(r4.preds))], key=lambda x: x[1])\n    prob_preds_5 = sorted([(pb, pr) for pb, pr in zip(eval(r5.probs), eval(r5.preds))], key=lambda x: x[1])\n\n    # Should be same for all row_ids\n    choices = [x[1] for x in prob_preds_1]\n\n    mean_probs = np.mean([\n        [x[0] for x in prob_preds_1],\n        [x[0] for x in prob_preds_2],\n        [x[0] for x in prob_preds_3],\n        [x[0] for x in prob_preds_4],\n        [x[0] for x in prob_preds_5],\n    ],\n    axis=0)\n\n    final_prob_preds = sorted([(l, p) for l, p in zip(choices, mean_probs)], key=lambda x: -x[1])\n\n    row = {\n        \"row_id\": r1.row_id,\n        \"Category:Misconception\": \" \".join([x[0] for x in final_prob_preds[:3]])\n    }\n\n    ensemble_predictions.append(row)\n\nensemble_predictions = pd.DataFrame(ensemble_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:09:43.151628Z","iopub.execute_input":"2025-09-04T04:09:43.152177Z","iopub.status.idle":"2025-09-04T04:09:43.163228Z","shell.execute_reply.started":"2025-09-04T04:09:43.152153Z","shell.execute_reply":"2025-09-04T04:09:43.162529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_predictions.to_csv('submission.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:10:13.836676Z","iopub.execute_input":"2025-09-04T04:10:13.837296Z","iopub.status.idle":"2025-09-04T04:10:13.842806Z","shell.execute_reply.started":"2025-09-04T04:10:13.837269Z","shell.execute_reply":"2025-09-04T04:10:13.842108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:10:14.471797Z","iopub.execute_input":"2025-09-04T04:10:14.472332Z","iopub.status.idle":"2025-09-04T04:10:14.47895Z","shell.execute_reply.started":"2025-09-04T04:10:14.472311Z","shell.execute_reply":"2025-09-04T04:10:14.478263Z"}},"outputs":[],"execution_count":null}]}